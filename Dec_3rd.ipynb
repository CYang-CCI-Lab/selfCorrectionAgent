{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from src.metrics import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('/home/yl3427/cylab/selfCorrectionAgent/mixtral_rag_result/0929_ltm_rag2.csv') # 이거야. 여기에 mixtral결과 다있어\n",
    "t14_calculate_metrics(df['t'], df['zscot_t_stage'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t14_calculate_metrics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_mean_std(results, cat):\n",
    "    precision_list = [result[cat][\"precision\"] for result in results]\n",
    "    recall_list = [result[cat][\"recall\"] for result in results]\n",
    "    f1_list = [result[cat][\"f1\"] for result in results]\n",
    "    support_list = [result[cat][\"support\"] for result in results]\n",
    "    num_errors_list = [result[cat][\"num_errors\"] for result in results]\n",
    "\n",
    "    mean_precision = sum(precision_list) / len(precision_list)\n",
    "    mean_recall = sum(recall_list) / len(recall_list)\n",
    "    mean_f1 = sum(f1_list) / len(f1_list)\n",
    "\n",
    "    std_precision = (\n",
    "        sum([(x - mean_precision) ** 2 for x in precision_list]) / len(precision_list)\n",
    "    ) ** 0.5\n",
    "    std_recall = (\n",
    "        sum([(x - mean_recall) ** 2 for x in recall_list]) / len(recall_list)\n",
    "    ) ** 0.5\n",
    "    std_f1 = (sum([(x - mean_f1) ** 2 for x in f1_list]) / len(f1_list)) ** 0.5\n",
    "\n",
    "    return {\n",
    "        \"mean_precision\": round(mean_precision, 3),\n",
    "        \"mean_recall\": round(mean_recall, 3),\n",
    "        \"mean_f1\": round(mean_f1, 3),\n",
    "        \"std_precision\": round(std_precision, 3),\n",
    "        \"std_recall\": round(std_recall, 3),\n",
    "        \"std_f1\": round(std_f1, 3),\n",
    "        \"sum_support\": sum(support_list),\n",
    "        \"sum_num_errors\": sum(num_errors_list),\n",
    "        \"raw_mean_precision\": mean_precision,\n",
    "        \"raw_mean_recall\": mean_recall,\n",
    "        \"raw_mean_f1\": mean_f1,\n",
    "    }\n",
    "\n",
    "\n",
    "def output_tabular_performance(results, categories=[\"T1\", \"T2\", \"T3\", \"T4\"]):\n",
    "    precisions = []\n",
    "    recalls = []\n",
    "    f1s = []\n",
    "\n",
    "    for category in categories:\n",
    "        eval = calculate_mean_std(results, category)\n",
    "        print(\n",
    "            \"{} {:.3f}({:.3f}) {:.3f}({:.3f}) {:.3f}({:.3f})\".format(\n",
    "                category,\n",
    "                eval[\"mean_precision\"],\n",
    "                eval[\"std_precision\"],\n",
    "                eval[\"mean_recall\"],\n",
    "                eval[\"std_recall\"],\n",
    "                eval[\"mean_f1\"],\n",
    "                eval[\"std_f1\"],\n",
    "            )\n",
    "        )\n",
    "\n",
    "        # for calculating macro average\n",
    "        precisions.append(eval[\"raw_mean_precision\"])\n",
    "        recalls.append(eval[\"raw_mean_recall\"])\n",
    "        f1s.append(eval[\"raw_mean_f1\"])\n",
    "\n",
    "    print(\n",
    "        \"MacroAvg. {:.3f} {:.3f} {:.3f}\".format(\n",
    "            round(sum(precisions) / len(precisions), 3),\n",
    "            round(sum(recalls) / len(recalls), 3),\n",
    "            round(sum(f1s) / len(f1s), 3),\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Index(['patient_filename', 't', 'text', 'n', 'zscot_t_reasoning',\n",
    "       'zscot_t_stage', 'zscot_n_reasoning', 'zscot_n_stage',\n",
    "       'rag_raw_t_reasoning', 'rag_raw_t_stage', 'rag_raw_n_reasoning',\n",
    "       'rag_raw_n_stage', 'ltm_zs_t_reasoning', 'ltm_zs_t_stage',\n",
    "       'ltm_zs_n_reasoning', 'ltm_zs_n_stage', 'ltm_rag1_t_reasoning',\n",
    "       'ltm_rag1_t_stage', 'ltm_rag1_n_reasoning', 'ltm_rag1_n_stage',\n",
    "       'ltm_rag2_t_reasoning', 'ltm_rag2_t_stage', 'ltm_rag2_n_reasoning',\n",
    "       'ltm_rag2_n_stage', 'zscot_t_flag', 'zscot_n_flag', 'rag_raw_t_flag',\n",
    "       'rag_raw_n_flag', 'ltm_zs_t_flag', 'ltm_zs_n_flag', 'ltm_rag1_t_flag',\n",
    "       'ltm_rag1_n_flag', 'ltm_rag2_t_flag', 'ltm_rag2_n_flag'],\n",
    "      dtype='object')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# kewltm_t_df = pd.read_csv(f\"/home/yl3427/cylab/selfCorrectionAgent/result/0718_t14_dynamic_test_{run}_outof_10runs.csv\")\n",
    "others_t_df = pd.read_csv('/home/yl3427/cylab/selfCorrectionAgent/mixtral_rag_result/0929_ltm_rag2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For T\n",
    "print(\"Mixtral\")\n",
    "zscot_t_results = []\n",
    "rag_t_results = []\n",
    "ltm_rag_t_results = []\n",
    "kewltm_t_results = []\n",
    "\n",
    "t_label = 't'\n",
    "zscot_t_stage = 'zscot_t_stage'\n",
    "rag_t_stage = 'rag_raw_t_stage'\n",
    "ltm_rag_t_stage = 'ltm_rag1_t_stage'\n",
    "kewltm_t_stage = \"cmem_t_40reports_ans_str\"\n",
    "\n",
    "others_t_df = pd.read_csv('/home/yl3427/cylab/selfCorrectionAgent/mixtral_rag_result/0929_ltm_rag2.csv')\n",
    "\n",
    "run_lst = [0, 1, 2, 3, 4, 5, 6, 8]\n",
    "\n",
    "for run in run_lst:\n",
    "    t_test_df = pd.read_csv(\n",
    "        f\"/home/yl3427/cylab/selfCorrectionAgent/result/0718_t14_dynamic_test_{run}_outof_10runs.csv\"\n",
    "    ).sort_values(by=\"patient_filename\")\n",
    "\n",
    "    kewltm_t_results.append(\n",
    "        t14_calculate_metrics(t_test_df[t_label], t_test_df[kewltm_t_stage])\n",
    "    )\n",
    "\n",
    "    split_ids = t_test_df.patient_filename\n",
    "    others_t_df = others_t_df[others_t_df[\"patient_filename\"].isin(split_ids)]\n",
    "    zscot_t_results.append(t14_calculate_metrics(others_t_df[t_label], others_t_df[zscot_t_stage]))\n",
    "    rag_t_results.append(t14_calculate_metrics(others_t_df[t_label], others_t_df[rag_t_stage]))\n",
    "    ltm_rag_t_results.append(t14_calculate_metrics(others_t_df[t_label], others_t_df[ltm_rag_t_stage]))\n",
    "\n",
    "print(\"ZSCoT\")\n",
    "output_tabular_performance(zscot_t_results)\n",
    "\n",
    "print(\"RAG\")\n",
    "output_tabular_performance(rag_t_results)\n",
    "\n",
    "print(\"LTM-RAG\")\n",
    "output_tabular_performance(ltm_rag_t_results)\n",
    "\n",
    "print(\"KEW-LTM\")\n",
    "output_tabular_performance(kewltm_t_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For N\n",
    "print(\"Mixtral\")\n",
    "zscot_n_results = []\n",
    "rag_n_results = []\n",
    "ltm_rag_n_results = []\n",
    "kewltm_n_results = []\n",
    "\n",
    "n_label = 'n'\n",
    "zscot_n_stage = 'zscot_n_stage'\n",
    "rag_n_stage = 'rag_raw_n_stage'\n",
    "ltm_rag_n_stage = 'ltm_rag1_n_stage'\n",
    "kewltm_n_stage = \"cmem_n_40reports_ans_str\"\n",
    "\n",
    "others_n_df = pd.read_csv('/home/yl3427/cylab/selfCorrectionAgent/mixtral_rag_result/0929_ltm_rag2.csv')\n",
    "\n",
    "run_lst = [0, 1, 3, 4, 5, 6, 7, 9]\n",
    "\n",
    "for run in run_lst:\n",
    "    n_test_df = pd.read_csv(\n",
    "        f\"/home/yl3427/cylab/selfCorrectionAgent/result/0718_n03_dynamic_test_{run}_outof_10runs.csv\"\n",
    "    ).sort_values(by=\"patient_filename\")\n",
    "\n",
    "    kewltm_n_results.append(\n",
    "        n03_calculate_metrics(n_test_df[n_label], n_test_df[kewltm_n_stage])\n",
    "    )\n",
    "\n",
    "    split_ids = n_test_df.patient_filename\n",
    "    others_n_df = others_n_df[others_n_df[\"patient_filename\"].isin(split_ids)]\n",
    "    zscot_n_results.append(n03_calculate_metrics(others_n_df[n_label], others_n_df[zscot_n_stage]))\n",
    "    rag_n_results.append(n03_calculate_metrics(others_n_df[n_label], others_n_df[rag_n_stage]))\n",
    "    ltm_rag_n_results.append(n03_calculate_metrics(others_n_df[n_label], others_n_df[ltm_rag_n_stage]))\n",
    "\n",
    "print(\"ZSCoT\")\n",
    "output_tabular_performance(zscot_n_results, categories=[\"N0\", \"N1\", \"N2\", \"N3\"])\n",
    "\n",
    "print(\"RAG\")\n",
    "output_tabular_performance(rag_n_results, categories=[\"N0\", \"N1\", \"N2\", \"N3\"])\n",
    "\n",
    "print(\"LTM-RAG\")\n",
    "output_tabular_performance(ltm_rag_n_results, categories=[\"N0\", \"N1\", \"N2\", \"N3\"])\n",
    "\n",
    "print(\"KEW-LTM\")\n",
    "output_tabular_performance(kewltm_n_results, categories=[\"N0\", \"N1\", \"N2\", \"N3\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lst = [\"N0\", \"N1\", \"N2\", \"N3\"]\n",
    "lst.append(\"\")\n",
    "lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('/home/yl3427/cylab/selfCorrectionAgent/result/1128_t14_ltm_rag1_med42_v2_800.csv')\n",
    "df.columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t14_calculate_metrics(df['t'], df['t14_ltm_rag1_t_pred'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_test_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For T\n",
    "print(\"Med42\")\n",
    "zscot_t_results = []\n",
    "rag_t_results = []\n",
    "ltm_rag_t_results = []\n",
    "kewltm_t_results = []\n",
    "\n",
    "t_label = 't'\n",
    "kewltm_t_stage = 'kepa_t_ans_str'\n",
    "zscot_t_df = pd.read_csv('/home/yl3427/cylab/selfCorrectionAgent/result/1118_t14_med42_v2_test_800.csv').sort_values(by=\"patient_filename\")[[\"patient_filename\", 't', 'zscot_t_ans_str']]\n",
    "rag_t_df = pd.read_csv('/home/yl3427/cylab/selfCorrectionAgent/result/1120_t14_rag_raw_med42_v2_800.csv').sort_values(by=\"patient_filename\")[[\"patient_filename\", 't', 't14_rag_raw_t_pred']]\n",
    "ltm_rag_t_df = pd.read_csv('/home/yl3427/cylab/selfCorrectionAgent/result/1128_t14_ltm_rag1_med42_v2_800.csv').sort_values(by=\"patient_filename\")[[\"patient_filename\", 't', 't14_ltm_rag1_t_pred']]\n",
    "\n",
    "run_lst = [0, 1, 2, 3, 4, 5, 6, 8]\n",
    "\n",
    "for run in run_lst:\n",
    "    t_test_df = pd.read_csv(\n",
    "        f\"/home/yl3427/cylab/selfCorrectionAgent/result/1114_t14_med42_v2_test_{run}_outof_10runs.csv\"\n",
    "    ).sort_values(by=\"patient_filename\")\n",
    "\n",
    "    kewltm_t_results.append(\n",
    "        t14_calculate_metrics(t_test_df[t_label], t_test_df[kewltm_t_stage])\n",
    "    )\n",
    "\n",
    "    split_ids = t_test_df.patient_filename\n",
    "    zscot_t_df = zscot_t_df[zscot_t_df[\"patient_filename\"].isin(split_ids)]\n",
    "    rag_t_df = rag_t_df[rag_t_df[\"patient_filename\"].isin(split_ids)]\n",
    "    ltm_rag_t_df = ltm_rag_t_df[ltm_rag_t_df[\"patient_filename\"].isin(split_ids)]\n",
    "\n",
    "    zscot_t_results.append(t14_calculate_metrics(zscot_t_df[t_label], zscot_t_df['zscot_t_ans_str']))\n",
    "    rag_t_results.append(t14_calculate_metrics(rag_t_df[t_label], rag_t_df['t14_rag_raw_t_pred']))\n",
    "    ltm_rag_t_results.append(t14_calculate_metrics(ltm_rag_t_df[t_label], ltm_rag_t_df['t14_ltm_rag1_t_pred']))\n",
    "\n",
    "print(\"ZSCoT\")\n",
    "output_tabular_performance(zscot_t_results)\n",
    "\n",
    "print(\"RAG\")\n",
    "output_tabular_performance(rag_t_results)\n",
    "\n",
    "print(\"LTM-RAG\")\n",
    "output_tabular_performance(ltm_rag_t_results)\n",
    "\n",
    "print(\"KEW-LTM\")\n",
    "output_tabular_performance(kewltm_t_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "brca_report = pd.read_csv(\n",
    "        \"/secure/shared_data/rag_tnm_results/summary/5_folds_summary/brca_df.csv\"\n",
    "    )\n",
    "df = brca_report[brca_report[\"n\"] != -1][[\"patient_filename\", \"t\", \"text\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['patient_filename', 't', 'text', 'memory_test_parsed',\n",
       "       'memory_test_reasoning', 'memory_test_pred',\n",
       "       'memory_test_error_analysis', 'memory_test_refine_memory_count',\n",
       "       'final_memory'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"1221_result.csv\")\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'T2': {'precision': 0.857,\n",
       "  'recall': 0.908,\n",
       "  'f1': 0.882,\n",
       "  'support': 468,\n",
       "  'num_errors': 114},\n",
       " 'T1': {'precision': 0.766,\n",
       "  'recall': 0.786,\n",
       "  'f1': 0.776,\n",
       "  'support': 187,\n",
       "  'num_errors': 85},\n",
       " 'T4': {'precision': 0.65,\n",
       "  'recall': 0.361,\n",
       "  'f1': 0.464,\n",
       "  'support': 36,\n",
       "  'num_errors': 30},\n",
       " 'T3': {'precision': 0.868,\n",
       "  'recall': 0.731,\n",
       "  'f1': 0.794,\n",
       "  'support': 108,\n",
       "  'num_errors': 41},\n",
       " 'overall': {'macro_precision': 0.785,\n",
       "  'macro_recall': 0.697,\n",
       "  'macro_f1': 0.729,\n",
       "  'micro_precision': 0.831,\n",
       "  'micro_recall': 0.831,\n",
       "  'micro_f1': 0.831,\n",
       "  'weighted_f1': 0.826,\n",
       "  'support': 799,\n",
       "  'num_errors': 270}}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from src.metrics import *\n",
    "# t14_calculate_metrics(df['t'], df['memory_test_pred'])\n",
    "df = df[df['memory_test_parsed']]\n",
    "t14_calculate_metrics2(df['t'], df['memory_test_pred'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"baseline_results.csv\")\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t14_calculate_metrics(df['t'], df['t14_baseline_pred'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"1221_result_2.csv\")\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[['t', '1221_test_pred_zs','1221_test_pred_rules', '1221_test_pred_memory_only','1221_test_pred']]\n",
    "t14_calculate_metrics(df['t'], df['1221_test_pred'])['overall']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t14_calculate_metrics(df['t'], df['1221_test_pred'])['overall'], t14_calculate_metrics(df['t'], df['1221_test_pred_zs'])['overall'], t14_calculate_metrics(df['t'], df['1221_test_pred_rules'])['overall'], t14_calculate_metrics(df['t'], df['1221_test_pred_memory_only'])['overall']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"1221_result_3.csv\")[:10]\n",
    "df[['t', '1221_test_pred_zs','1221_test_pred_rules', '1221_test_pred_memory_only','1221_test_pred']]\n",
    "t14_calculate_metrics(df['t'], df['1221_test_pred'])['overall'], t14_calculate_metrics(df['t'], df['1221_test_pred_zs'])['overall'], t14_calculate_metrics(df['t'], df['1221_test_pred_rules'])['overall'], t14_calculate_metrics(df['t'], df['1221_test_pred_memory_only'])['overall']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"1221_result_3.csv\")[10:20]\n",
    "df[['t', '1221_test_pred_zs','1221_test_pred_rules', '1221_test_pred_memory_only','1221_test_pred']]\n",
    "t14_calculate_metrics(df['t'], df['1221_test_pred'])['overall'], t14_calculate_metrics(df['t'], df['1221_test_pred_zs'])['overall'], t14_calculate_metrics(df['t'], df['1221_test_pred_rules'])['overall'], t14_calculate_metrics(df['t'], df['1221_test_pred_memory_only'])['overall']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"1221_result_3.csv\")[20:30]\n",
    "df[['t', '1221_test_pred_zs','1221_test_pred_rules', '1221_test_pred_memory_only','1221_test_pred']]\n",
    "t14_calculate_metrics(df['t'], df['1221_test_pred'])['overall'], t14_calculate_metrics(df['t'], df['1221_test_pred_zs'])['overall'], t14_calculate_metrics(df['t'], df['1221_test_pred_rules'])['overall'], t14_calculate_metrics(df['t'], df['1221_test_pred_memory_only'])['overall']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"1221_result_3.csv\")[30:40]\n",
    "df[['t', '1221_test_pred_zs','1221_test_pred_rules', '1221_test_pred_memory_only','1221_test_pred']]\n",
    "t14_calculate_metrics(df['t'], df['1221_test_pred'])['overall'], t14_calculate_metrics(df['t'], df['1221_test_pred_zs'])['overall'], t14_calculate_metrics(df['t'], df['1221_test_pred_rules'])['overall'], t14_calculate_metrics(df['t'], df['1221_test_pred_memory_only'])['overall']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"1221_result_3.csv\")[40:]\n",
    "df[['t', '1221_test_pred_zs','1221_test_pred_rules', '1221_test_pred_memory_only','1221_test_pred']]\n",
    "t14_calculate_metrics(df['t'], df['1221_test_pred'])['overall'], t14_calculate_metrics(df['t'], df['1221_test_pred_zs'])['overall'], t14_calculate_metrics(df['t'], df['1221_test_pred_rules'])['overall'], t14_calculate_metrics(df['t'], df['1221_test_pred_memory_only'])['overall']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"1221_result_mixtral.csv\")\n",
    "for i in range(0, 790, 10):\n",
    "    sub_df = df[i:i+10]\n",
    "    print(len(sub_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "macro F1: 0.793 (0.825) (0.726) (0.706)\n",
      "micro F1: 0.825 (0.825) (0.800) (0.787)\n",
      "weighted F1: 0.829 (0.824) (0.813) (0.783)\n",
      "-----------------\n",
      "macro F1: 0.911 (0.901) (0.927) (0.908)\n",
      "micro F1: 0.950 (0.938) (0.963) (0.931)\n",
      "weighted F1: 0.947 (0.937) (0.960) (0.929)\n",
      "-----------------\n",
      "macro F1: 0.813 (0.962) (0.860) (0.722)\n",
      "micro F1: 0.937 (0.988) (0.925) (0.875)\n",
      "weighted F1: 0.933 (0.987) (0.931) (0.872)\n",
      "-----------------\n",
      "macro F1: 0.733 (0.617) (0.685) (0.442)\n",
      "micro F1: 0.838 (0.800) (0.812) (0.717)\n",
      "weighted F1: 0.848 (0.792) (0.825) (0.715)\n",
      "-----------------\n",
      "macro F1: 0.594 (0.706) (0.571) (0.720)\n",
      "micro F1: 0.684 (0.825) (0.750) (0.787)\n",
      "weighted F1: 0.678 (0.815) (0.739) (0.775)\n",
      "-----------------\n",
      "macro F1: 0.676 (0.747) (0.704) (0.488)\n",
      "micro F1: 0.812 (0.812) (0.838) (0.713)\n",
      "weighted F1: 0.801 (0.809) (0.830) (0.689)\n",
      "-----------------\n",
      "macro F1: 0.703 (0.923) (0.751) (0.710)\n",
      "micro F1: 0.812 (0.912) (0.875) (0.750)\n",
      "weighted F1: 0.819 (0.912) (0.878) (0.755)\n",
      "-----------------\n",
      "macro F1: 0.770 (0.886) (0.810) (0.682)\n",
      "micro F1: 0.800 (0.975) (0.912) (0.838)\n",
      "weighted F1: 0.805 (0.973) (0.911) (0.843)\n",
      "-----------------\n",
      "macro F1: 0.760 (0.937) (0.889) (0.764)\n",
      "micro F1: 0.787 (0.938) (0.950) (0.850)\n",
      "weighted F1: 0.795 (0.938) (0.952) (0.858)\n",
      "-----------------\n",
      "macro F1: 0.739 (0.752) (0.719) (0.694)\n",
      "micro F1: 0.775 (0.863) (0.838) (0.812)\n",
      "weighted F1: 0.771 (0.850) (0.829) (0.803)\n",
      "-----------------\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from src.metrics import *\n",
    "\n",
    "data_df = pd.read_csv(\"1221_result_mixtral.csv\")\n",
    "for i in range(0, 790, 80):\n",
    "    df = data_df[i:i+80]\n",
    "    main = t14_calculate_metrics2(df['t'],df['1221_test_pred'])['overall']\n",
    "    zs = t14_calculate_metrics2(df['t'], df['1221_test_pred_zs'])['overall']\n",
    "    rules = t14_calculate_metrics2(df['t'], df['1221_test_pred_rules'])['overall']\n",
    "    memory = t14_calculate_metrics2(df['t'], df['1221_test_pred_memory_only'])['overall']\n",
    "    print(f\"macro F1: {main['macro_f1']:.3f} ({zs['macro_f1']:.3f}) ({rules['macro_f1']:.3f}) ({memory['macro_f1']:.3f})\")\n",
    "    print(f\"micro F1: {main['micro_f1']:.3f} ({zs['micro_f1']:.3f}) ({rules['micro_f1']:.3f}) ({memory['micro_f1']:.3f})\")\n",
    "    print(f\"weighted F1: {main['weighted_f1']:.3f} ({zs['weighted_f1']:.3f}) ({rules['weighted_f1']:.3f}) ({memory['weighted_f1']:.3f})\")\n",
    "    print(\"-----------------\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['patient_filename', 't', 'text', 'current_memory', 'ltm_test_pred_zs',\n",
       "       'ltm_test_pred_rules', 'ltm_test_pred_memory_only', 'ltm_test_parsed',\n",
       "       'ltm_test_reasoning', 'ltm_test_pred', 'ltm_test_refine_memory_count',\n",
       "       'base_rules', 'final_memory'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from src.metrics import *\n",
    "data_df = pd.read_csv(\"1222_result_llama_4_30.csv\")\n",
    "data_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t14_calculate_metrics2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "macro F1: 0.780 (0.769) (0.765) (0.753)\n",
      "micro F1: 0.863 (0.835) (0.856) (0.838)\n",
      "weighted F1: 0.867 (0.836) (0.864) (0.841)\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"1222_result_llama_4_30.csv\")\n",
    "\n",
    "main = t14_calculate_metrics2(df['t'],df['ltm_test_pred'])['overall']\n",
    "zs = t14_calculate_metrics2(df['t'], df['ltm_test_pred_zs'])['overall']\n",
    "rules = t14_calculate_metrics2(df['t'], df['ltm_test_pred_rules'])['overall']\n",
    "memory = t14_calculate_metrics2(df['t'], df['ltm_test_pred_memory_only'])['overall']\n",
    "print(f\"macro F1: {main['macro_f1']:.3f} ({zs['macro_f1']:.3f}) ({rules['macro_f1']:.3f}) ({memory['macro_f1']:.3f})\")\n",
    "print(f\"micro F1: {main['micro_f1']:.3f} ({zs['micro_f1']:.3f}) ({rules['micro_f1']:.3f}) ({memory['micro_f1']:.3f})\")\n",
    "print(f\"weighted F1: {main['weighted_f1']:.3f} ({zs['weighted_f1']:.3f}) ({rules['weighted_f1']:.3f}) ({memory['weighted_f1']:.3f})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "macro F1: 0.720 (0.637) (0.686) (0.662)\n",
      "micro F1: 0.825 (0.750) (0.800) (0.775)\n",
      "weighted F1: 0.840 (0.774) (0.822) (0.792)\n",
      "-----------------\n",
      "macro F1: 0.877 (0.925) (0.866) (0.861)\n",
      "micro F1: 0.912 (0.950) (0.912) (0.912)\n",
      "weighted F1: 0.914 (0.950) (0.913) (0.912)\n",
      "-----------------\n",
      "macro F1: 0.915 (0.930) (0.915) (0.881)\n",
      "micro F1: 0.963 (0.963) (0.963) (0.938)\n",
      "weighted F1: 0.967 (0.964) (0.967) (0.945)\n",
      "-----------------\n",
      "macro F1: 0.704 (0.666) (0.735) (0.631)\n",
      "micro F1: 0.850 (0.812) (0.875) (0.825)\n",
      "weighted F1: 0.851 (0.806) (0.883) (0.828)\n",
      "-----------------\n",
      "macro F1: 0.659 (0.652) (0.617) (0.680)\n",
      "micro F1: 0.787 (0.725) (0.738) (0.805)\n",
      "weighted F1: 0.779 (0.727) (0.744) (0.802)\n",
      "-----------------\n",
      "macro F1: 0.799 (0.687) (0.798) (0.720)\n",
      "micro F1: 0.850 (0.738) (0.838) (0.752)\n",
      "weighted F1: 0.850 (0.733) (0.843) (0.751)\n",
      "-----------------\n",
      "macro F1: 0.702 (0.765) (0.677) (0.740)\n",
      "micro F1: 0.850 (0.900) (0.838) (0.875)\n",
      "weighted F1: 0.861 (0.908) (0.854) (0.887)\n",
      "-----------------\n",
      "macro F1: 0.824 (0.761) (0.764) (0.757)\n",
      "micro F1: 0.900 (0.900) (0.887) (0.875)\n",
      "weighted F1: 0.915 (0.904) (0.900) (0.887)\n",
      "-----------------\n",
      "macro F1: 0.776 (0.728) (0.800) (0.677)\n",
      "micro F1: 0.925 (0.838) (0.938) (0.863)\n",
      "weighted F1: 0.935 (0.838) (0.947) (0.862)\n",
      "-----------------\n",
      "macro F1: 0.674 (0.736) (0.685) (0.686)\n",
      "micro F1: 0.762 (0.775) (0.775) (0.762)\n",
      "weighted F1: 0.764 (0.773) (0.784) (0.755)\n",
      "-----------------\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from src.metrics import *\n",
    "\n",
    "data_df = pd.read_csv(\"1222_result_llama_4_30.csv\")\n",
    "for i in range(0, 790, 80):\n",
    "    df = data_df[i:i+80]\n",
    "    main = t14_calculate_metrics2(df['t'],df['ltm_test_pred'])['overall']\n",
    "    zs = t14_calculate_metrics2(df['t'], df['ltm_test_pred_zs'])['overall']\n",
    "    rules = t14_calculate_metrics2(df['t'], df['ltm_test_pred_rules'])['overall']\n",
    "    memory = t14_calculate_metrics2(df['t'], df['ltm_test_pred_memory_only'])['overall']\n",
    "    print(f\"macro F1: {main['macro_f1']:.3f} ({zs['macro_f1']:.3f}) ({rules['macro_f1']:.3f}) ({memory['macro_f1']:.3f})\")\n",
    "    print(f\"micro F1: {main['micro_f1']:.3f} ({zs['micro_f1']:.3f}) ({rules['micro_f1']:.3f}) ({memory['micro_f1']:.3f})\")\n",
    "    print(f\"weighted F1: {main['weighted_f1']:.3f} ({zs['weighted_f1']:.3f}) ({rules['weighted_f1']:.3f}) ({memory['weighted_f1']:.3f})\")\n",
    "    print(\"-----------------\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "macro F1: 0.690 (0.671) (0.708) (0.644)\n",
      "micro F1: 0.800 (0.775) (0.825) (0.762)\n",
      "weighted F1: 0.816 (0.794) (0.841) (0.777)\n",
      "-----------------\n",
      "macro F1: 0.890 (0.925) (0.854) (0.861)\n",
      "micro F1: 0.925 (0.950) (0.900) (0.900)\n",
      "weighted F1: 0.926 (0.950) (0.902) (0.896)\n",
      "-----------------\n",
      "macro F1: 0.936 (0.929) (0.915) (0.921)\n",
      "micro F1: 0.975 (0.963) (0.963) (0.963)\n",
      "weighted F1: 0.977 (0.964) (0.967) (0.964)\n",
      "-----------------\n",
      "macro F1: 0.722 (0.657) (0.735) (0.568)\n",
      "micro F1: 0.875 (0.800) (0.875) (0.812)\n",
      "weighted F1: 0.882 (0.795) (0.883) (0.796)\n",
      "-----------------\n",
      "macro F1: 0.631 (0.654) (0.589) (0.650)\n",
      "micro F1: 0.750 (0.725) (0.700) (0.775)\n",
      "weighted F1: 0.745 (0.728) (0.708) (0.763)\n",
      "-----------------\n",
      "macro F1: 0.837 (0.695) (0.798) (0.679)\n",
      "micro F1: 0.887 (0.750) (0.838) (0.762)\n",
      "weighted F1: 0.890 (0.745) (0.843) (0.750)\n",
      "-----------------\n",
      "macro F1: 0.718 (0.765) (0.735) (0.635)\n",
      "micro F1: 0.863 (0.900) (0.863) (0.863)\n",
      "weighted F1: 0.884 (0.908) (0.878) (0.856)\n",
      "-----------------\n",
      "macro F1: 0.764 (0.705) (0.818) (0.757)\n",
      "micro F1: 0.887 (0.875) (0.912) (0.875)\n",
      "weighted F1: 0.900 (0.881) (0.923) (0.887)\n",
      "-----------------\n",
      "macro F1: 0.800 (0.759) (0.800) (0.687)\n",
      "micro F1: 0.938 (0.875) (0.938) (0.875)\n",
      "weighted F1: 0.947 (0.877) (0.947) (0.876)\n",
      "-----------------\n",
      "macro F1: 0.714 (0.729) (0.688) (0.711)\n",
      "micro F1: 0.800 (0.787) (0.775) (0.787)\n",
      "weighted F1: 0.804 (0.785) (0.785) (0.788)\n",
      "-----------------\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from src.metrics import *\n",
    "\n",
    "data_df = pd.read_csv(\"1222_result_llama_1_1.csv\")\n",
    "for i in range(0, 790, 80):\n",
    "    df = data_df[i:i+80]\n",
    "    main = t14_calculate_metrics2(df['t'],df['ltm_test_pred'])['overall']\n",
    "    zs = t14_calculate_metrics2(df['t'], df['ltm_test_pred_zs'])['overall']\n",
    "    rules = t14_calculate_metrics2(df['t'], df['ltm_test_pred_rules'])['overall']\n",
    "    memory = t14_calculate_metrics2(df['t'], df['ltm_test_pred_memory_only'])['overall']\n",
    "    print(f\"macro F1: {main['macro_f1']:.3f} ({zs['macro_f1']:.3f}) ({rules['macro_f1']:.3f}) ({memory['macro_f1']:.3f})\")\n",
    "    print(f\"micro F1: {main['micro_f1']:.3f} ({zs['micro_f1']:.3f}) ({rules['micro_f1']:.3f}) ({memory['micro_f1']:.3f})\")\n",
    "    print(f\"weighted F1: {main['weighted_f1']:.3f} ({zs['weighted_f1']:.3f}) ({rules['weighted_f1']:.3f}) ({memory['weighted_f1']:.3f})\")\n",
    "    print(\"-----------------\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "macro F1: 0.783 (0.774) (0.771) (0.747)\n",
      "micro F1: 0.870 (0.840) (0.859) (0.838)\n",
      "weighted F1: 0.875 (0.842) (0.866) (0.836)\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"1222_result_llama_1_1.csv\")\n",
    "\n",
    "main = t14_calculate_metrics2(df['t'],df['ltm_test_pred'])['overall']\n",
    "zs = t14_calculate_metrics2(df['t'], df['ltm_test_pred_zs'])['overall']\n",
    "rules = t14_calculate_metrics2(df['t'], df['ltm_test_pred_rules'])['overall']\n",
    "memory = t14_calculate_metrics2(df['t'], df['ltm_test_pred_memory_only'])['overall']\n",
    "print(f\"macro F1: {main['macro_f1']:.3f} ({zs['macro_f1']:.3f}) ({rules['macro_f1']:.3f}) ({memory['macro_f1']:.3f})\")\n",
    "print(f\"micro F1: {main['micro_f1']:.3f} ({zs['micro_f1']:.3f}) ({rules['micro_f1']:.3f}) ({memory['micro_f1']:.3f})\")\n",
    "print(f\"weighted F1: {main['weighted_f1']:.3f} ({zs['weighted_f1']:.3f}) ({rules['weighted_f1']:.3f}) ({memory['weighted_f1']:.3f})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "macro F1: 0.746 (0.812) (0.789) (0.749)\n",
      "micro F1: 0.834 (0.883) (0.861) (0.851)\n",
      "weighted F1: 0.836 (0.881) (0.859) (0.846)\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"1223_result_mixtral_1_1.csv\")\n",
    "\n",
    "main = t14_calculate_metrics2(df['t'],df['ltm_test_pred'])['overall']\n",
    "zs = t14_calculate_metrics2(df['t'], df['ltm_test_pred_zs'])['overall']\n",
    "rules = t14_calculate_metrics2(df['t'], df['ltm_test_pred_rules'])['overall']\n",
    "memory = t14_calculate_metrics2(df['t'], df['ltm_test_pred_memory_only'])['overall']\n",
    "print(f\"macro F1: {main['macro_f1']:.3f} ({zs['macro_f1']:.3f}) ({rules['macro_f1']:.3f}) ({memory['macro_f1']:.3f})\")\n",
    "print(f\"micro F1: {main['micro_f1']:.3f} ({zs['micro_f1']:.3f}) ({rules['micro_f1']:.3f}) ({memory['micro_f1']:.3f})\")\n",
    "print(f\"weighted F1: {main['weighted_f1']:.3f} ({zs['weighted_f1']:.3f}) ({rules['weighted_f1']:.3f}) ({memory['weighted_f1']:.3f})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "macro F1: 0.764 (0.741) (0.761) (0.721)\n",
      "micro F1: 0.864 (0.823) (0.857) (0.817)\n",
      "weighted F1: 0.870 (0.824) (0.865) (0.816)\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"/home/yl3427/cylab/selfCorrectionAgent/1223_result_llama_1_1_increased_model_len_65536_full.csv\")\n",
    "\n",
    "main = t14_calculate_metrics2(df['t'],df['ltm_test_pred'])['overall']\n",
    "zs = t14_calculate_metrics2(df['t'], df['ltm_test_pred_zs'])['overall']\n",
    "rules = t14_calculate_metrics2(df['t'], df['ltm_test_pred_rules'])['overall']\n",
    "memory = t14_calculate_metrics2(df['t'], df['ltm_test_pred_memory_only'])['overall']\n",
    "print(f\"macro F1: {main['macro_f1']:.3f} ({zs['macro_f1']:.3f}) ({rules['macro_f1']:.3f}) ({memory['macro_f1']:.3f})\")\n",
    "print(f\"micro F1: {main['micro_f1']:.3f} ({zs['micro_f1']:.3f}) ({rules['micro_f1']:.3f}) ({memory['micro_f1']:.3f})\")\n",
    "print(f\"weighted F1: {main['weighted_f1']:.3f} ({zs['weighted_f1']:.3f}) ({rules['weighted_f1']:.3f}) ({memory['weighted_f1']:.3f})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "macro F1: 0.758 (0.703) (0.776) (0.650)\n",
      "micro F1: 0.838 (0.775) (0.863) (0.738)\n",
      "weighted F1: 0.845 (0.780) (0.875) (0.737)\n",
      "-----------------\n",
      "macro F1: 0.678 (0.583) (0.671) (0.745)\n",
      "micro F1: 0.825 (0.762) (0.825) (0.838)\n",
      "weighted F1: 0.834 (0.766) (0.839) (0.829)\n",
      "-----------------\n",
      "macro F1: 0.842 (0.808) (0.817) (0.783)\n",
      "micro F1: 0.912 (0.863) (0.887) (0.825)\n",
      "weighted F1: 0.912 (0.862) (0.890) (0.816)\n",
      "-----------------\n",
      "macro F1: 0.844 (0.772) (0.811) (0.763)\n",
      "micro F1: 0.938 (0.875) (0.900) (0.850)\n",
      "weighted F1: 0.933 (0.870) (0.895) (0.844)\n",
      "-----------------\n",
      "macro F1: 0.764 (0.674) (0.781) (0.739)\n",
      "micro F1: 0.850 (0.800) (0.863) (0.850)\n",
      "weighted F1: 0.852 (0.790) (0.869) (0.846)\n",
      "-----------------\n",
      "macro F1: 0.811 (0.764) (0.793) (0.761)\n",
      "micro F1: 0.875 (0.825) (0.850) (0.825)\n",
      "weighted F1: 0.874 (0.818) (0.853) (0.818)\n",
      "-----------------\n",
      "macro F1: 0.822 (0.803) (0.814) (0.700)\n",
      "micro F1: 0.900 (0.838) (0.887) (0.762)\n",
      "weighted F1: 0.901 (0.829) (0.888) (0.757)\n",
      "-----------------\n",
      "macro F1: 0.827 (0.795) (0.810) (0.754)\n",
      "micro F1: 0.870 (0.826) (0.863) (0.837)\n",
      "weighted F1: 0.868 (0.827) (0.857) (0.828)\n",
      "-----------------\n",
      "macro F1: 0.727 (0.714) (0.719) (0.642)\n",
      "micro F1: 0.887 (0.825) (0.875) (0.800)\n",
      "weighted F1: 0.892 (0.823) (0.880) (0.797)\n",
      "-----------------\n",
      "macro F1: 0.735 (0.777) (0.761) (0.736)\n",
      "micro F1: 0.812 (0.850) (0.825) (0.812)\n",
      "weighted F1: 0.824 (0.845) (0.831) (0.814)\n",
      "-----------------\n"
     ]
    }
   ],
   "source": [
    "data_df = pd.read_csv(\"/home/yl3427/cylab/selfCorrectionAgent/1223_result_llama_1_1_increased_model_len_65536_full.csv\")\n",
    "\n",
    "for i in range(0, 790, 80):\n",
    "    df = data_df[i:i+80]\n",
    "    main = t14_calculate_metrics2(df['t'],df['ltm_test_pred'])['overall']\n",
    "    zs = t14_calculate_metrics2(df['t'], df['ltm_test_pred_zs'])['overall']\n",
    "    rules = t14_calculate_metrics2(df['t'], df['ltm_test_pred_rules'])['overall']\n",
    "    memory = t14_calculate_metrics2(df['t'], df['ltm_test_pred_memory_only'])['overall']\n",
    "    print(f\"macro F1: {main['macro_f1']:.3f} ({zs['macro_f1']:.3f}) ({rules['macro_f1']:.3f}) ({memory['macro_f1']:.3f})\")\n",
    "    print(f\"micro F1: {main['micro_f1']:.3f} ({zs['micro_f1']:.3f}) ({rules['micro_f1']:.3f}) ({memory['micro_f1']:.3f})\")\n",
    "    print(f\"weighted F1: {main['weighted_f1']:.3f} ({zs['weighted_f1']:.3f}) ({rules['weighted_f1']:.3f}) ({memory['weighted_f1']:.3f})\")\n",
    "    print(\"-----------------\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
