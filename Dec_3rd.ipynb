{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from src.metrics import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['patient_filename', 't', 'text', 'n', 'zscot_t_reasoning',\n",
       "       'zscot_t_stage', 'zscot_n_reasoning', 'zscot_n_stage',\n",
       "       'rag_raw_t_reasoning', 'rag_raw_t_stage', 'rag_raw_n_reasoning',\n",
       "       'rag_raw_n_stage', 'ltm_zs_t_reasoning', 'ltm_zs_t_stage',\n",
       "       'ltm_zs_n_reasoning', 'ltm_zs_n_stage', 'ltm_rag1_t_reasoning',\n",
       "       'ltm_rag1_t_stage', 'ltm_rag1_n_reasoning', 'ltm_rag1_n_stage',\n",
       "       'ltm_rag2_t_reasoning', 'ltm_rag2_t_stage', 'ltm_rag2_n_reasoning',\n",
       "       'ltm_rag2_n_stage', 'zscot_t_flag', 'zscot_n_flag', 'rag_raw_t_flag',\n",
       "       'rag_raw_n_flag', 'ltm_zs_t_flag', 'ltm_zs_n_flag', 'ltm_rag1_t_flag',\n",
       "       'ltm_rag1_n_flag', 'ltm_rag2_t_flag', 'ltm_rag2_n_flag'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Example for t14_calculate_metrics ---\n",
      "Per-label metrics:\n",
      "  T1: {'precision': 0.75, 'recall': 1.0, 'f1': 0.857, 'support': 3, 'tp': 3, 'fp': 1, 'fn': 0, 'num_errors': 1}\n",
      "  T2: {'precision': 0.5, 'recall': 0.667, 'f1': 0.571, 'support': 3, 'tp': 2, 'fp': 2, 'fn': 1, 'num_errors': 3}\n",
      "  T3: {'precision': 1.0, 'recall': 0.333, 'f1': 0.5, 'support': 3, 'tp': 1, 'fp': 0, 'fn': 2, 'num_errors': 2}\n",
      "  T4: {'precision': 0.5, 'recall': 1.0, 'f1': 0.667, 'support': 1, 'tp': 1, 'fp': 1, 'fn': 0, 'num_errors': 1}\n",
      "Overall metrics:\n",
      "  {'micro_precision': 0.636, 'micro_recall': 0.7, 'micro_f1': 0.667, 'macro_precision': 0.688, 'macro_recall': 0.75, 'macro_f1': 0.649, 'weighted_f1': 0.645, 'support': 10, 'total_tp': 7, 'total_fp': 4, 'total_fn': 3, 'num_errors': 7}\n",
      "\n",
      "\n",
      "--- Example for n03_calculate_metrics ---\n",
      "Per-label metrics:\n",
      "  N0: {'precision': 0.6, 'recall': 0.75, 'f1': 0.667, 'support': 4, 'tp': 3, 'fp': 2, 'fn': 1, 'num_errors': 3}\n",
      "  N1: {'precision': 0.75, 'recall': 0.75, 'f1': 0.75, 'support': 4, 'tp': 3, 'fp': 1, 'fn': 1, 'num_errors': 2}\n",
      "  N2: {'precision': 1.0, 'recall': 1.0, 'f1': 1.0, 'support': 2, 'tp': 2, 'fp': 0, 'fn': 0, 'num_errors': 0}\n",
      "Overall metrics:\n",
      "  {'micro_precision': 0.727, 'micro_recall': 0.8, 'micro_f1': 0.762, 'macro_precision': 0.783, 'macro_recall': 0.833, 'macro_f1': 0.806, 'weighted_f1': 0.767, 'support': 10, 'total_tp': 8, 'total_fp': 3, 'total_fn': 2, 'num_errors': 5}\n",
      "\n",
      "--- Example for t14_calculate_metrics with all correct predictions ---\n",
      "Overall metrics (all correct):\n",
      "  {'micro_precision': 1.0, 'micro_recall': 1.0, 'micro_f1': 1.0, 'macro_precision': 1.0, 'macro_recall': 1.0, 'macro_f1': 1.0, 'weighted_f1': 1.0, 'support': 4, 'total_tp': 4, 'total_fp': 0, 'total_fn': 0, 'num_errors': 0}\n",
      "\n",
      "--- Example for t14_calculate_metrics with all incorrect predictions (different labels) ---\n",
      "Overall metrics (all incorrect, different labels):\n",
      "  {'micro_precision': 0.0, 'micro_recall': 0.0, 'micro_f1': 0.0, 'macro_precision': 0.0, 'macro_recall': 0.0, 'macro_f1': 0.0, 'weighted_f1': 0.0, 'support': 3, 'total_tp': 0, 'total_fp': 3, 'total_fn': 3, 'num_errors': 6}\n",
      "  T1: {'precision': 0.0, 'recall': 0.0, 'f1': 0.0, 'support': 1, 'tp': 0, 'fp': 1, 'fn': 1, 'num_errors': 2}\n",
      "  T2: {'precision': 0.0, 'recall': 0.0, 'f1': 0.0, 'support': 1, 'tp': 0, 'fp': 1, 'fn': 1, 'num_errors': 2}\n",
      "  T3: {'precision': 0.0, 'recall': 0.0, 'f1': 0.0, 'support': 1, 'tp': 0, 'fp': 1, 'fn': 1, 'num_errors': 2}\n",
      "\n",
      "--- Example for t14_calculate_metrics with predictions containing non-existent labels ---\n",
      "Overall metrics (predictions with non-existent labels):\n",
      "  {'micro_precision': 1.0, 'micro_recall': 0.5, 'micro_f1': 0.667, 'macro_precision': 0.5, 'macro_recall': 0.5, 'macro_f1': 0.5, 'weighted_f1': 0.5, 'support': 2, 'total_tp': 1, 'total_fp': 0, 'total_fn': 1, 'num_errors': 1}\n",
      "  T1: {'precision': 1.0, 'recall': 1.0, 'f1': 1.0, 'support': 1, 'tp': 1, 'fp': 0, 'fn': 0, 'num_errors': 0}\n",
      "  T2: {'precision': 0.0, 'recall': 0.0, 'f1': 0.0, 'support': 1, 'tp': 0, 'fp': 0, 'fn': 1, 'num_errors': 1}\n",
      "\n",
      "--- Example for n03_calculate_metrics ---\n",
      "Per-label metrics:\n",
      "  N0: {'precision': 0.5, 'recall': 0.5, 'f1': 0.5, 'support': 2, 'tp': 1, 'fp': 1, 'fn': 1, 'num_errors': 2}\n",
      "  N1: {'precision': 0.0, 'recall': 0.0, 'f1': 0.0, 'support': 2, 'tp': 0, 'fp': 1, 'fn': 2, 'num_errors': 3}\n",
      "  N2: {'precision': 0.5, 'recall': 1.0, 'f1': 0.667, 'support': 1, 'tp': 1, 'fp': 1, 'fn': 0, 'num_errors': 1}\n",
      "Overall metrics:\n",
      "  {'micro_precision': 0.4, 'micro_recall': 0.4, 'micro_f1': 0.4, 'macro_precision': 0.333, 'macro_recall': 0.5, 'macro_f1': 0.389, 'weighted_f1': 0.333, 'support': 5, 'total_tp': 2, 'total_fp': 3, 'total_fn': 3, 'num_errors': 6}\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def t14_calculate_metrics(true_labels: pd.Series, predictions: pd.Series) -> dict:\n",
    "    \"\"\"\n",
    "    Calculates precision, recall, F1-score, and support for T-stage predictions.\n",
    "    Includes both per-label, micro-average, and macro-average scores.\n",
    "\n",
    "    Args:\n",
    "        true_labels: A pandas Series of true labels (e.g., integers from 0 to N-1).\n",
    "        predictions: A pandas Series of predicted labels (strings).\n",
    "\n",
    "    Returns:\n",
    "        A dictionary containing the metrics for each label and overall scores.\n",
    "    \"\"\"\n",
    "    # Check for valid inputs\n",
    "    if len(true_labels) != len(predictions):\n",
    "        raise ValueError(\"The length of true_labels and predictions must be the same.\")\n",
    "\n",
    "    if not isinstance(true_labels, pd.Series) or not isinstance(predictions, pd.Series):\n",
    "        raise TypeError(\"true_labels and predictions must be pandas Series.\")\n",
    "\n",
    "    if any(pd.isna(pred) or not isinstance(pred, str) for pred in predictions):\n",
    "        raise ValueError(\"All predictions must be non-null strings.\")\n",
    "\n",
    "    # Standardize true labels to \"T{x+1}\" format\n",
    "    true_labels = true_labels.apply(lambda x: f\"T{int(x)+1}\")\n",
    "\n",
    "    metrics = {}\n",
    "    label_counts = {}\n",
    "    unique_true_labels = sorted(list(set(true_labels))) # Ensure consistent order\n",
    "\n",
    "    for label in unique_true_labels:\n",
    "        metrics[label] = {\"tp\": 0, \"fp\": 0, \"fn\": 0}\n",
    "        label_counts[label] = 0\n",
    "\n",
    "    for true_label, prediction in zip(true_labels, predictions):\n",
    "        # Ensure prediction is a string and convert to uppercase\n",
    "        prediction_str = str(prediction).upper()\n",
    "        \n",
    "        label_counts[true_label] += 1\n",
    "        if true_label in prediction_str:\n",
    "            metrics[true_label][\"tp\"] += 1\n",
    "        else:\n",
    "            metrics[true_label][\"fn\"] += 1\n",
    "\n",
    "        # Calculate false positives\n",
    "        # A prediction is a false positive for a label if:\n",
    "        # 1. The label is present in the prediction string.\n",
    "        # 2. The label is NOT the true_label.\n",
    "        for label_to_check_fp in unique_true_labels:\n",
    "            if label_to_check_fp in prediction_str and label_to_check_fp != true_label:\n",
    "                metrics[label_to_check_fp][\"fp\"] += 1\n",
    "    \n",
    "    results = {}\n",
    "    # Variables for micro-averaging\n",
    "    total_tp_micro = 0\n",
    "    total_fp_micro = 0\n",
    "    total_fn_micro = 0\n",
    "    \n",
    "    # Variables for macro-averaging\n",
    "    macro_precision_sum = 0.0\n",
    "    macro_recall_sum = 0.0\n",
    "    macro_f1_sum = 0.0\n",
    "    \n",
    "    total_instances = len(true_labels)\n",
    "\n",
    "    for label in unique_true_labels: # Iterate in a defined order\n",
    "        counts = metrics[label]\n",
    "        tp = counts[\"tp\"]\n",
    "        fp = counts[\"fp\"]\n",
    "        fn = counts[\"fn\"]\n",
    "\n",
    "        # Precision: TP / (TP + FP)\n",
    "        precision = tp / (tp + fp) if (tp + fp) > 0 else 0.0\n",
    "        # Recall: TP / (TP + FN)\n",
    "        recall = tp / (tp + fn) if (tp + fn) > 0 else 0.0\n",
    "        # F1-Score: 2 * (Precision * Recall) / (Precision + Recall)\n",
    "        f1 = (\n",
    "            2 * precision * recall / (precision + recall)\n",
    "            if (precision + recall) > 0\n",
    "            else 0.0\n",
    "        )\n",
    "        support = label_counts[label]\n",
    "\n",
    "        results[label] = {\n",
    "            \"precision\": round(precision, 3),\n",
    "            \"recall\": round(recall, 3),\n",
    "            \"f1\": round(f1, 3),\n",
    "            \"support\": support,\n",
    "            \"tp\": tp,\n",
    "            \"fp\": fp,\n",
    "            \"fn\": fn,\n",
    "            \"num_errors\": fp + fn, # Sum of false positives and false negatives for this class\n",
    "        }\n",
    "\n",
    "        # Accumulate for micro-averages\n",
    "        total_tp_micro += tp\n",
    "        total_fp_micro += fp\n",
    "        total_fn_micro += fn\n",
    "        \n",
    "        # Accumulate for macro-averages\n",
    "        macro_precision_sum += precision\n",
    "        macro_recall_sum += recall\n",
    "        macro_f1_sum += f1\n",
    "\n",
    "    # Calculate macro-averaged metrics\n",
    "    num_labels = len(unique_true_labels)\n",
    "    macro_precision = macro_precision_sum / num_labels if num_labels > 0 else 0.0\n",
    "    macro_recall = macro_recall_sum / num_labels if num_labels > 0 else 0.0\n",
    "    macro_f1 = macro_f1_sum / num_labels if num_labels > 0 else 0.0 # Often calculated as harmonic mean of macro_precision and macro_recall\n",
    "\n",
    "    # Calculate micro-averaged (overall) precision, recall, and F1 score\n",
    "    # Micro-Precision: Sum of all TPs / (Sum of all TPs + Sum of all FPs)\n",
    "    micro_precision = (\n",
    "        total_tp_micro / (total_tp_micro + total_fp_micro) if (total_tp_micro + total_fp_micro) > 0 else 0.0\n",
    "    )\n",
    "    # Micro-Recall: Sum of all TPs / (Sum of all TPs + Sum of all FNs)\n",
    "    micro_recall = (\n",
    "        total_tp_micro / (total_tp_micro + total_fn_micro) if (total_tp_micro + total_fn_micro) > 0 else 0.0\n",
    "    )\n",
    "    # Micro-F1: 2 * (Micro-Precision * Micro-Recall) / (Micro-Precision + Micro-Recall)\n",
    "    micro_f1 = (\n",
    "        2 * micro_precision * micro_recall / (micro_precision + micro_recall)\n",
    "        if (micro_precision + micro_recall) > 0\n",
    "        else 0.0\n",
    "    )\n",
    "\n",
    "    # Calculate weighted F1 score\n",
    "    weighted_f1_sum = 0.0\n",
    "    for label in unique_true_labels:\n",
    "        weighted_f1_sum += results[label][\"f1\"] * label_counts[label]\n",
    "    weighted_f1 = weighted_f1_sum / total_instances if total_instances > 0 else 0.0\n",
    "\n",
    "    results[\"overall\"] = {\n",
    "        \"micro_precision\": round(micro_precision, 3),\n",
    "        \"micro_recall\": round(micro_recall, 3),\n",
    "        \"micro_f1\": round(micro_f1, 3),\n",
    "        \"macro_precision\": round(macro_precision, 3),\n",
    "        \"macro_recall\": round(macro_recall, 3),\n",
    "        \"macro_f1\": round(macro_f1, 3),\n",
    "        \"weighted_f1\": round(weighted_f1, 3),\n",
    "        \"support\": total_instances,\n",
    "        \"total_tp\": total_tp_micro,\n",
    "        \"total_fp\": total_fp_micro,\n",
    "        \"total_fn\": total_fn_micro,\n",
    "        \"num_errors\": total_fp_micro + total_fn_micro, # Sum of all false positives and false negatives\n",
    "    }\n",
    "    return results\n",
    "\n",
    "\n",
    "def n03_calculate_metrics(true_labels: pd.Series, predictions: pd.Series) -> dict:\n",
    "    \"\"\"\n",
    "    Calculates precision, recall, F1-score, and support for N-stage predictions.\n",
    "    Includes both per-label, micro-average, and macro-average scores.\n",
    "    Handles specific label replacements: \"NO\" to \"N0\", \"NL\" to \"N1\".\n",
    "\n",
    "    Args:\n",
    "        true_labels: A pandas Series of true labels (e.g., integers from 0 to N-1).\n",
    "        predictions: A pandas Series of predicted labels (strings).\n",
    "\n",
    "    Returns:\n",
    "        A dictionary containing the metrics for each label and overall scores.\n",
    "    \"\"\"\n",
    "    # Check for valid inputs\n",
    "    if len(true_labels) != len(predictions):\n",
    "        raise ValueError(\"The length of true_labels and predictions must be the same.\")\n",
    "\n",
    "    if not isinstance(true_labels, pd.Series) or not isinstance(predictions, pd.Series):\n",
    "        raise TypeError(\"true_labels and predictions must be pandas Series.\")\n",
    "\n",
    "    if any(pd.isna(pred) or not isinstance(pred, str) for pred in predictions):\n",
    "        raise ValueError(\"All predictions must be non-null strings.\")\n",
    "\n",
    "    # Standardize true labels to \"N{x}\" format\n",
    "    true_labels = true_labels.apply(lambda x: f\"N{int(x)}\")\n",
    "\n",
    "    metrics = {}\n",
    "    label_counts = {}\n",
    "    unique_true_labels = sorted(list(set(true_labels))) # Ensure consistent order\n",
    "\n",
    "    for label in unique_true_labels:\n",
    "        metrics[label] = {\"tp\": 0, \"fp\": 0, \"fn\": 0}\n",
    "        label_counts[label] = 0\n",
    "\n",
    "    for true_label, prediction in zip(true_labels, predictions):\n",
    "        # Ensure prediction is a string, convert to uppercase, and apply replacements\n",
    "        prediction_str = str(prediction).upper()\n",
    "        prediction_str = prediction_str.replace(\"NO\", \"N0\").replace(\"NL\", \"N1\")\n",
    "        \n",
    "        label_counts[true_label] += 1\n",
    "        if true_label in prediction_str:\n",
    "            metrics[true_label][\"tp\"] += 1\n",
    "        else:\n",
    "            metrics[true_label][\"fn\"] += 1\n",
    "\n",
    "        # Calculate false positives\n",
    "        for label_to_check_fp in unique_true_labels:\n",
    "            if label_to_check_fp in prediction_str and label_to_check_fp != true_label:\n",
    "                metrics[label_to_check_fp][\"fp\"] += 1\n",
    "\n",
    "    results = {}\n",
    "    # Variables for micro-averaging\n",
    "    total_tp_micro = 0\n",
    "    total_fp_micro = 0\n",
    "    total_fn_micro = 0\n",
    "    \n",
    "    # Variables for macro-averaging\n",
    "    macro_precision_sum = 0.0\n",
    "    macro_recall_sum = 0.0\n",
    "    macro_f1_sum = 0.0\n",
    "\n",
    "    total_instances = len(true_labels)\n",
    "\n",
    "    for label in unique_true_labels: # Iterate in a defined order\n",
    "        counts = metrics[label]\n",
    "        tp = counts[\"tp\"]\n",
    "        fp = counts[\"fp\"]\n",
    "        fn = counts[\"fn\"]\n",
    "\n",
    "        precision = tp / (tp + fp) if (tp + fp) > 0 else 0.0\n",
    "        recall = tp / (tp + fn) if (tp + fn) > 0 else 0.0\n",
    "        f1 = (\n",
    "            2 * precision * recall / (precision + recall)\n",
    "            if (precision + recall) > 0\n",
    "            else 0.0\n",
    "        )\n",
    "        support = label_counts[label]\n",
    "\n",
    "        results[label] = {\n",
    "            \"precision\": round(precision, 3),\n",
    "            \"recall\": round(recall, 3),\n",
    "            \"f1\": round(f1, 3),\n",
    "            \"support\": support,\n",
    "            \"tp\": tp,\n",
    "            \"fp\": fp,\n",
    "            \"fn\": fn,\n",
    "            \"num_errors\": fp + fn,\n",
    "        }\n",
    "\n",
    "        total_tp_micro += tp\n",
    "        total_fp_micro += fp\n",
    "        total_fn_micro += fn\n",
    "\n",
    "        macro_precision_sum += precision\n",
    "        macro_recall_sum += recall\n",
    "        macro_f1_sum += f1\n",
    "\n",
    "    # Calculate macro-averaged metrics\n",
    "    num_labels = len(unique_true_labels)\n",
    "    macro_precision = macro_precision_sum / num_labels if num_labels > 0 else 0.0\n",
    "    macro_recall = macro_recall_sum / num_labels if num_labels > 0 else 0.0\n",
    "    macro_f1 = macro_f1_sum / num_labels if num_labels > 0 else 0.0\n",
    "\n",
    "    # Calculate micro-averaged (overall) precision, recall, and F1 score\n",
    "    micro_precision = (\n",
    "        total_tp_micro / (total_tp_micro + total_fp_micro) if (total_tp_micro + total_fp_micro) > 0 else 0.0\n",
    "    )\n",
    "    micro_recall = (\n",
    "        total_tp_micro / (total_tp_micro + total_fn_micro) if (total_tp_micro + total_fn_micro) > 0 else 0.0\n",
    "    )\n",
    "    micro_f1 = (\n",
    "        2 * micro_precision * micro_recall / (micro_precision + micro_recall)\n",
    "        if (micro_precision + micro_recall) > 0\n",
    "        else 0.0\n",
    "    )\n",
    "\n",
    "    # Calculate weighted F1 score\n",
    "    weighted_f1_sum = 0.0\n",
    "    for label in unique_true_labels:\n",
    "        weighted_f1_sum += results[label][\"f1\"] * label_counts[label]\n",
    "    weighted_f1 = weighted_f1_sum / total_instances if total_instances > 0 else 0.0\n",
    "    \n",
    "    results[\"overall\"] = {\n",
    "        \"micro_precision\": round(micro_precision, 3),\n",
    "        \"micro_recall\": round(micro_recall, 3),\n",
    "        \"micro_f1\": round(micro_f1, 3),\n",
    "        \"macro_precision\": round(macro_precision, 3),\n",
    "        \"macro_recall\": round(macro_recall, 3),\n",
    "        \"macro_f1\": round(macro_f1, 3),\n",
    "        \"weighted_f1\": round(weighted_f1, 3),\n",
    "        \"support\": total_instances,\n",
    "        \"total_tp\": total_tp_micro,\n",
    "        \"total_fp\": total_fp_micro,\n",
    "        \"total_fn\": total_fn_micro,\n",
    "        \"num_errors\": total_fp_micro + total_fn_micro,\n",
    "    }\n",
    "\n",
    "    return results\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # Example Usage for t14_calculate_metrics\n",
    "    print(\"--- Example for t14_calculate_metrics ---\")\n",
    "    true_t_labels = pd.Series([0, 1, 2, 0, 1, 2, 0, 1, 2, 3]) \n",
    "    # Corresponding to T1, T2, T3, T1, T2, T3, T1, T2, T3, T4\n",
    "    predictions_t = pd.Series([\n",
    "        \"T1\", \"T2\", \"T3\", \n",
    "        \"T1\", \"T1\", \"T2\", # T2 pred as T1, T3 pred as T2\n",
    "        \"T1 and T2\", \"T2\", \"T4\", # T1 pred as T1 and T2 (FP for T2), T3 pred as T4 (FN for T3, FP for T4 if T4 is not true)\n",
    "        \"T4\"\n",
    "    ])\n",
    "    \n",
    "    # Test with mixed types in predictions (should raise ValueError)\n",
    "    # predictions_t_invalid = pd.Series([\"T1\", \"T2\", None, \"T1\"])\n",
    "    # try:\n",
    "    #     t14_metrics_invalid = t14_calculate_metrics(true_t_labels, predictions_t_invalid)\n",
    "    #     for key, value in t14_metrics_invalid.items():\n",
    "    #         print(f\"{key}: {value}\")\n",
    "    # except ValueError as e:\n",
    "    #     print(f\"Error: {e}\")\n",
    "\n",
    "    # Test with valid predictions\n",
    "    t14_metrics = t14_calculate_metrics(true_t_labels, predictions_t)\n",
    "    print(\"Per-label metrics:\")\n",
    "    for label, metrics_val in t14_metrics.items():\n",
    "        if label != \"overall\":\n",
    "            print(f\"  {label}: {metrics_val}\")\n",
    "    print(\"Overall metrics:\")\n",
    "    print(f\"  {t14_metrics['overall']}\")\n",
    "    print(\"\\n\")\n",
    "\n",
    "    # Example Usage for n03_calculate_metrics\n",
    "    print(\"--- Example for n03_calculate_metrics ---\")\n",
    "    true_n_labels = pd.Series([0, 1, 0, 1, 0, 0, 1, 1, 2, 2]) \n",
    "    # Corresponding to N0, N1, N0, N1, N0, N0, N1, N1, N2, N2\n",
    "    predictions_n = pd.Series([\n",
    "        \"N0\", \"N1\", \"N0\", \"N1\", \n",
    "        \"NO\", # N0, but \"NO\" should be converted to \"N0\"\n",
    "        \"N1\", # True N0, predicted N1 (FP for N1, FN for N0)\n",
    "        \"NL\", # N1, but \"NL\" should be converted to \"N1\"\n",
    "        \"N0\", # True N1, predicted N0 (FP for N0, FN for N1)\n",
    "        \"N2\", \n",
    "        \"N2 and N0\" # True N2, predicted N2 and N0 (FP for N0)\n",
    "    ])\n",
    "    n03_metrics = n03_calculate_metrics(true_n_labels, predictions_n)\n",
    "    print(\"Per-label metrics:\")\n",
    "    for label, metrics_val in n03_metrics.items():\n",
    "        if label != \"overall\":\n",
    "            print(f\"  {label}: {metrics_val}\")\n",
    "    print(\"Overall metrics:\")\n",
    "    print(f\"  {n03_metrics['overall']}\")\n",
    "\n",
    "    # Example with empty inputs (should ideally be handled or raise error based on design)\n",
    "    # print(\"\\n--- Example with empty inputs ---\")\n",
    "    # empty_true = pd.Series([], dtype='object')\n",
    "    # empty_preds = pd.Series([], dtype='object')\n",
    "    # try:\n",
    "    #     empty_metrics = t14_calculate_metrics(empty_true, empty_preds)\n",
    "    #     print(empty_metrics) # Should produce all zeros or handle gracefully\n",
    "    # except ValueError as e:\n",
    "    #     print(f\"Error with empty inputs: {e}\") # Current code will raise ValueError due to len check\n",
    "\n",
    "    # Example with labels not present in true_labels but in predictions\n",
    "    # This scenario is implicitly handled as FPs for those \"extra\" predicted labels won't be counted against any true class directly,\n",
    "    # but they might influence the FP count of actual classes if the prediction string contains multiple labels.\n",
    "    # The current logic for FP:\n",
    "    #   `if label in prediction and label != true_label: metrics[label][\"fp\"] += 1`\n",
    "    # This means FP is only counted for labels that are actual true labels in the dataset.\n",
    "    # If a prediction contains \"T5\" but \"T5\" is not in `unique_true_labels`, it won't get an FP count.\n",
    "    # This is a common way to handle it, focusing on the predefined set of classes.\n",
    "\n",
    "    print(\"\\n--- Example for t14_calculate_metrics with all correct predictions ---\")\n",
    "    true_t_all_correct = pd.Series([0, 1, 2, 0])\n",
    "    predictions_t_all_correct = pd.Series([\"T1\", \"T2\", \"T3\", \"T1\"])\n",
    "    t14_metrics_correct = t14_calculate_metrics(true_t_all_correct, predictions_t_all_correct)\n",
    "    print(\"Overall metrics (all correct):\")\n",
    "    print(f\"  {t14_metrics_correct['overall']}\")\n",
    "    # Expected: micro_precision: 1.0, micro_recall: 1.0, micro_f1: 1.0\n",
    "    # Expected: macro_precision: 1.0, macro_recall: 1.0, macro_f1: 1.0\n",
    "\n",
    "    print(\"\\n--- Example for t14_calculate_metrics with all incorrect predictions (different labels) ---\")\n",
    "    true_t_all_incorrect = pd.Series([0, 1, 2]) # T1, T2, T3\n",
    "    predictions_t_all_incorrect = pd.Series([\"T2\", \"T3\", \"T1\"]) # Predicted T2 for T1, T3 for T2, T1 for T3\n",
    "    t14_metrics_incorrect = t14_calculate_metrics(true_t_all_incorrect, predictions_t_all_incorrect)\n",
    "    print(\"Overall metrics (all incorrect, different labels):\")\n",
    "    # For T1: TP=0, FN=1 (true T1, pred T2), FP=1 (pred T1 for true T3)\n",
    "    # For T2: TP=0, FN=1 (true T2, pred T3), FP=1 (pred T2 for true T1)\n",
    "    # For T3: TP=0, FN=1 (true T3, pred T1), FP=1 (pred T3 for true T2)\n",
    "    # Micro: total_tp_micro=0, total_fp_micro=3, total_fn_micro=3. P=0, R=0, F1=0\n",
    "    # Macro: P for T1=0, R for T1=0, F1 for T1=0. Same for T2, T3. Macro P,R,F1 = 0\n",
    "    print(f\"  {t14_metrics_incorrect['overall']}\")\n",
    "    for label in ['T1', 'T2', 'T3']:\n",
    "        print(f\"  {label}: {t14_metrics_incorrect[label]}\")\n",
    "\n",
    "    print(\"\\n--- Example for t14_calculate_metrics with predictions containing non-existent labels ---\")\n",
    "    true_t_extra_pred = pd.Series([0, 1]) # T1, T2\n",
    "    predictions_t_extra_pred = pd.Series([\"T1 and T5\", \"T6\"]) # T5 and T6 are not in true labels\n",
    "    t14_metrics_extra = t14_calculate_metrics(true_t_extra_pred, predictions_t_extra_pred)\n",
    "    print(\"Overall metrics (predictions with non-existent labels):\")\n",
    "    print(f\"  {t14_metrics_extra['overall']}\")\n",
    "    # T1: true T1, pred \"T1 and T5\". TP=1 for T1. FN=0 for T1. FP=0 for T1 (T5 is not a known label to check against).\n",
    "    # T2: true T2, pred \"T6\". TP=0 for T2. FN=1 for T2. FP=0 for T2.\n",
    "    # Micro: total_tp_micro=1, total_fp_micro=0, total_fn_micro=1. P=1/(1+0)=1, R=1/(1+1)=0.5, F1=2*1*0.5/(1+0.5) = 1/1.5 = 0.667\n",
    "    # Macro P: (P_T1 + P_T2)/2 = (1+0)/2 = 0.5\n",
    "    # Macro R: (R_T1 + R_T2)/2 = (1+0)/2 = 0.5\n",
    "    # Macro F1: (F1_T1 + F1_T2)/2 = (1+0)/2 = 0.5\n",
    "    # Wait, my FP logic: `if label_to_check_fp in prediction_str and label_to_check_fp != true_label:`\n",
    "    # `label_to_check_fp` comes from `unique_true_labels`. So T5 and T6 won't cause FPs for T1 or T2.\n",
    "    # This is correct.\n",
    "    print(f\"  T1: {t14_metrics_extra['T1']}\")\n",
    "    print(f\"  T2: {t14_metrics_extra['T2']}\")\n",
    "\n",
    "\n",
    "    # Example for n03 with a mix\n",
    "    print(\"\\n--- Example for n03_calculate_metrics ---\")\n",
    "    true_n = pd.Series([0, 0, 1, 1, 2]) # N0, N0, N1, N1, N2\n",
    "    pred_n = pd.Series([\"N0\", \"N1\", \"N0\", \"N2\", \"N2\"])\n",
    "    # N0: True=N0, Pred=N0 -> TP for N0\n",
    "    # N0: True=N0, Pred=N1 -> FN for N0, FP for N1\n",
    "    # N1: True=N1, Pred=N0 -> FN for N1, FP for N0\n",
    "    # N1: True=N1, Pred=N2 -> FN for N1, FP for N2\n",
    "    # N2: True=N2, Pred=N2 -> TP for N2\n",
    "\n",
    "    # N0: TP=1, FN=1, FP=1 -> P=0.5, R=0.5, F1=0.5\n",
    "    # N1: TP=0, FN=2, FP=1 -> P=0, R=0, F1=0\n",
    "    # N2: TP=1, FN=0, FP=1 -> P=0.5, R=1, F1=0.667\n",
    "\n",
    "    # Micro: total_tp_micro=2, total_fp_micro=3, total_fn_micro=3\n",
    "    # Micro P = 2 / (2+3) = 2/5 = 0.4\n",
    "    # Micro R = 2 / (2+3) = 2/5 = 0.4\n",
    "    # Micro F1 = 2 * 0.4 * 0.4 / (0.4+0.4) = 0.32 / 0.8 = 0.4\n",
    "\n",
    "    # Macro P = (0.5 + 0 + 0.5) / 3 = 1/3 = 0.333\n",
    "    # Macro R = (0.5 + 0 + 1) / 3 = 1.5/3 = 0.5\n",
    "    # Macro F1 = (0.5 + 0 + 0.667) / 3 = 1.167/3 = 0.389\n",
    "\n",
    "    n03_res = n03_calculate_metrics(true_n, pred_n)\n",
    "    print(\"Per-label metrics:\")\n",
    "    for label, metrics_val in n03_res.items():\n",
    "        if label != \"overall\":\n",
    "            print(f\"  {label}: {metrics_val}\")\n",
    "    print(\"Overall metrics:\")\n",
    "    print(f\"  {n03_res['overall']}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'micro_precision': 0.85, 'micro_recall': 0.863, 'micro_f1': 0.856, 'macro_precision': 0.831, 'macro_recall': 0.765, 'macro_f1': 0.792, 'weighted_f1': 0.854, 'support': 800, 'total_tp': 690, 'total_fp': 122, 'total_fn': 110, 'num_errors': 232}\n",
      "\n",
      "{'micro_precision': 0.81, 'micro_recall': 0.815, 'micro_f1': 0.812, 'macro_precision': 0.771, 'macro_recall': 0.73, 'macro_f1': 0.743, 'weighted_f1': 0.812, 'support': 800, 'total_tp': 652, 'total_fp': 153, 'total_fn': 148, 'num_errors': 301}\n",
      "\n",
      "{'micro_precision': 0.853, 'micro_recall': 0.848, 'micro_f1': 0.85, 'macro_precision': 0.792, 'macro_recall': 0.728, 'macro_f1': 0.746, 'weighted_f1': 0.846, 'support': 800, 'total_tp': 678, 'total_fp': 117, 'total_fn': 122, 'num_errors': 239}\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('/home/yl3427/cylab/selfCorrectionAgent/mixtral_rag_result/0929_ltm_rag2.csv') # 이거야. 여기에 mixtral결과 다있어\n",
    "print(t14_calculate_metrics(df['t'], df['zscot_t_stage'])['overall'])\n",
    "print()\n",
    "print(t14_calculate_metrics(df['t'], df['rag_raw_t_stage'])['overall'])\n",
    "print()\n",
    "print(t14_calculate_metrics(df['t'], df['ltm_rag1_t_stage'])['overall'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'micro_precision': 0.874, 'micro_recall': 0.873, 'micro_f1': 0.873, 'macro_precision': 0.843, 'macro_recall': 0.822, 'macro_f1': 0.832, 'weighted_f1': 0.872, 'support': 800, 'total_tp': 698, 'total_fp': 101, 'total_fn': 102, 'num_errors': 203}\n",
      "\n",
      "{'micro_precision': 0.841, 'micro_recall': 0.835, 'micro_f1': 0.838, 'macro_precision': 0.803, 'macro_recall': 0.799, 'macro_f1': 0.797, 'weighted_f1': 0.84, 'support': 800, 'total_tp': 668, 'total_fp': 126, 'total_fn': 132, 'num_errors': 258}\n",
      "\n",
      "{'micro_precision': 0.853, 'micro_recall': 0.859, 'micro_f1': 0.856, 'macro_precision': 0.807, 'macro_recall': 0.814, 'macro_f1': 0.81, 'weighted_f1': 0.856, 'support': 800, 'total_tp': 687, 'total_fp': 118, 'total_fn': 113, 'num_errors': 231}\n"
     ]
    }
   ],
   "source": [
    "print(n03_calculate_metrics(df['n'], df['zscot_n_stage'])['overall'])\n",
    "print()\n",
    "print(n03_calculate_metrics(df['n'], df['rag_raw_n_stage'])['overall'])\n",
    "print()\n",
    "print(n03_calculate_metrics(df['n'], df['ltm_rag1_n_stage'])['overall'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_mean_std(results, cat):\n",
    "    precision_list = [result[cat][\"precision\"] for result in results]\n",
    "    recall_list = [result[cat][\"recall\"] for result in results]\n",
    "    f1_list = [result[cat][\"f1\"] for result in results]\n",
    "    support_list = [result[cat][\"support\"] for result in results]\n",
    "    num_errors_list = [result[cat][\"num_errors\"] for result in results]\n",
    "\n",
    "    mean_precision = sum(precision_list) / len(precision_list)\n",
    "    mean_recall = sum(recall_list) / len(recall_list)\n",
    "    mean_f1 = sum(f1_list) / len(f1_list)\n",
    "\n",
    "    std_precision = (\n",
    "        sum([(x - mean_precision) ** 2 for x in precision_list]) / len(precision_list)\n",
    "    ) ** 0.5\n",
    "    std_recall = (\n",
    "        sum([(x - mean_recall) ** 2 for x in recall_list]) / len(recall_list)\n",
    "    ) ** 0.5\n",
    "    std_f1 = (sum([(x - mean_f1) ** 2 for x in f1_list]) / len(f1_list)) ** 0.5\n",
    "\n",
    "    return {\n",
    "        \"mean_precision\": round(mean_precision, 3),\n",
    "        \"mean_recall\": round(mean_recall, 3),\n",
    "        \"mean_f1\": round(mean_f1, 3),\n",
    "        \"std_precision\": round(std_precision, 3),\n",
    "        \"std_recall\": round(std_recall, 3),\n",
    "        \"std_f1\": round(std_f1, 3),\n",
    "        \"sum_support\": sum(support_list),\n",
    "        \"sum_num_errors\": sum(num_errors_list),\n",
    "        \"raw_mean_precision\": mean_precision,\n",
    "        \"raw_mean_recall\": mean_recall,\n",
    "        \"raw_mean_f1\": mean_f1,\n",
    "    }\n",
    "\n",
    "\n",
    "def output_tabular_performance(results, categories=[\"T1\", \"T2\", \"T3\", \"T4\"]):\n",
    "    precisions = []\n",
    "    recalls = []\n",
    "    f1s = []\n",
    "\n",
    "    for category in categories:\n",
    "        eval = calculate_mean_std(results, category)\n",
    "        print(\n",
    "            \"{} {:.3f}({:.3f}) {:.3f}({:.3f}) {:.3f}({:.3f})\".format(\n",
    "                category,\n",
    "                eval[\"mean_precision\"],\n",
    "                eval[\"std_precision\"],\n",
    "                eval[\"mean_recall\"],\n",
    "                eval[\"std_recall\"],\n",
    "                eval[\"mean_f1\"],\n",
    "                eval[\"std_f1\"],\n",
    "            )\n",
    "        )\n",
    "\n",
    "        # for calculating macro average\n",
    "        precisions.append(eval[\"raw_mean_precision\"])\n",
    "        recalls.append(eval[\"raw_mean_recall\"])\n",
    "        f1s.append(eval[\"raw_mean_f1\"])\n",
    "\n",
    "    print(\n",
    "        \"MacroAvg. {:.3f} {:.3f} {:.3f}\".format(\n",
    "            round(sum(precisions) / len(precisions), 3),\n",
    "            round(sum(recalls) / len(recalls), 3),\n",
    "            round(sum(f1s) / len(f1s), 3),\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Index(['patient_filename', 't', 'text', 'n', 'zscot_t_reasoning',\n",
    "       'zscot_t_stage', 'zscot_n_reasoning', 'zscot_n_stage',\n",
    "       'rag_raw_t_reasoning', 'rag_raw_t_stage', 'rag_raw_n_reasoning',\n",
    "       'rag_raw_n_stage', 'ltm_zs_t_reasoning', 'ltm_zs_t_stage',\n",
    "       'ltm_zs_n_reasoning', 'ltm_zs_n_stage', 'ltm_rag1_t_reasoning',\n",
    "       'ltm_rag1_t_stage', 'ltm_rag1_n_reasoning', 'ltm_rag1_n_stage',\n",
    "       'ltm_rag2_t_reasoning', 'ltm_rag2_t_stage', 'ltm_rag2_n_reasoning',\n",
    "       'ltm_rag2_n_stage', 'zscot_t_flag', 'zscot_n_flag', 'rag_raw_t_flag',\n",
    "       'rag_raw_n_flag', 'ltm_zs_t_flag', 'ltm_zs_n_flag', 'ltm_rag1_t_flag',\n",
    "       'ltm_rag1_n_flag', 'ltm_rag2_t_flag', 'ltm_rag2_n_flag'],\n",
    "      dtype='object')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# kewltm_t_df = pd.read_csv(f\"/home/yl3427/cylab/selfCorrectionAgent/result/0718_t14_dynamic_test_{run}_outof_10runs.csv\")\n",
    "# others_t_df = pd.read_csv('/home/yl3427/cylab/selfCorrectionAgent/mixtral_rag_result/0929_ltm_rag2.csv')\n",
    "\n",
    "\n",
    "###############################################################################\n",
    "# Updated helper: calculate_mean_std                                          #\n",
    "###############################################################################\n",
    "def calculate_mean_std(\n",
    "        results: list[dict],\n",
    "        cat: str | None,\n",
    "        level: str = \"label\"     # \"label\", \"micro\", or \"macro\"\n",
    "    ) -> dict:\n",
    "    \"\"\"\n",
    "    Compute mean ± std for a single class (\"label\" level) or for the\n",
    "    overall micro / macro aggregates produced by *t14_calculate_metrics*.\n",
    "\n",
    "    Args\n",
    "    ----\n",
    "    results : list of metrics dicts (output of t14_calculate_metrics)\n",
    "    cat     : class label (e.g. \"T1\") – ignored for micro/macro levels\n",
    "    level   : \"label\" | \"micro\" | \"macro\"\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    dict with keys:\n",
    "        mean_precision, mean_recall, mean_f1,\n",
    "        std_precision,  std_recall,  std_f1,\n",
    "        (plus sums and raw means used elsewhere)\n",
    "    \"\"\"\n",
    "    # ------------------------------------------------------------------ #\n",
    "    # Gather the three score lists                                       #\n",
    "    # ------------------------------------------------------------------ #\n",
    "    precision_list, recall_list, f1_list = [], [], []\n",
    "    support_list, num_errors_list = [], []\n",
    "\n",
    "    for res in results:\n",
    "        if level == \"label\":\n",
    "            src = res[cat]                                 # per‑class block\n",
    "        elif level == \"micro\":\n",
    "            src = {                                        # NEW ↓\n",
    "                \"precision\": res[\"overall\"][\"micro_precision\"],\n",
    "                \"recall\":    res[\"overall\"][\"micro_recall\"],\n",
    "                \"f1\":        res[\"overall\"][\"micro_f1\"],\n",
    "                \"support\":   res[\"overall\"][\"support\"],\n",
    "                \"num_errors\": res[\"overall\"][\"num_errors\"],\n",
    "            }\n",
    "        elif level == \"macro\":\n",
    "            src = {                                        # NEW ↓\n",
    "                \"precision\": res[\"overall\"][\"macro_precision\"],\n",
    "                \"recall\":    res[\"overall\"][\"macro_recall\"],\n",
    "                \"f1\":        res[\"overall\"][\"macro_f1\"],\n",
    "                \"support\":   res[\"overall\"][\"support\"],\n",
    "                \"num_errors\": res[\"overall\"][\"num_errors\"],\n",
    "            }\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown level: {level}\")\n",
    "\n",
    "        precision_list.append(src[\"precision\"])\n",
    "        recall_list.append(src[\"recall\"])\n",
    "        f1_list.append(src[\"f1\"])\n",
    "        support_list.append(src[\"support\"])\n",
    "        num_errors_list.append(src[\"num_errors\"])\n",
    "\n",
    "    # ------------------------------------------------------------------ #\n",
    "    # Mean / std                                                         #\n",
    "    # ------------------------------------------------------------------ #\n",
    "    mean_p = sum(precision_list) / len(precision_list)\n",
    "    mean_r = sum(recall_list)    / len(recall_list)\n",
    "    mean_f = sum(f1_list)        / len(f1_list)\n",
    "\n",
    "    std_p = (sum((x - mean_p) ** 2 for x in precision_list) / len(precision_list)) ** 0.5\n",
    "    std_r = (sum((x - mean_r) ** 2 for x in recall_list)    / len(recall_list))    ** 0.5\n",
    "    std_f = (sum((x - mean_f) ** 2 for x in f1_list)        / len(f1_list))        ** 0.5\n",
    "\n",
    "    return {\n",
    "        \"mean_precision\": round(mean_p, 3),\n",
    "        \"mean_recall\":    round(mean_r, 3),\n",
    "        \"mean_f1\":        round(mean_f, 3),\n",
    "        \"std_precision\":  round(std_p, 3),\n",
    "        \"std_recall\":     round(std_r, 3),\n",
    "        \"std_f1\":         round(std_f, 3),\n",
    "        \"sum_support\":    sum(support_list),\n",
    "        \"sum_num_errors\": sum(num_errors_list),\n",
    "        \"raw_mean_precision\": mean_p,   # keep raw for higher‑level macro\n",
    "        \"raw_mean_recall\":    mean_r,\n",
    "        \"raw_mean_f1\":        mean_f,\n",
    "    }\n",
    "\n",
    "\n",
    "###############################################################################\n",
    "# Updated helper: output_tabular_performance                                  #\n",
    "###############################################################################\n",
    "def output_tabular_performance(\n",
    "        results: list[dict],\n",
    "        categories: list[str] = (\"T1\", \"T2\", \"T3\", \"T4\"),\n",
    "        show_overall: bool = True         # NEW flag\n",
    "    ) -> None:\n",
    "    \"\"\"\n",
    "    Print mean ± std precision/recall/F1 for each class, followed by:\n",
    "      • category‑macro average (same as before)\n",
    "      • micro‑average (overall)\n",
    "      • macro‑average (overall)\n",
    "    \"\"\"\n",
    "    # ------------------------------------------------------------------ #\n",
    "    # Per‑class lines                                                    #\n",
    "    # ------------------------------------------------------------------ #\n",
    "    label_means_p, label_means_r, label_means_f = [], [], []\n",
    "\n",
    "    for cat in categories:\n",
    "        stats = calculate_mean_std(results, cat, level=\"label\")\n",
    "        print(f\"{cat:8s} \"\n",
    "              f\"{stats['mean_precision']:.3f}({stats['std_precision']:.3f}) \"\n",
    "              f\"{stats['mean_recall']:.3f}({stats['std_recall']:.3f}) \"\n",
    "              f\"{stats['mean_f1']:.3f}({stats['std_f1']:.3f})\")\n",
    "\n",
    "        label_means_p.append(stats[\"raw_mean_precision\"])\n",
    "        label_means_r.append(stats[\"raw_mean_recall\"])\n",
    "        label_means_f.append(stats[\"raw_mean_f1\"])\n",
    "\n",
    "    # ------------------------------------------------------------------ #\n",
    "    # Category‑macro (average of label means)                            #\n",
    "    # ------------------------------------------------------------------ #\n",
    "    print(f\"{'Cat‑Macro':8s} \"\n",
    "          f\"{sum(label_means_p)/len(label_means_p):.3f} \"\n",
    "          f\"{sum(label_means_r)/len(label_means_r):.3f} \"\n",
    "          f\"{sum(label_means_f)/len(label_means_f):.3f}\")\n",
    "\n",
    "    # ------------------------------------------------------------------ #\n",
    "    # Overall micro / macro                                              #\n",
    "    # ------------------------------------------------------------------ #\n",
    "    if show_overall:\n",
    "        micro = calculate_mean_std(results, None, level=\"micro\")\n",
    "        macro = calculate_mean_std(results, None, level=\"macro\")\n",
    "\n",
    "        print(f\"{'MicroAvg.':8s} \"\n",
    "              f\"{micro['mean_precision']:.3f}({micro['std_precision']:.3f}) \"\n",
    "              f\"{micro['mean_recall']:.3f}({micro['std_recall']:.3f}) \"\n",
    "              f\"{micro['mean_f1']:.3f}({micro['std_f1']:.3f})\")\n",
    "\n",
    "        print(f\"{'MacroAvg.':8s} \"\n",
    "              f\"{macro['mean_precision']:.3f}({macro['std_precision']:.3f}) \"\n",
    "              f\"{macro['mean_recall']:.3f}({macro['std_recall']:.3f}) \"\n",
    "              f\"{macro['mean_f1']:.3f}({macro['std_f1']:.3f})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mixtral\n",
      "KEW-LTM\n",
      "T1       0.904(0.017) 0.812(0.040) 0.855(0.018)\n",
      "T2       0.882(0.022) 0.938(0.018) 0.909(0.005)\n",
      "T3       0.834(0.054) 0.810(0.058) 0.818(0.018)\n",
      "T4       0.807(0.082) 0.634(0.038) 0.707(0.029)\n",
      "Cat‑Macro 0.857 0.799 0.822\n",
      "MicroAvg. 0.876(0.006) 0.878(0.007) 0.877(0.007)\n",
      "MacroAvg. 0.857(0.022) 0.799(0.020) 0.822(0.010)\n"
     ]
    }
   ],
   "source": [
    "# For T\n",
    "print(\"Mixtral\")\n",
    "# zscot_t_results = []\n",
    "# rag_t_results = []\n",
    "# ltm_rag_t_results = []\n",
    "kewltm_t_results = []\n",
    "\n",
    "t_label = 't'\n",
    "# zscot_t_stage = 'zscot_t_stage'\n",
    "# rag_t_stage = 'rag_raw_t_stage'\n",
    "# ltm_rag_t_stage = 'ltm_rag1_t_stage'\n",
    "kewltm_t_stage = \"cmem_t_40reports_ans_str\"\n",
    "\n",
    "# others_t_df = pd.read_csv('/home/yl3427/cylab/selfCorrectionAgent/mixtral_rag_result/0929_ltm_rag2.csv')\n",
    "\n",
    "run_lst = [0, 1, 2, 3, 4, 5, 6, 8]\n",
    "\n",
    "for run in run_lst:\n",
    "    t_test_df = pd.read_csv(\n",
    "        f\"/home/yl3427/cylab/selfCorrectionAgent/result/0718_t14_dynamic_test_{run}_outof_10runs.csv\"\n",
    "    ).sort_values(by=\"patient_filename\")\n",
    "\n",
    "    kewltm_t_results.append(\n",
    "        t14_calculate_metrics(t_test_df[t_label], t_test_df[kewltm_t_stage])\n",
    "    )\n",
    "\n",
    "    # split_ids = t_test_df.patient_filename\n",
    "    # others_t_df = others_t_df[others_t_df[\"patient_filename\"].isin(split_ids)]\n",
    "    # zscot_t_results.append(t14_calculate_metrics(others_t_df[t_label], others_t_df[zscot_t_stage]))\n",
    "    # rag_t_results.append(t14_calculate_metrics(others_t_df[t_label], others_t_df[rag_t_stage]))\n",
    "    # ltm_rag_t_results.append(t14_calculate_metrics(others_t_df[t_label], others_t_df[ltm_rag_t_stage]))\n",
    "\n",
    "# print(\"ZSCoT\")\n",
    "# output_tabular_performance(zscot_t_results)\n",
    "\n",
    "# print(\"RAG\")\n",
    "# output_tabular_performance(rag_t_results)\n",
    "\n",
    "# print(\"LTM-RAG\")\n",
    "# output_tabular_performance(ltm_rag_t_results)\n",
    "\n",
    "print(\"KEW-LTM\")\n",
    "output_tabular_performance(kewltm_t_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mixtral\n",
      "KEW-LTM\n",
      "N0       0.944(0.008) 0.952(0.018) 0.948(0.011)\n",
      "N1       0.885(0.020) 0.883(0.026) 0.884(0.010)\n",
      "N2       0.713(0.031) 0.745(0.054) 0.727(0.022)\n",
      "N3       0.886(0.058) 0.784(0.042) 0.830(0.017)\n",
      "Cat‑Macro 0.857 0.841 0.847\n",
      "MicroAvg. 0.883(0.007) 0.883(0.007) 0.883(0.007)\n",
      "MacroAvg. 0.857(0.011) 0.841(0.011) 0.847(0.008)\n"
     ]
    }
   ],
   "source": [
    "# For N\n",
    "print(\"Mixtral\")\n",
    "# zscot_n_results = []\n",
    "# rag_n_results = []\n",
    "# ltm_rag_n_results = []\n",
    "kewltm_n_results = []\n",
    "\n",
    "n_label = 'n'\n",
    "# zscot_n_stage = 'zscot_n_stage'\n",
    "# rag_n_stage = 'rag_raw_n_stage'\n",
    "# ltm_rag_n_stage = 'ltm_rag1_n_stage'\n",
    "kewltm_n_stage = \"cmem_n_40reports_ans_str\"\n",
    "\n",
    "# others_n_df = pd.read_csv('/home/yl3427/cylab/selfCorrectionAgent/mixtral_rag_result/0929_ltm_rag2.csv')\n",
    "\n",
    "run_lst = [0, 1, 3, 4, 5, 6, 7, 9]\n",
    "\n",
    "for run in run_lst:\n",
    "    n_test_df = pd.read_csv(\n",
    "        f\"/home/yl3427/cylab/selfCorrectionAgent/result/0718_n03_dynamic_test_{run}_outof_10runs.csv\"\n",
    "    ).sort_values(by=\"patient_filename\")\n",
    "\n",
    "    kewltm_n_results.append(\n",
    "        n03_calculate_metrics(n_test_df[n_label], n_test_df[kewltm_n_stage])\n",
    "    )\n",
    "\n",
    "    # split_ids = n_test_df.patient_filename\n",
    "    # others_n_df = others_n_df[others_n_df[\"patient_filename\"].isin(split_ids)]\n",
    "    # zscot_n_results.append(n03_calculate_metrics(others_n_df[n_label], others_n_df[zscot_n_stage]))\n",
    "    # rag_n_results.append(n03_calculate_metrics(others_n_df[n_label], others_n_df[rag_n_stage]))\n",
    "    # ltm_rag_n_results.append(n03_calculate_metrics(others_n_df[n_label], others_n_df[ltm_rag_n_stage]))\n",
    "\n",
    "# print(\"ZSCoT\")\n",
    "# output_tabular_performance(zscot_n_results, categories=[\"N0\", \"N1\", \"N2\", \"N3\"])\n",
    "\n",
    "# print(\"RAG\")\n",
    "# output_tabular_performance(rag_n_results, categories=[\"N0\", \"N1\", \"N2\", \"N3\"])\n",
    "\n",
    "# print(\"LTM-RAG\")\n",
    "# output_tabular_performance(ltm_rag_n_results, categories=[\"N0\", \"N1\", \"N2\", \"N3\"])\n",
    "\n",
    "print(\"KEW-LTM\")\n",
    "output_tabular_performance(kewltm_n_results, categories=[\"N0\", \"N1\", \"N2\", \"N3\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lst = [\"N0\", \"N1\", \"N2\", \"N3\"]\n",
    "lst.append(\"\")\n",
    "lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('/home/yl3427/cylab/selfCorrectionAgent/result/1128_t14_ltm_rag1_med42_v2_800.csv')\n",
    "df.columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t14_calculate_metrics(df['t'], df['t14_ltm_rag1_t_pred'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_test_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Med42\n",
      "ZSCoT\n",
      "T1 0.586(0.012) 0.705(0.018) 0.640(0.013)\n",
      "T2 0.822(0.015) 0.801(0.015) 0.811(0.015)\n",
      "T3 0.827(0.011) 0.703(0.026) 0.760(0.015)\n",
      "T4 0.832(0.078) 0.545(0.059) 0.658(0.064)\n",
      "MacroAvg. 0.767 0.689 0.717\n",
      "RAG\n",
      "T1 0.840(0.004) 0.782(0.023) 0.810(0.014)\n",
      "T2 0.834(0.015) 0.897(0.008) 0.864(0.012)\n",
      "T3 0.841(0.012) 0.672(0.021) 0.747(0.014)\n",
      "T4 0.546(0.071) 0.541(0.073) 0.543(0.070)\n",
      "MacroAvg. 0.765 0.723 0.741\n",
      "LTM-RAG\n",
      "T1 0.918(0.015) 0.807(0.008) 0.859(0.010)\n",
      "T2 0.896(0.008) 0.947(0.008) 0.920(0.007)\n",
      "T3 0.842(0.021) 0.859(0.012) 0.850(0.014)\n",
      "T4 0.692(0.065) 0.601(0.052) 0.643(0.055)\n",
      "MacroAvg. 0.837 0.803 0.818\n",
      "KEW-LTM\n",
      "T1 0.813(0.073) 0.759(0.076) 0.783(0.064)\n",
      "T2 0.855(0.031) 0.913(0.023) 0.882(0.016)\n",
      "T3 0.869(0.063) 0.703(0.099) 0.770(0.065)\n",
      "T4 0.630(0.046) 0.615(0.057) 0.621(0.042)\n",
      "MacroAvg. 0.792 0.747 0.764\n"
     ]
    }
   ],
   "source": [
    "# For T\n",
    "print(\"Med42\")\n",
    "zscot_t_results = []\n",
    "rag_t_results = []\n",
    "ltm_rag_t_results = []\n",
    "kewltm_t_results = []\n",
    "\n",
    "t_label = 't'\n",
    "kewltm_t_stage = 'kepa_t_ans_str'\n",
    "zscot_t_df = pd.read_csv('/home/yl3427/cylab/selfCorrectionAgent/result/1118_t14_med42_v2_test_800.csv').sort_values(by=\"patient_filename\")[[\"patient_filename\", 't', 'zscot_t_ans_str']]\n",
    "rag_t_df = pd.read_csv('/home/yl3427/cylab/selfCorrectionAgent/result/1120_t14_rag_raw_med42_v2_800.csv').sort_values(by=\"patient_filename\")[[\"patient_filename\", 't', 't14_rag_raw_t_pred']]\n",
    "ltm_rag_t_df = pd.read_csv('/home/yl3427/cylab/selfCorrectionAgent/result/1128_t14_ltm_rag1_med42_v2_800.csv').sort_values(by=\"patient_filename\")[[\"patient_filename\", 't', 't14_ltm_rag1_t_pred']]\n",
    "\n",
    "run_lst = [0, 1, 2, 3, 4, 5, 6, 8]\n",
    "\n",
    "for run in run_lst:\n",
    "    t_test_df = pd.read_csv(\n",
    "        f\"/home/yl3427/cylab/selfCorrectionAgent/result/1114_t14_med42_v2_test_{run}_outof_10runs.csv\"\n",
    "    ).sort_values(by=\"patient_filename\")\n",
    "\n",
    "    kewltm_t_results.append(\n",
    "        t14_calculate_metrics(t_test_df[t_label], t_test_df[kewltm_t_stage])\n",
    "    )\n",
    "\n",
    "    split_ids = t_test_df.patient_filename\n",
    "    zscot_t_df = zscot_t_df[zscot_t_df[\"patient_filename\"].isin(split_ids)]\n",
    "    rag_t_df = rag_t_df[rag_t_df[\"patient_filename\"].isin(split_ids)]\n",
    "    ltm_rag_t_df = ltm_rag_t_df[ltm_rag_t_df[\"patient_filename\"].isin(split_ids)]\n",
    "\n",
    "    zscot_t_results.append(t14_calculate_metrics(zscot_t_df[t_label], zscot_t_df['zscot_t_ans_str']))\n",
    "    rag_t_results.append(t14_calculate_metrics(rag_t_df[t_label], rag_t_df['t14_rag_raw_t_pred']))\n",
    "    ltm_rag_t_results.append(t14_calculate_metrics(ltm_rag_t_df[t_label], ltm_rag_t_df['t14_ltm_rag1_t_pred']))\n",
    "\n",
    "print(\"ZSCoT\")\n",
    "output_tabular_performance(zscot_t_results)\n",
    "\n",
    "print(\"RAG\")\n",
    "output_tabular_performance(rag_t_results)\n",
    "\n",
    "print(\"LTM-RAG\")\n",
    "output_tabular_performance(ltm_rag_t_results)\n",
    "\n",
    "print(\"KEW-LTM\")\n",
    "output_tabular_performance(kewltm_t_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "brca_report = pd.read_csv(\n",
    "        \"/secure/shared_data/rag_tnm_results/summary/5_folds_summary/brca_df.csv\"\n",
    "    )\n",
    "df = brca_report[brca_report[\"n\"] != -1][[\"patient_filename\", \"t\", \"text\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"1221_result.csv\")\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.metrics import *\n",
    "# t14_calculate_metrics(df['t'], df['memory_test_pred'])\n",
    "df = df[df['memory_test_parsed']]\n",
    "t14_calculate_metrics2(df['t'], df['memory_test_pred'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"baseline_results.csv\")\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t14_calculate_metrics(df['t'], df['t14_baseline_pred'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"1221_result_2.csv\")\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[['t', '1221_test_pred_zs','1221_test_pred_rules', '1221_test_pred_memory_only','1221_test_pred']]\n",
    "t14_calculate_metrics(df['t'], df['1221_test_pred'])['overall']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t14_calculate_metrics(df['t'], df['1221_test_pred'])['overall'], t14_calculate_metrics(df['t'], df['1221_test_pred_zs'])['overall'], t14_calculate_metrics(df['t'], df['1221_test_pred_rules'])['overall'], t14_calculate_metrics(df['t'], df['1221_test_pred_memory_only'])['overall']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"1221_result_3.csv\")[:10]\n",
    "df[['t', '1221_test_pred_zs','1221_test_pred_rules', '1221_test_pred_memory_only','1221_test_pred']]\n",
    "t14_calculate_metrics(df['t'], df['1221_test_pred'])['overall'], t14_calculate_metrics(df['t'], df['1221_test_pred_zs'])['overall'], t14_calculate_metrics(df['t'], df['1221_test_pred_rules'])['overall'], t14_calculate_metrics(df['t'], df['1221_test_pred_memory_only'])['overall']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"1221_result_3.csv\")[10:20]\n",
    "df[['t', '1221_test_pred_zs','1221_test_pred_rules', '1221_test_pred_memory_only','1221_test_pred']]\n",
    "t14_calculate_metrics(df['t'], df['1221_test_pred'])['overall'], t14_calculate_metrics(df['t'], df['1221_test_pred_zs'])['overall'], t14_calculate_metrics(df['t'], df['1221_test_pred_rules'])['overall'], t14_calculate_metrics(df['t'], df['1221_test_pred_memory_only'])['overall']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"1221_result_3.csv\")[20:30]\n",
    "df[['t', '1221_test_pred_zs','1221_test_pred_rules', '1221_test_pred_memory_only','1221_test_pred']]\n",
    "t14_calculate_metrics(df['t'], df['1221_test_pred'])['overall'], t14_calculate_metrics(df['t'], df['1221_test_pred_zs'])['overall'], t14_calculate_metrics(df['t'], df['1221_test_pred_rules'])['overall'], t14_calculate_metrics(df['t'], df['1221_test_pred_memory_only'])['overall']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"1221_result_3.csv\")[30:40]\n",
    "df[['t', '1221_test_pred_zs','1221_test_pred_rules', '1221_test_pred_memory_only','1221_test_pred']]\n",
    "t14_calculate_metrics(df['t'], df['1221_test_pred'])['overall'], t14_calculate_metrics(df['t'], df['1221_test_pred_zs'])['overall'], t14_calculate_metrics(df['t'], df['1221_test_pred_rules'])['overall'], t14_calculate_metrics(df['t'], df['1221_test_pred_memory_only'])['overall']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"1221_result_3.csv\")[40:]\n",
    "df[['t', '1221_test_pred_zs','1221_test_pred_rules', '1221_test_pred_memory_only','1221_test_pred']]\n",
    "t14_calculate_metrics(df['t'], df['1221_test_pred'])['overall'], t14_calculate_metrics(df['t'], df['1221_test_pred_zs'])['overall'], t14_calculate_metrics(df['t'], df['1221_test_pred_rules'])['overall'], t14_calculate_metrics(df['t'], df['1221_test_pred_memory_only'])['overall']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"1221_result_mixtral.csv\")\n",
    "for i in range(0, 790, 10):\n",
    "    sub_df = df[i:i+10]\n",
    "    print(len(sub_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from src.metrics import *\n",
    "\n",
    "data_df = pd.read_csv(\"1221_result_mixtral.csv\")\n",
    "for i in range(0, 790, 80):\n",
    "    df = data_df[i:i+80]\n",
    "    main = t14_calculate_metrics2(df['t'],df['1221_test_pred'])['overall']\n",
    "    zs = t14_calculate_metrics2(df['t'], df['1221_test_pred_zs'])['overall']\n",
    "    rules = t14_calculate_metrics2(df['t'], df['1221_test_pred_rules'])['overall']\n",
    "    memory = t14_calculate_metrics2(df['t'], df['1221_test_pred_memory_only'])['overall']\n",
    "    print(f\"macro F1: {main['macro_f1']:.3f} ({zs['macro_f1']:.3f}) ({rules['macro_f1']:.3f}) ({memory['macro_f1']:.3f})\")\n",
    "    print(f\"micro F1: {main['micro_f1']:.3f} ({zs['micro_f1']:.3f}) ({rules['micro_f1']:.3f}) ({memory['micro_f1']:.3f})\")\n",
    "    print(f\"weighted F1: {main['weighted_f1']:.3f} ({zs['weighted_f1']:.3f}) ({rules['weighted_f1']:.3f}) ({memory['weighted_f1']:.3f})\")\n",
    "    print(\"-----------------\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from src.metrics import *\n",
    "data_df = pd.read_csv(\"1222_result_llama_4_30.csv\")\n",
    "data_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"/home/yl3427/cylab/selfCorrectionAgent/results_t_stage_run_0.csv\")\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"1222_result_llama_4_30.csv\")\n",
    "\n",
    "main = t14_calculate_metrics2(df['t'],df['ltm_test_pred'])['overall']\n",
    "zs = t14_calculate_metrics2(df['t'], df['ltm_test_pred_zs'])['overall']\n",
    "rules = t14_calculate_metrics2(df['t'], df['ltm_test_pred_rules'])['overall']\n",
    "memory = t14_calculate_metrics2(df['t'], df['ltm_test_pred_memory_only'])['overall']\n",
    "print(f\"macro F1: {main['macro_f1']:.3f} ({zs['macro_f1']:.3f}) ({rules['macro_f1']:.3f}) ({memory['macro_f1']:.3f})\")\n",
    "print(f\"micro F1: {main['micro_f1']:.3f} ({zs['micro_f1']:.3f}) ({rules['micro_f1']:.3f}) ({memory['micro_f1']:.3f})\")\n",
    "print(f\"weighted F1: {main['weighted_f1']:.3f} ({zs['weighted_f1']:.3f}) ({rules['weighted_f1']:.3f}) ({memory['weighted_f1']:.3f})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from src.metrics import *\n",
    "\n",
    "data_df = pd.read_csv(\"1222_result_llama_4_30.csv\")\n",
    "for i in range(0, 790, 80):\n",
    "    df = data_df[i:i+80]\n",
    "    main = t14_calculate_metrics2(df['t'],df['ltm_test_pred'])['overall']\n",
    "    zs = t14_calculate_metrics2(df['t'], df['ltm_test_pred_zs'])['overall']\n",
    "    rules = t14_calculate_metrics2(df['t'], df['ltm_test_pred_rules'])['overall']\n",
    "    memory = t14_calculate_metrics2(df['t'], df['ltm_test_pred_memory_only'])['overall']\n",
    "    print(f\"macro F1: {main['macro_f1']:.3f} ({zs['macro_f1']:.3f}) ({rules['macro_f1']:.3f}) ({memory['macro_f1']:.3f})\")\n",
    "    print(f\"micro F1: {main['micro_f1']:.3f} ({zs['micro_f1']:.3f}) ({rules['micro_f1']:.3f}) ({memory['micro_f1']:.3f})\")\n",
    "    print(f\"weighted F1: {main['weighted_f1']:.3f} ({zs['weighted_f1']:.3f}) ({rules['weighted_f1']:.3f}) ({memory['weighted_f1']:.3f})\")\n",
    "    print(\"-----------------\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from src.metrics import *\n",
    "\n",
    "data_df = pd.read_csv(\"1222_result_llama_1_1.csv\")\n",
    "for i in range(0, 790, 80):\n",
    "    df = data_df[i:i+80]\n",
    "    main = t14_calculate_metrics2(df['t'],df['ltm_test_pred'])['overall']\n",
    "    zs = t14_calculate_metrics2(df['t'], df['ltm_test_pred_zs'])['overall']\n",
    "    rules = t14_calculate_metrics2(df['t'], df['ltm_test_pred_rules'])['overall']\n",
    "    memory = t14_calculate_metrics2(df['t'], df['ltm_test_pred_memory_only'])['overall']\n",
    "    print(f\"macro F1: {main['macro_f1']:.3f} ({zs['macro_f1']:.3f}) ({rules['macro_f1']:.3f}) ({memory['macro_f1']:.3f})\")\n",
    "    print(f\"micro F1: {main['micro_f1']:.3f} ({zs['micro_f1']:.3f}) ({rules['micro_f1']:.3f}) ({memory['micro_f1']:.3f})\")\n",
    "    print(f\"weighted F1: {main['weighted_f1']:.3f} ({zs['weighted_f1']:.3f}) ({rules['weighted_f1']:.3f}) ({memory['weighted_f1']:.3f})\")\n",
    "    print(\"-----------------\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"1222_result_llama_1_1.csv\")\n",
    "\n",
    "main = t14_calculate_metrics2(df['t'],df['ltm_test_pred'])['overall']\n",
    "zs = t14_calculate_metrics2(df['t'], df['ltm_test_pred_zs'])['overall']\n",
    "rules = t14_calculate_metrics2(df['t'], df['ltm_test_pred_rules'])['overall']\n",
    "memory = t14_calculate_metrics2(df['t'], df['ltm_test_pred_memory_only'])['overall']\n",
    "print(f\"macro F1: {main['macro_f1']:.3f} ({zs['macro_f1']:.3f}) ({rules['macro_f1']:.3f}) ({memory['macro_f1']:.3f})\")\n",
    "print(f\"micro F1: {main['micro_f1']:.3f} ({zs['micro_f1']:.3f}) ({rules['micro_f1']:.3f}) ({memory['micro_f1']:.3f})\")\n",
    "print(f\"weighted F1: {main['weighted_f1']:.3f} ({zs['weighted_f1']:.3f}) ({rules['weighted_f1']:.3f}) ({memory['weighted_f1']:.3f})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"1223_result_mixtral_1_1.csv\")\n",
    "\n",
    "main = t14_calculate_metrics2(df['t'],df['ltm_test_pred'])['overall']\n",
    "zs = t14_calculate_metrics2(df['t'], df['ltm_test_pred_zs'])['overall']\n",
    "rules = t14_calculate_metrics2(df['t'], df['ltm_test_pred_rules'])['overall']\n",
    "memory = t14_calculate_metrics2(df['t'], df['ltm_test_pred_memory_only'])['overall']\n",
    "print(f\"macro F1: {main['macro_f1']:.3f} ({zs['macro_f1']:.3f}) ({rules['macro_f1']:.3f}) ({memory['macro_f1']:.3f})\")\n",
    "print(f\"micro F1: {main['micro_f1']:.3f} ({zs['micro_f1']:.3f}) ({rules['micro_f1']:.3f}) ({memory['micro_f1']:.3f})\")\n",
    "print(f\"weighted F1: {main['weighted_f1']:.3f} ({zs['weighted_f1']:.3f}) ({rules['weighted_f1']:.3f}) ({memory['weighted_f1']:.3f})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"/home/yl3427/cylab/selfCorrectionAgent/results_n_stage_run_0.csv\")\n",
    "main_1 = 'experiment_n_stage_pred_llm_mem'\n",
    "main_2 = 'experiment_n_stage_pred_rag_mem'\n",
    "zs = 'experiment_n_stage_pred_zs'\n",
    "rules_1 = 'experiment_n_stage_pred_llm_rulesonly'\n",
    "rules_2 = 'experiment_n_stage_pred_rag_rulesonly'\n",
    "memory_1 = 'experiment_n_stage_pred_memonly'\n",
    "memory_2 = 'experiment_n_stage_pred_memonly2'\n",
    "rag = 'experiment_n_stage_pred_rag_only'\n",
    "\n",
    "main_1 = n03_calculate_metrics2(df['n'],df[main_1])['overall']\n",
    "main_2 = n03_calculate_metrics2(df['n'],df[main_2])['overall']\n",
    "zs = n03_calculate_metrics2(df['n'], df[zs])['overall']\n",
    "rules_1 = n03_calculate_metrics2(df['n'], df[rules_1])['overall']\n",
    "rules_2 = n03_calculate_metrics2(df['n'], df[rules_2])['overall']\n",
    "memory_1 = n03_calculate_metrics2(df['n'], df[memory_1])['overall']\n",
    "memory_2 = n03_calculate_metrics2(df['n'], df[memory_2])['overall']\n",
    "rag = n03_calculate_metrics2(df['n'], df[rag])['overall']\n",
    "\n",
    "print(f\"main_1: {main_1['macro_f1']}, {main_1['micro_f1']}, {main_1['weighted_f1']}\")\n",
    "print(f\"main_2: {main_2['macro_f1']}, {main_2['micro_f1']}, {main_2['weighted_f1']}\")\n",
    "print(f\"zs: {zs['macro_f1']}, {zs['micro_f1']}, {zs['weighted_f1']}\")\n",
    "print(f\"rules_1: {rules_1['macro_f1']}, {rules_1['micro_f1']}, {rules_1['weighted_f1']}\")\n",
    "print(f\"rules_2: {rules_2['macro_f1']}, {rules_2['micro_f1']}, {rules_2['weighted_f1']}\")\n",
    "print(f\"memory_1: {memory_1['macro_f1']}, {memory_1['micro_f1']}, {memory_1['weighted_f1']}\")\n",
    "print(f\"memory_2: {memory_2['macro_f1']}, {memory_2['micro_f1']}, {memory_2['weighted_f1']}\")\n",
    "print(f\"rag: {rag['macro_f1']}, {rag['micro_f1']}, {rag['weighted_f1']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tabulate import tabulate\n",
    "\n",
    "full_df = pd.read_csv(\"/home/yl3427/cylab/selfCorrectionAgent/results_n_stage_run_0.csv\")\n",
    "for i in range(0, 790, 40):\n",
    "    df = full_df[i:i+40]\n",
    "\n",
    "    main_1 = 'experiment_n_stage_pred_llm_mem'\n",
    "    main_2 = 'experiment_n_stage_pred_rag_mem'\n",
    "    zs = 'experiment_n_stage_pred_zs'\n",
    "    rules_1 = 'experiment_n_stage_pred_llm_rulesonly'\n",
    "    rules_2 = 'experiment_n_stage_pred_rag_rulesonly'\n",
    "    memory_1 = 'experiment_n_stage_pred_memonly'\n",
    "    memory_2 = 'experiment_n_stage_pred_memonly2'\n",
    "    rag = 'experiment_n_stage_pred_rag_only'\n",
    "\n",
    "    main_1 = n03_calculate_metrics2(df['n'],df[main_1])['overall']\n",
    "    main_2 = n03_calculate_metrics2(df['n'],df[main_2])['overall']\n",
    "    zs = n03_calculate_metrics2(df['n'], df[zs])['overall']\n",
    "    rules_1 = n03_calculate_metrics2(df['n'], df[rules_1])['overall']\n",
    "    rules_2 = n03_calculate_metrics2(df['n'], df[rules_2])['overall']\n",
    "    memory_1 = n03_calculate_metrics2(df['n'], df[memory_1])['overall']\n",
    "    memory_2 = n03_calculate_metrics2(df['n'], df[memory_2])['overall']\n",
    "    rag = n03_calculate_metrics2(df['n'], df[rag])['overall']\n",
    "\n",
    "    table_data = [\n",
    "        [\"main_1\",   main_1['macro_f1'],   main_1['micro_f1'],   main_1['weighted_f1']],\n",
    "        [\"main_2\",   main_2['macro_f1'],   main_2['micro_f1'],   main_2['weighted_f1']],\n",
    "        [\"zs\",       zs['macro_f1'],       zs['micro_f1'],       zs['weighted_f1']],\n",
    "        [\"rules_1\",  rules_1['macro_f1'],  rules_1['micro_f1'],  rules_1['weighted_f1']],\n",
    "        [\"rules_2\",  rules_2['macro_f1'],  rules_2['micro_f1'],  rules_2['weighted_f1']],\n",
    "        [\"memory_1\", memory_1['macro_f1'], memory_1['micro_f1'], memory_1['weighted_f1']],\n",
    "        [\"memory_2\", memory_2['macro_f1'], memory_2['micro_f1'], memory_2['weighted_f1']],\n",
    "        [\"rag\",      rag['macro_f1'],      rag['micro_f1'],      rag['weighted_f1']],\n",
    "    ]\n",
    "\n",
    "    print(tabulate(\n",
    "        table_data,\n",
    "        headers=[\"Method\", \"Macro F1\", \"Micro F1\", \"Weighted F1\"],\n",
    "        floatfmt=\".4f\",  # 4 decimal places\n",
    "        tablefmt=\"github\"\n",
    "    ))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"/home/yl3427/cylab/selfCorrectionAgent/results_n_stage_run_4.csv\")\n",
    "main_1 = 'experiment_n_stage_pred_llm_mem'\n",
    "main_2 = 'experiment_n_stage_pred_rag_mem'\n",
    "zs = 'experiment_n_stage_pred_zs'\n",
    "rules_1 = 'experiment_n_stage_pred_llm_rulesonly'\n",
    "rules_2 = 'experiment_n_stage_pred_rag_rulesonly'\n",
    "memory_1 = 'experiment_n_stage_pred_memonly'\n",
    "memory_2 = 'experiment_n_stage_pred_memonly2'\n",
    "rag = 'experiment_n_stage_pred_rag_only'\n",
    "\n",
    "main_1 = n03_calculate_metrics2(df['n'],df[main_1])['overall']\n",
    "main_2 = n03_calculate_metrics2(df['n'],df[main_2])['overall']\n",
    "zs = n03_calculate_metrics2(df['n'], df[zs])['overall']\n",
    "rules_1 = n03_calculate_metrics2(df['n'], df[rules_1])['overall']\n",
    "rules_2 = n03_calculate_metrics2(df['n'], df[rules_2])['overall']\n",
    "memory_1 = n03_calculate_metrics2(df['n'], df[memory_1])['overall']\n",
    "memory_2 = n03_calculate_metrics2(df['n'], df[memory_2])['overall']\n",
    "rag = n03_calculate_metrics2(df['n'], df[rag])['overall']\n",
    "\n",
    "print(f\"main_1: {main_1['macro_f1']}, {main_1['micro_f1']}, {main_1['weighted_f1']}\")\n",
    "print(f\"main_2: {main_2['macro_f1']}, {main_2['micro_f1']}, {main_2['weighted_f1']}\")\n",
    "print(f\"zs: {zs['macro_f1']}, {zs['micro_f1']}, {zs['weighted_f1']}\")\n",
    "print(f\"rules_1: {rules_1['macro_f1']}, {rules_1['micro_f1']}, {rules_1['weighted_f1']}\")\n",
    "print(f\"rules_2: {rules_2['macro_f1']}, {rules_2['micro_f1']}, {rules_2['weighted_f1']}\")\n",
    "print(f\"memory_1: {memory_1['macro_f1']}, {memory_1['micro_f1']}, {memory_1['weighted_f1']}\")\n",
    "print(f\"memory_2: {memory_2['macro_f1']}, {memory_2['micro_f1']}, {memory_2['weighted_f1']}\")\n",
    "print(f\"rag: {rag['macro_f1']}, {rag['micro_f1']}, {rag['weighted_f1']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"/home/yl3427/cylab/selfCorrectionAgent/results_t_stage_run_0.csv\")\n",
    "main_1 = 'experiment_t_stage_pred_llm_mem'\n",
    "main_2 = 'experiment_t_stage_pred_rag_mem'\n",
    "zs = 'experiment_t_stage_pred_zs'\n",
    "rules_1 = 'experiment_t_stage_pred_llm_rulesonly'\n",
    "rules_2 = 'experiment_t_stage_pred_rag_rulesonly'\n",
    "memory_1 = 'experiment_t_stage_pred_memonly'\n",
    "memory_2 = 'experiment_t_stage_pred_memonly2'\n",
    "rag = 'experiment_t_stage_pred_rag_only'\n",
    "\n",
    "main_1 = t14_calculate_metrics2(df['t'],df[main_1])['overall']\n",
    "main_2 = t14_calculate_metrics2(df['t'],df[main_2])['overall']\n",
    "zs = t14_calculate_metrics2(df['t'], df[zs])['overall']\n",
    "rules_1 = t14_calculate_metrics2(df['t'], df[rules_1])['overall']\n",
    "rules_2 = t14_calculate_metrics2(df['t'], df[rules_2])['overall']\n",
    "memory_1 = t14_calculate_metrics2(df['t'], df[memory_1])['overall']\n",
    "memory_2 = t14_calculate_metrics2(df['t'], df[memory_2])['overall']\n",
    "rag = t14_calculate_metrics2(df['t'], df[rag])['overall']\n",
    "\n",
    "print(f\"main_1: {main_1['macro_f1']}, {main_1['micro_f1']}, {main_1['weighted_f1']}\")\n",
    "print(f\"main_2: {main_2['macro_f1']}, {main_2['micro_f1']}, {main_2['weighted_f1']}\")\n",
    "print(f\"zs: {zs['macro_f1']}, {zs['micro_f1']}, {zs['weighted_f1']}\")\n",
    "print(f\"rules_1: {rules_1['macro_f1']}, {rules_1['micro_f1']}, {rules_1['weighted_f1']}\")\n",
    "print(f\"rules_2: {rules_2['macro_f1']}, {rules_2['micro_f1']}, {rules_2['weighted_f1']}\")\n",
    "print(f\"memory_1: {memory_1['macro_f1']}, {memory_1['micro_f1']}, {memory_1['weighted_f1']}\")\n",
    "print(f\"memory_2: {memory_2['macro_f1']}, {memory_2['micro_f1']}, {memory_2['weighted_f1']}\")\n",
    "print(f\"rag: {rag['macro_f1']}, {rag['micro_f1']}, {rag['weighted_f1']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"/home/yl3427/cylab/selfCorrectionAgent/results_t_stage_run_1.csv\")\n",
    "main_1 = 'experiment_t_stage_pred_llm_mem'\n",
    "main_2 = 'experiment_t_stage_pred_rag_mem'\n",
    "zs = 'experiment_t_stage_pred_zs'\n",
    "rules_1 = 'experiment_t_stage_pred_llm_rulesonly'\n",
    "rules_2 = 'experiment_t_stage_pred_rag_rulesonly'\n",
    "memory_1 = 'experiment_t_stage_pred_memonly'\n",
    "memory_2 = 'experiment_t_stage_pred_memonly2'\n",
    "rag = 'experiment_t_stage_pred_rag_only'\n",
    "\n",
    "main_1 = t14_calculate_metrics2(df['t'],df[main_1])['overall']\n",
    "main_2 = t14_calculate_metrics2(df['t'],df[main_2])['overall']\n",
    "zs = t14_calculate_metrics2(df['t'], df[zs])['overall']\n",
    "rules_1 = t14_calculate_metrics2(df['t'], df[rules_1])['overall']\n",
    "rules_2 = t14_calculate_metrics2(df['t'], df[rules_2])['overall']\n",
    "memory_1 = t14_calculate_metrics2(df['t'], df[memory_1])['overall']\n",
    "memory_2 = t14_calculate_metrics2(df['t'], df[memory_2])['overall']\n",
    "rag = t14_calculate_metrics2(df['t'], df[rag])['overall']\n",
    "\n",
    "print(f\"main_1: {main_1['macro_f1']}, {main_1['micro_f1']}, {main_1['weighted_f1']}\")\n",
    "print(f\"main_2: {main_2['macro_f1']}, {main_2['micro_f1']}, {main_2['weighted_f1']}\")\n",
    "print(f\"zs: {zs['macro_f1']}, {zs['micro_f1']}, {zs['weighted_f1']}\")\n",
    "print(f\"rules_1: {rules_1['macro_f1']}, {rules_1['micro_f1']}, {rules_1['weighted_f1']}\")\n",
    "print(f\"rules_2: {rules_2['macro_f1']}, {rules_2['micro_f1']}, {rules_2['weighted_f1']}\")\n",
    "print(f\"memory_1: {memory_1['macro_f1']}, {memory_1['micro_f1']}, {memory_1['weighted_f1']}\")\n",
    "print(f\"memory_2: {memory_2['macro_f1']}, {memory_2['micro_f1']}, {memory_2['weighted_f1']}\")\n",
    "print(f\"rag: {rag['macro_f1']}, {rag['micro_f1']}, {rag['weighted_f1']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"/home/yl3427/cylab/selfCorrectionAgent/results_t_stage_run_2.csv\")\n",
    "main_1 = 'experiment_t_stage_pred_llm_mem'\n",
    "main_2 = 'experiment_t_stage_pred_rag_mem'\n",
    "zs = 'experiment_t_stage_pred_zs'\n",
    "rules_1 = 'experiment_t_stage_pred_llm_rulesonly'\n",
    "rules_2 = 'experiment_t_stage_pred_rag_rulesonly'\n",
    "memory_1 = 'experiment_t_stage_pred_memonly'\n",
    "memory_2 = 'experiment_t_stage_pred_memonly2'\n",
    "rag = 'experiment_t_stage_pred_rag_only'\n",
    "\n",
    "main_1 = t14_calculate_metrics2(df['t'],df[main_1])['overall']\n",
    "main_2 = t14_calculate_metrics2(df['t'],df[main_2])['overall']\n",
    "zs = t14_calculate_metrics2(df['t'], df[zs])['overall']\n",
    "rules_1 = t14_calculate_metrics2(df['t'], df[rules_1])['overall']\n",
    "rules_2 = t14_calculate_metrics2(df['t'], df[rules_2])['overall']\n",
    "memory_1 = t14_calculate_metrics2(df['t'], df[memory_1])['overall']\n",
    "memory_2 = t14_calculate_metrics2(df['t'], df[memory_2])['overall']\n",
    "rag = t14_calculate_metrics2(df['t'], df[rag])['overall']\n",
    "\n",
    "print(f\"main_1: {main_1['macro_f1']}, {main_1['micro_f1']}, {main_1['weighted_f1']}\")\n",
    "print(f\"main_2: {main_2['macro_f1']}, {main_2['micro_f1']}, {main_2['weighted_f1']}\")\n",
    "print(f\"zs: {zs['macro_f1']}, {zs['micro_f1']}, {zs['weighted_f1']}\")\n",
    "print(f\"rules_1: {rules_1['macro_f1']}, {rules_1['micro_f1']}, {rules_1['weighted_f1']}\")\n",
    "print(f\"rules_2: {rules_2['macro_f1']}, {rules_2['micro_f1']}, {rules_2['weighted_f1']}\")\n",
    "print(f\"memory_1: {memory_1['macro_f1']}, {memory_1['micro_f1']}, {memory_1['weighted_f1']}\")\n",
    "print(f\"memory_2: {memory_2['macro_f1']}, {memory_2['micro_f1']}, {memory_2['weighted_f1']}\")\n",
    "print(f\"rag: {rag['macro_f1']}, {rag['micro_f1']}, {rag['weighted_f1']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"/home/yl3427/cylab/selfCorrectionAgent/results_t_stage_run_3.csv\")\n",
    "main_1 = 'experiment_t_stage_pred_llm_mem'\n",
    "main_2 = 'experiment_t_stage_pred_rag_mem'\n",
    "zs = 'experiment_t_stage_pred_zs'\n",
    "rules_1 = 'experiment_t_stage_pred_llm_rulesonly'\n",
    "rules_2 = 'experiment_t_stage_pred_rag_rulesonly'\n",
    "memory_1 = 'experiment_t_stage_pred_memonly'\n",
    "memory_2 = 'experiment_t_stage_pred_memonly2'\n",
    "rag = 'experiment_t_stage_pred_rag_only'\n",
    "\n",
    "main_1 = t14_calculate_metrics2(df['t'],df[main_1])['overall']\n",
    "main_2 = t14_calculate_metrics2(df['t'],df[main_2])['overall']\n",
    "zs = t14_calculate_metrics2(df['t'], df[zs])['overall']\n",
    "rules_1 = t14_calculate_metrics2(df['t'], df[rules_1])['overall']\n",
    "rules_2 = t14_calculate_metrics2(df['t'], df[rules_2])['overall']\n",
    "memory_1 = t14_calculate_metrics2(df['t'], df[memory_1])['overall']\n",
    "memory_2 = t14_calculate_metrics2(df['t'], df[memory_2])['overall']\n",
    "rag = t14_calculate_metrics2(df['t'], df[rag])['overall']\n",
    "\n",
    "print(f\"main_1: {main_1['macro_f1']}, {main_1['micro_f1']}, {main_1['weighted_f1']}\")\n",
    "print(f\"main_2: {main_2['macro_f1']}, {main_2['micro_f1']}, {main_2['weighted_f1']}\")\n",
    "print(f\"zs: {zs['macro_f1']}, {zs['micro_f1']}, {zs['weighted_f1']}\")\n",
    "print(f\"rules_1: {rules_1['macro_f1']}, {rules_1['micro_f1']}, {rules_1['weighted_f1']}\")\n",
    "print(f\"rules_2: {rules_2['macro_f1']}, {rules_2['micro_f1']}, {rules_2['weighted_f1']}\")\n",
    "print(f\"memory_1: {memory_1['macro_f1']}, {memory_1['micro_f1']}, {memory_1['weighted_f1']}\")\n",
    "print(f\"memory_2: {memory_2['macro_f1']}, {memory_2['micro_f1']}, {memory_2['weighted_f1']}\")\n",
    "print(f\"rag: {rag['macro_f1']}, {rag['micro_f1']}, {rag['weighted_f1']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"/secure/shared_data/rag_tnm_results/summary/5_folds_summary/luad_df.csv\")\n",
    "df.n.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"/home/yl3427/cylab/selfCorrectionAgent/results_t_stage_run_0_lung.csv\")\n",
    "main_1 = 'experiment_t_stage_pred_llm_mem'\n",
    "main_2 = 'experiment_t_stage_pred_rag_mem'\n",
    "zs = 'experiment_t_stage_pred_zs'\n",
    "rules_1 = 'experiment_t_stage_pred_llm_rulesonly'\n",
    "rules_2 = 'experiment_t_stage_pred_rag_rulesonly'\n",
    "memory_1 = 'experiment_t_stage_pred_memonly'\n",
    "memory_2 = 'experiment_t_stage_pred_memonly2'\n",
    "rag = 'experiment_t_stage_pred_rag_only'\n",
    "\n",
    "main_1 = t14_calculate_metrics2(df['t'],df[main_1])['overall']\n",
    "main_2 = t14_calculate_metrics2(df['t'],df[main_2])['overall']\n",
    "zs = t14_calculate_metrics2(df['t'], df[zs])['overall']\n",
    "rules_1 = t14_calculate_metrics2(df['t'], df[rules_1])['overall']\n",
    "rules_2 = t14_calculate_metrics2(df['t'], df[rules_2])['overall']\n",
    "memory_1 = t14_calculate_metrics2(df['t'], df[memory_1])['overall']\n",
    "memory_2 = t14_calculate_metrics2(df['t'], df[memory_2])['overall']\n",
    "rag = t14_calculate_metrics2(df['t'], df[rag])['overall']\n",
    "\n",
    "print(f\"main_1: {main_1['macro_f1']}, {main_1['micro_f1']}, {main_1['weighted_f1']}\")\n",
    "print(f\"main_2: {main_2['macro_f1']}, {main_2['micro_f1']}, {main_2['weighted_f1']}\")\n",
    "print(f\"zs: {zs['macro_f1']}, {zs['micro_f1']}, {zs['weighted_f1']}\")\n",
    "print(f\"rules_1: {rules_1['macro_f1']}, {rules_1['micro_f1']}, {rules_1['weighted_f1']}\")\n",
    "print(f\"rules_2: {rules_2['macro_f1']}, {rules_2['micro_f1']}, {rules_2['weighted_f1']}\")\n",
    "print(f\"memory_1: {memory_1['macro_f1']}, {memory_1['micro_f1']}, {memory_1['weighted_f1']}\")\n",
    "print(f\"memory_2: {memory_2['macro_f1']}, {memory_2['micro_f1']}, {memory_2['weighted_f1']}\")\n",
    "print(f\"rag: {rag['macro_f1']}, {rag['micro_f1']}, {rag['weighted_f1']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"/home/yl3427/cylab/selfCorrectionAgent/results_t_stage_run_1_lung.csv\")\n",
    "main_1 = 'experiment_t_stage_pred_llm_mem'\n",
    "main_2 = 'experiment_t_stage_pred_rag_mem'\n",
    "zs = 'experiment_t_stage_pred_zs'\n",
    "rules_1 = 'experiment_t_stage_pred_llm_rulesonly'\n",
    "rules_2 = 'experiment_t_stage_pred_rag_rulesonly'\n",
    "memory_1 = 'experiment_t_stage_pred_memonly'\n",
    "memory_2 = 'experiment_t_stage_pred_memonly2'\n",
    "rag = 'experiment_t_stage_pred_rag_only'\n",
    "\n",
    "main_1 = t14_calculate_metrics2(df['t'],df[main_1])['overall']\n",
    "main_2 = t14_calculate_metrics2(df['t'],df[main_2])['overall']\n",
    "zs = t14_calculate_metrics2(df['t'], df[zs])['overall']\n",
    "rules_1 = t14_calculate_metrics2(df['t'], df[rules_1])['overall']\n",
    "rules_2 = t14_calculate_metrics2(df['t'], df[rules_2])['overall']\n",
    "memory_1 = t14_calculate_metrics2(df['t'], df[memory_1])['overall']\n",
    "memory_2 = t14_calculate_metrics2(df['t'], df[memory_2])['overall']\n",
    "rag = t14_calculate_metrics2(df['t'], df[rag])['overall']\n",
    "\n",
    "print(f\"main_1: {main_1['macro_f1']}, {main_1['micro_f1']}, {main_1['weighted_f1']}\")\n",
    "print(f\"main_2: {main_2['macro_f1']}, {main_2['micro_f1']}, {main_2['weighted_f1']}\")\n",
    "print(f\"zs: {zs['macro_f1']}, {zs['micro_f1']}, {zs['weighted_f1']}\")\n",
    "print(f\"rules_1: {rules_1['macro_f1']}, {rules_1['micro_f1']}, {rules_1['weighted_f1']}\")\n",
    "print(f\"rules_2: {rules_2['macro_f1']}, {rules_2['micro_f1']}, {rules_2['weighted_f1']}\")\n",
    "print(f\"memory_1: {memory_1['macro_f1']}, {memory_1['micro_f1']}, {memory_1['weighted_f1']}\")\n",
    "print(f\"memory_2: {memory_2['macro_f1']}, {memory_2['micro_f1']}, {memory_2['weighted_f1']}\")\n",
    "print(f\"rag: {rag['macro_f1']}, {rag['micro_f1']}, {rag['weighted_f1']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"/home/yl3427/cylab/selfCorrectionAgent/results_t_stage_run_2_lung.csv\")\n",
    "main_1 = 'experiment_t_stage_pred_llm_mem'\n",
    "main_2 = 'experiment_t_stage_pred_rag_mem'\n",
    "zs = 'experiment_t_stage_pred_zs'\n",
    "rules_1 = 'experiment_t_stage_pred_llm_rulesonly'\n",
    "rules_2 = 'experiment_t_stage_pred_rag_rulesonly'\n",
    "memory_1 = 'experiment_t_stage_pred_memonly'\n",
    "memory_2 = 'experiment_t_stage_pred_memonly2'\n",
    "rag = 'experiment_t_stage_pred_rag_only'\n",
    "\n",
    "main_1 = t14_calculate_metrics2(df['t'],df[main_1])['overall']\n",
    "main_2 = t14_calculate_metrics2(df['t'],df[main_2])['overall']\n",
    "zs = t14_calculate_metrics2(df['t'], df[zs])['overall']\n",
    "rules_1 = t14_calculate_metrics2(df['t'], df[rules_1])['overall']\n",
    "rules_2 = t14_calculate_metrics2(df['t'], df[rules_2])['overall']\n",
    "memory_1 = t14_calculate_metrics2(df['t'], df[memory_1])['overall']\n",
    "memory_2 = t14_calculate_metrics2(df['t'], df[memory_2])['overall']\n",
    "rag = t14_calculate_metrics2(df['t'], df[rag])['overall']\n",
    "\n",
    "print(f\"main_1: {main_1['macro_f1']}, {main_1['micro_f1']}, {main_1['weighted_f1']}\")\n",
    "print(f\"main_2: {main_2['macro_f1']}, {main_2['micro_f1']}, {main_2['weighted_f1']}\")\n",
    "print(f\"zs: {zs['macro_f1']}, {zs['micro_f1']}, {zs['weighted_f1']}\")\n",
    "print(f\"rules_1: {rules_1['macro_f1']}, {rules_1['micro_f1']}, {rules_1['weighted_f1']}\")\n",
    "print(f\"rules_2: {rules_2['macro_f1']}, {rules_2['micro_f1']}, {rules_2['weighted_f1']}\")\n",
    "print(f\"memory_1: {memory_1['macro_f1']}, {memory_1['micro_f1']}, {memory_1['weighted_f1']}\")\n",
    "print(f\"memory_2: {memory_2['macro_f1']}, {memory_2['micro_f1']}, {memory_2['weighted_f1']}\")\n",
    "print(f\"rag: {rag['macro_f1']}, {rag['micro_f1']}, {rag['weighted_f1']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"/home/yl3427/cylab/selfCorrectionAgent/results_t_stage_run_3_lung.csv\")\n",
    "main_1 = 'experiment_t_stage_pred_llm_mem'\n",
    "main_2 = 'experiment_t_stage_pred_rag_mem'\n",
    "zs = 'experiment_t_stage_pred_zs'\n",
    "rules_1 = 'experiment_t_stage_pred_llm_rulesonly'\n",
    "rules_2 = 'experiment_t_stage_pred_rag_rulesonly'\n",
    "memory_1 = 'experiment_t_stage_pred_memonly'\n",
    "memory_2 = 'experiment_t_stage_pred_memonly2'\n",
    "rag = 'experiment_t_stage_pred_rag_only'\n",
    "\n",
    "main_1 = t14_calculate_metrics2(df['t'],df[main_1])['overall']\n",
    "main_2 = t14_calculate_metrics2(df['t'],df[main_2])['overall']\n",
    "zs = t14_calculate_metrics2(df['t'], df[zs])['overall']\n",
    "rules_1 = t14_calculate_metrics2(df['t'], df[rules_1])['overall']\n",
    "rules_2 = t14_calculate_metrics2(df['t'], df[rules_2])['overall']\n",
    "memory_1 = t14_calculate_metrics2(df['t'], df[memory_1])['overall']\n",
    "memory_2 = t14_calculate_metrics2(df['t'], df[memory_2])['overall']\n",
    "rag = t14_calculate_metrics2(df['t'], df[rag])['overall']\n",
    "\n",
    "print(f\"main_1: {main_1['macro_f1']}, {main_1['micro_f1']}, {main_1['weighted_f1']}\")\n",
    "print(f\"main_2: {main_2['macro_f1']}, {main_2['micro_f1']}, {main_2['weighted_f1']}\")\n",
    "print(f\"zs: {zs['macro_f1']}, {zs['micro_f1']}, {zs['weighted_f1']}\")\n",
    "print(f\"rules_1: {rules_1['macro_f1']}, {rules_1['micro_f1']}, {rules_1['weighted_f1']}\")\n",
    "print(f\"rules_2: {rules_2['macro_f1']}, {rules_2['micro_f1']}, {rules_2['weighted_f1']}\")\n",
    "print(f\"memory_1: {memory_1['macro_f1']}, {memory_1['micro_f1']}, {memory_1['weighted_f1']}\")\n",
    "print(f\"memory_2: {memory_2['macro_f1']}, {memory_2['micro_f1']}, {memory_2['weighted_f1']}\")\n",
    "print(f\"rag: {rag['macro_f1']}, {rag['micro_f1']}, {rag['weighted_f1']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"/home/yl3427/cylab/selfCorrectionAgent/results_t_stage_run_4_lung.csv\")\n",
    "main_1 = 'experiment_t_stage_pred_llm_mem'\n",
    "main_2 = 'experiment_t_stage_pred_rag_mem'\n",
    "zs = 'experiment_t_stage_pred_zs'\n",
    "rules_1 = 'experiment_t_stage_pred_llm_rulesonly'\n",
    "rules_2 = 'experiment_t_stage_pred_rag_rulesonly'\n",
    "memory_1 = 'experiment_t_stage_pred_memonly'\n",
    "memory_2 = 'experiment_t_stage_pred_memonly2'\n",
    "rag = 'experiment_t_stage_pred_rag_only'\n",
    "\n",
    "main_1 = t14_calculate_metrics2(df['t'],df[main_1])['overall']\n",
    "main_2 = t14_calculate_metrics2(df['t'],df[main_2])['overall']\n",
    "zs = t14_calculate_metrics2(df['t'], df[zs])['overall']\n",
    "rules_1 = t14_calculate_metrics2(df['t'], df[rules_1])['overall']\n",
    "rules_2 = t14_calculate_metrics2(df['t'], df[rules_2])['overall']\n",
    "memory_1 = t14_calculate_metrics2(df['t'], df[memory_1])['overall']\n",
    "memory_2 = t14_calculate_metrics2(df['t'], df[memory_2])['overall']\n",
    "rag = t14_calculate_metrics2(df['t'], df[rag])['overall']\n",
    "\n",
    "print(f\"main_1: {main_1['macro_f1']}, {main_1['micro_f1']}, {main_1['weighted_f1']}\")\n",
    "print(f\"main_2: {main_2['macro_f1']}, {main_2['micro_f1']}, {main_2['weighted_f1']}\")\n",
    "print(f\"zs: {zs['macro_f1']}, {zs['micro_f1']}, {zs['weighted_f1']}\")\n",
    "print(f\"rules_1: {rules_1['macro_f1']}, {rules_1['micro_f1']}, {rules_1['weighted_f1']}\")\n",
    "print(f\"rules_2: {rules_2['macro_f1']}, {rules_2['micro_f1']}, {rules_2['weighted_f1']}\")\n",
    "print(f\"memory_1: {memory_1['macro_f1']}, {memory_1['micro_f1']}, {memory_1['weighted_f1']}\")\n",
    "print(f\"memory_2: {memory_2['macro_f1']}, {memory_2['micro_f1']}, {memory_2['weighted_f1']}\")\n",
    "print(f\"rag: {rag['macro_f1']}, {rag['micro_f1']}, {rag['weighted_f1']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"/home/yl3427/cylab/selfCorrectionAgent/results_n_stage_run_0_lung.csv\")\n",
    "main_1 = 'experiment_n_stage_pred_llm_mem'\n",
    "main_2 = 'experiment_n_stage_pred_rag_mem'\n",
    "zs = 'experiment_n_stage_pred_zs'\n",
    "rules_1 = 'experiment_n_stage_pred_llm_rulesonly'\n",
    "rules_2 = 'experiment_n_stage_pred_rag_rulesonly'\n",
    "memory_1 = 'experiment_n_stage_pred_memonly'\n",
    "memory_2 = 'experiment_n_stage_pred_memonly2'\n",
    "rag = 'experiment_n_stage_pred_rag_only'\n",
    "\n",
    "main_1 = n03_calculate_metrics2(df['n'],df[main_1])['overall']\n",
    "main_2 = n03_calculate_metrics2(df['n'],df[main_2])['overall']\n",
    "zs = n03_calculate_metrics2(df['n'], df[zs])['overall']\n",
    "rules_1 = n03_calculate_metrics2(df['n'], df[rules_1])['overall']\n",
    "rules_2 = n03_calculate_metrics2(df['n'], df[rules_2])['overall']\n",
    "memory_1 = n03_calculate_metrics2(df['n'], df[memory_1])['overall']\n",
    "memory_2 = n03_calculate_metrics2(df['n'], df[memory_2])['overall']\n",
    "rag = n03_calculate_metrics2(df['n'], df[rag])['overall']\n",
    "\n",
    "print(f\"main_1: {main_1['macro_f1']}, {main_1['micro_f1']}, {main_1['weighted_f1']}\")\n",
    "print(f\"main_2: {main_2['macro_f1']}, {main_2['micro_f1']}, {main_2['weighted_f1']}\")\n",
    "print(f\"zs: {zs['macro_f1']}, {zs['micro_f1']}, {zs['weighted_f1']}\")\n",
    "print(f\"rules_1: {rules_1['macro_f1']}, {rules_1['micro_f1']}, {rules_1['weighted_f1']}\")\n",
    "print(f\"rules_2: {rules_2['macro_f1']}, {rules_2['micro_f1']}, {rules_2['weighted_f1']}\")\n",
    "print(f\"memory_1: {memory_1['macro_f1']}, {memory_1['micro_f1']}, {memory_1['weighted_f1']}\")\n",
    "print(f\"memory_2: {memory_2['macro_f1']}, {memory_2['micro_f1']}, {memory_2['weighted_f1']}\")\n",
    "print(f\"rag: {rag['macro_f1']}, {rag['micro_f1']}, {rag['weighted_f1']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"/home/yl3427/cylab/selfCorrectionAgent/results_n_stage_run_1_lung.csv\")\n",
    "main_1 = 'experiment_n_stage_pred_llm_mem'\n",
    "main_2 = 'experiment_n_stage_pred_rag_mem'\n",
    "zs = 'experiment_n_stage_pred_zs'\n",
    "rules_1 = 'experiment_n_stage_pred_llm_rulesonly'\n",
    "rules_2 = 'experiment_n_stage_pred_rag_rulesonly'\n",
    "memory_1 = 'experiment_n_stage_pred_memonly'\n",
    "memory_2 = 'experiment_n_stage_pred_memonly2'\n",
    "rag = 'experiment_n_stage_pred_rag_only'\n",
    "\n",
    "main_1 = n03_calculate_metrics2(df['n'],df[main_1])['overall']\n",
    "main_2 = n03_calculate_metrics2(df['n'],df[main_2])['overall']\n",
    "zs = n03_calculate_metrics2(df['n'], df[zs])['overall']\n",
    "rules_1 = n03_calculate_metrics2(df['n'], df[rules_1])['overall']\n",
    "rules_2 = n03_calculate_metrics2(df['n'], df[rules_2])['overall']\n",
    "memory_1 = n03_calculate_metrics2(df['n'], df[memory_1])['overall']\n",
    "memory_2 = n03_calculate_metrics2(df['n'], df[memory_2])['overall']\n",
    "rag = n03_calculate_metrics2(df['n'], df[rag])['overall']\n",
    "\n",
    "print(f\"main_1: {main_1['macro_f1']}, {main_1['micro_f1']}, {main_1['weighted_f1']}\")\n",
    "print(f\"main_2: {main_2['macro_f1']}, {main_2['micro_f1']}, {main_2['weighted_f1']}\")\n",
    "print(f\"zs: {zs['macro_f1']}, {zs['micro_f1']}, {zs['weighted_f1']}\")\n",
    "print(f\"rules_1: {rules_1['macro_f1']}, {rules_1['micro_f1']}, {rules_1['weighted_f1']}\")\n",
    "print(f\"rules_2: {rules_2['macro_f1']}, {rules_2['micro_f1']}, {rules_2['weighted_f1']}\")\n",
    "print(f\"memory_1: {memory_1['macro_f1']}, {memory_1['micro_f1']}, {memory_1['weighted_f1']}\")\n",
    "print(f\"memory_2: {memory_2['macro_f1']}, {memory_2['micro_f1']}, {memory_2['weighted_f1']}\")\n",
    "print(f\"rag: {rag['macro_f1']}, {rag['micro_f1']}, {rag['weighted_f1']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tabulate import tabulate\n",
    "print(\"T14: Breast Cancer\")\n",
    "print()\n",
    "for i in range(5):\n",
    "    df = pd.read_csv(f\"/home/yl3427/cylab/selfCorrectionAgent/result_breast/results_t_stage_run_{i}.csv\")\n",
    "    main_1 = 'experiment_t_stage_pred_llm_mem'\n",
    "    main_2 = 'experiment_t_stage_pred_rag_mem'\n",
    "    zs = 'experiment_t_stage_pred_zs'\n",
    "    rules_1 = 'experiment_t_stage_pred_llm_rulesonly'\n",
    "    rules_2 = 'experiment_t_stage_pred_rag_rulesonly'\n",
    "    memory_1 = 'experiment_t_stage_pred_memonly'\n",
    "    memory_2 = 'experiment_t_stage_pred_memonly2'\n",
    "    rag = 'experiment_t_stage_pred_rag_only'\n",
    "\n",
    "    main_1 = t14_calculate_metrics2(df['t'],df[main_1])['overall']\n",
    "    main_2 = t14_calculate_metrics2(df['t'],df[main_2])['overall']\n",
    "    zs = t14_calculate_metrics2(df['t'], df[zs])['overall']\n",
    "    rules_1 = t14_calculate_metrics2(df['t'], df[rules_1])['overall']\n",
    "    rules_2 = t14_calculate_metrics2(df['t'], df[rules_2])['overall']\n",
    "    memory_1 = t14_calculate_metrics2(df['t'], df[memory_1])['overall']\n",
    "    memory_2 = t14_calculate_metrics2(df['t'], df[memory_2])['overall']\n",
    "    rag = t14_calculate_metrics2(df['t'], df[rag])['overall']\n",
    "\n",
    "\n",
    "    table_data = [\n",
    "        [\"ZS\",       zs['macro_f1']],\n",
    "        [\"Rule1 (from nothing)\",  rules_1['macro_f1']],\n",
    "        [\"Memory1 (from Rule1)\", memory_1['macro_f1']],\n",
    "        [\"Rule1 + Memory1\",   main_1['macro_f1']],\n",
    "        [],\n",
    "        [\"RAG\",      rag['macro_f1']],\n",
    "        [\"Rule2 (from RAG excerpt)\",  rules_2['macro_f1']],\n",
    "        [\"Memory2 (from Rule2)\", memory_2['macro_f1']],\n",
    "        [\"Rule2 + Memory2\",   main_2['macro_f1']],\n",
    "    ]\n",
    "    print(f\"Run {i}\")\n",
    "    print(tabulate(\n",
    "        table_data,\n",
    "        headers=[\"Method\", \"Macro F1\"],\n",
    "        floatfmt=\".3f\",  \n",
    "        tablefmt=\"github\"\n",
    "    ))\n",
    "    print()\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tabulate import tabulate\n",
    "print(\"T14: Lung Cancer\")\n",
    "for i in range(5):\n",
    "    df = pd.read_csv(f\"/home/yl3427/cylab/selfCorrectionAgent/results_t_stage_run_{i}_lung.csv\")\n",
    "    main_1 = 'experiment_t_stage_pred_llm_mem'\n",
    "    main_2 = 'experiment_t_stage_pred_rag_mem'\n",
    "    zs = 'experiment_t_stage_pred_zs'\n",
    "    rules_1 = 'experiment_t_stage_pred_llm_rulesonly'\n",
    "    rules_2 = 'experiment_t_stage_pred_rag_rulesonly'\n",
    "    memory_1 = 'experiment_t_stage_pred_memonly'\n",
    "    memory_2 = 'experiment_t_stage_pred_memonly2'\n",
    "    rag = 'experiment_t_stage_pred_rag_only'\n",
    "\n",
    "    main_1 = t14_calculate_metrics2(df['t'],df[main_1])['overall']\n",
    "    main_2 = t14_calculate_metrics2(df['t'],df[main_2])['overall']\n",
    "    zs = t14_calculate_metrics2(df['t'], df[zs])['overall']\n",
    "    rules_1 = t14_calculate_metrics2(df['t'], df[rules_1])['overall']\n",
    "    rules_2 = t14_calculate_metrics2(df['t'], df[rules_2])['overall']\n",
    "    memory_1 = t14_calculate_metrics2(df['t'], df[memory_1])['overall']\n",
    "    memory_2 = t14_calculate_metrics2(df['t'], df[memory_2])['overall']\n",
    "    rag = t14_calculate_metrics2(df['t'], df[rag])['overall']\n",
    "\n",
    "\n",
    "    table_data = [\n",
    "        [\"ZS\",       zs['macro_f1']],\n",
    "        [\"Rule1 (from nothing)\",  rules_1['macro_f1']],\n",
    "        [\"Memory1 (from Rule1)\", memory_1['macro_f1']],\n",
    "        [\"Rule1 + Memory1\",   main_1['macro_f1']],\n",
    "        [],\n",
    "        [\"RAG\",      rag['macro_f1']],\n",
    "        [\"Rule2 (from RAG excerpt)\",  rules_2['macro_f1']],\n",
    "        [\"Memory2 (from Rule2)\", memory_2['macro_f1']],\n",
    "        [\"Rule2 + Memory2\",   main_2['macro_f1']],\n",
    "    ]\n",
    "    print(f\"Run {i}\")\n",
    "    print(tabulate(\n",
    "        table_data,\n",
    "        headers=[\"Method\", \"Macro F1\"],\n",
    "        floatfmt=\".3f\",  \n",
    "        tablefmt=\"github\"\n",
    "    ))\n",
    "    print()\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tabulate import tabulate\n",
    "print(\"N03: Breast Cancer\")\n",
    "for i in range(5):\n",
    "    df = pd.read_csv(f\"/home/yl3427/cylab/selfCorrectionAgent/result_breast/results_n_stage_run_{i}.csv\")\n",
    "    main_1 = 'experiment_n_stage_pred_llm_mem'\n",
    "    main_2 = 'experiment_n_stage_pred_rag_mem'\n",
    "    zs = 'experiment_n_stage_pred_zs'\n",
    "    rules_1 = 'experiment_n_stage_pred_llm_rulesonly'\n",
    "    rules_2 = 'experiment_n_stage_pred_rag_rulesonly'\n",
    "    memory_1 = 'experiment_n_stage_pred_memonly'\n",
    "    memory_2 = 'experiment_n_stage_pred_memonly2'\n",
    "    rag = 'experiment_n_stage_pred_rag_only'\n",
    "\n",
    "    main_1 = n03_calculate_metrics2(df['n'],df[main_1])['overall']\n",
    "    main_2 = n03_calculate_metrics2(df['n'],df[main_2])['overall']\n",
    "    zs = n03_calculate_metrics2(df['n'], df[zs])['overall']\n",
    "    rules_1 = n03_calculate_metrics2(df['n'], df[rules_1])['overall']\n",
    "    rules_2 = n03_calculate_metrics2(df['n'], df[rules_2])['overall']\n",
    "    memory_1 = n03_calculate_metrics2(df['n'], df[memory_1])['overall']\n",
    "    memory_2 = n03_calculate_metrics2(df['n'], df[memory_2])['overall']\n",
    "    rag = n03_calculate_metrics2(df['n'], df[rag])['overall']\n",
    "\n",
    "\n",
    "    table_data = [\n",
    "        [\"ZS\",       zs['macro_f1']],\n",
    "        [\"Rule1 (from nothing)\",  rules_1['macro_f1']],\n",
    "        [\"Memory1 (from Rule1)\", memory_1['macro_f1']],\n",
    "        [\"Rule1 + Memory1\",   main_1['macro_f1']],\n",
    "        [],\n",
    "        [\"RAG\",      rag['macro_f1']],\n",
    "        [\"Rule2 (from RAG excerpt)\",  rules_2['macro_f1']],\n",
    "        [\"Memory2 (from Rule2)\", memory_2['macro_f1']],\n",
    "        [\"Rule2 + Memory2\",   main_2['macro_f1']],\n",
    "    ]\n",
    "    print(f\"Run {i}\")\n",
    "    print(tabulate(\n",
    "        table_data,\n",
    "        headers=[\"Method\", \"Macro F1\"],\n",
    "        floatfmt=\".3f\",  \n",
    "        tablefmt=\"github\"\n",
    "    ))\n",
    "    print()\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tabulate import tabulate\n",
    "print(\"N03: Lung Cancer\")\n",
    "for i in range(5):\n",
    "    df = pd.read_csv(f\"/home/yl3427/cylab/selfCorrectionAgent/results_n_stage_run_{i}_lung.csv\")\n",
    "    main_1 = 'experiment_n_stage_pred_llm_mem'\n",
    "    main_2 = 'experiment_n_stage_pred_rag_mem'\n",
    "    zs = 'experiment_n_stage_pred_zs'\n",
    "    rules_1 = 'experiment_n_stage_pred_llm_rulesonly'\n",
    "    rules_2 = 'experiment_n_stage_pred_rag_rulesonly'\n",
    "    memory_1 = 'experiment_n_stage_pred_memonly'\n",
    "    memory_2 = 'experiment_n_stage_pred_memonly2'\n",
    "    rag = 'experiment_n_stage_pred_rag_only'\n",
    "\n",
    "    main_1 = n03_calculate_metrics2(df['n'],df[main_1])['overall']\n",
    "    main_2 = n03_calculate_metrics2(df['n'],df[main_2])['overall']\n",
    "    zs = n03_calculate_metrics2(df['n'], df[zs])['overall']\n",
    "    rules_1 = n03_calculate_metrics2(df['n'], df[rules_1])['overall']\n",
    "    rules_2 = n03_calculate_metrics2(df['n'], df[rules_2])['overall']\n",
    "    memory_1 = n03_calculate_metrics2(df['n'], df[memory_1])['overall']\n",
    "    memory_2 = n03_calculate_metrics2(df['n'], df[memory_2])['overall']\n",
    "    rag = n03_calculate_metrics2(df['n'], df[rag])['overall']\n",
    "\n",
    "\n",
    "    table_data = [\n",
    "        [\"ZS\",       zs['macro_f1']],\n",
    "        [\"Rule1 (from nothing)\",  rules_1['macro_f1']],\n",
    "        [\"Memory1 (from Rule1)\", memory_1['macro_f1']],\n",
    "        [\"Rule1 + Memory1\",   main_1['macro_f1']],\n",
    "        [],\n",
    "        [\"RAG\",      rag['macro_f1']],\n",
    "        [\"Rule2 (from RAG excerpt)\",  rules_2['macro_f1']],\n",
    "        [\"Memory2 (from Rule2)\", memory_2['macro_f1']],\n",
    "        [\"Rule2 + Memory2\",   main_2['macro_f1']],\n",
    "    ]\n",
    "    print(f\"Run {i}\")\n",
    "    print(tabulate(\n",
    "        table_data,\n",
    "        headers=[\"Method\", \"Macro F1\"],\n",
    "        floatfmt=\".3f\",  \n",
    "        tablefmt=\"github\"\n",
    "    ))\n",
    "    print()\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"/secure/shared_data/rag_tnm_results/summary/5_folds_summary/brca_df.csv\")\n",
    "df[df.n != -1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"/secure/shared_data/rag_tnm_results/summary/5_folds_summary/luad_df.csv\")\n",
    "df[df.n != -1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tabulate import tabulate\n",
    "\n",
    "cancer = \"Breast\"\n",
    "tnm = \"T14\"\n",
    "results_path = \"/home/yl3427/cylab/selfCorrectionAgent/result_breast/results_t_stage_run_{i}.csv\"\n",
    "\n",
    "###\n",
    "results_tracker = {\n",
    "    \"ZS\": [],\n",
    "    \"Rule1 (from nothing)\": [],\n",
    "    \"Memory1 (from Rule1)\": [],\n",
    "    \"Rule1 + Memory1\": [],\n",
    "    \"RAG\": [],\n",
    "    \"Rule2 (from RAG excerpt)\": [],\n",
    "    \"Memory2 (from Rule2)\": [],\n",
    "    \"Rule2 + Memory2\": []\n",
    "}\n",
    "\n",
    "print(f\"{tnm.upper()}: {cancer} Cancer\\n\")\n",
    "\n",
    "if tnm.lower()[0] == 't':\n",
    "    calculate_metrics = t14_calculate_metrics2\n",
    "elif tnm.lower()[0] == 'n':\n",
    "    calculate_metrics = n03_calculate_metrics2\n",
    "# Loop through the 5 runs\n",
    "\n",
    "for i in range(5):\n",
    "    df = pd.read_csv(results_path.format(i=i))\n",
    "    \n",
    "    main_1_col = f\"experiment_{tnm.lower()[0]}_stage_pred_llm_mem\"\n",
    "    main_2_col = f\"experiment_{tnm.lower()[0]}_stage_pred_rag_mem\"\n",
    "    zs_col     = f\"experiment_{tnm.lower()[0]}_stage_pred_zs\"\n",
    "    rules_1_col= f\"experiment_{tnm.lower()[0]}_stage_pred_llm_rulesonly\"\n",
    "    rules_2_col= f\"experiment_{tnm.lower()[0]}_stage_pred_rag_rulesonly\"\n",
    "    memory_1_col=f\"experiment_{tnm.lower()[0]}_stage_pred_memonly\"\n",
    "    memory_2_col=f\"experiment_{tnm.lower()[0]}_stage_pred_memonly2\"\n",
    "    rag_col    = f\"experiment_{tnm.lower()[0]}_stage_pred_rag_only\"\n",
    "\n",
    "    main_1   = calculate_metrics(df[tnm.lower()[0]], df[main_1_col])['overall']\n",
    "    main_2   = calculate_metrics(df[tnm.lower()[0]], df[main_2_col])['overall']\n",
    "    zs       = calculate_metrics(df[tnm.lower()[0]], df[zs_col])['overall']\n",
    "    rules_1  = calculate_metrics(df[tnm.lower()[0]], df[rules_1_col])['overall']\n",
    "    rules_2  = calculate_metrics(df[tnm.lower()[0]], df[rules_2_col])['overall']\n",
    "    memory_1 = calculate_metrics(df[tnm.lower()[0]], df[memory_1_col])['overall']\n",
    "    memory_2 = calculate_metrics(df[tnm.lower()[0]], df[memory_2_col])['overall']\n",
    "    rag      = calculate_metrics(df[tnm.lower()[0]], df[rag_col])['overall']\n",
    "\n",
    "    \n",
    "    # what calcuate_metrics returns\n",
    "    # results[\"overall\"] = {\n",
    "    #     \"macro_precision\": round(macro_precision, 3),\n",
    "    #     \"macro_recall\":    round(macro_recall, 3),\n",
    "    #     \"macro_f1\":        round(macro_f1, 3),\n",
    "    #     \"micro_precision\": round(micro_precision, 3),\n",
    "    #     \"micro_recall\":    round(micro_recall, 3),\n",
    "    #     \"micro_f1\":        round(micro_f1, 3),\n",
    "    #     \"weighted_f1\":     round(weighted_f1, 3),\n",
    "    #     \"support\":         total_instances,\n",
    "    #     \"num_errors\":      total_fp + total_fn,\n",
    "    # }\n",
    "    \n",
    "    # Append the macro_f1 to our tracker\n",
    "    results_tracker[\"ZS\"].append(zs['macro_f1'])\n",
    "    results_tracker[\"Rule1 (from nothing)\"].append(rules_1['macro_f1'])\n",
    "    results_tracker[\"Memory1 (from Rule1)\"].append(memory_1['macro_f1'])\n",
    "    results_tracker[\"Rule1 + Memory1\"].append(main_1['macro_f1'])\n",
    "    results_tracker[\"RAG\"].append(rag['macro_f1'])\n",
    "    results_tracker[\"Rule2 (from RAG excerpt)\"].append(rules_2['macro_f1'])\n",
    "    results_tracker[\"Memory2 (from Rule2)\"].append(memory_2['macro_f1'])\n",
    "    results_tracker[\"Rule2 + Memory2\"].append(main_2['macro_f1'])\n",
    "    \n",
    "    # Print a run-specific table\n",
    "    table_data = [\n",
    "        [\"ZS\", zs['macro_f1']],\n",
    "        [\"Rule1 (from nothing)\", rules_1['macro_f1']],\n",
    "        [\"Memory1 (from Rule1)\", memory_1['macro_f1']],\n",
    "        [\"Rule1 + Memory1\", main_1['macro_f1']],\n",
    "        [],\n",
    "        [\"RAG\", rag['macro_f1']],\n",
    "        [\"Rule2 (from RAG excerpt)\", rules_2['macro_f1']],\n",
    "        [\"Memory2 (from Rule2)\", memory_2['macro_f1']],\n",
    "        [\"Rule2 + Memory2\", main_2['macro_f1']],\n",
    "    ]\n",
    "    print(f\"Run {i}\")\n",
    "    print(tabulate(\n",
    "        table_data,\n",
    "        headers=[\"Method\", \"Macro F1\"],\n",
    "        floatfmt=\".3f\",  \n",
    "        tablefmt=\"github\"\n",
    "    ))\n",
    "    print()\n",
    "    print()\n",
    "\n",
    "# Now build a summary table (averages and std) across all runs\n",
    "summary_table = []\n",
    "methods_order = [\n",
    "    \"ZS\",\n",
    "    \"Rule1 (from nothing)\",\n",
    "    \"Memory1 (from Rule1)\",\n",
    "    \"Rule1 + Memory1\",\n",
    "    \"RAG\",\n",
    "    \"Rule2 (from RAG excerpt)\",\n",
    "    \"Memory2 (from Rule2)\",\n",
    "    \"Rule2 + Memory2\"\n",
    "]\n",
    "\n",
    "for method in methods_order:\n",
    "    scores = results_tracker[method]\n",
    "    mean_val = np.mean(scores)\n",
    "    std_val  = np.std(scores, ddof=1)  # sample standard deviation\n",
    "    summary_table.append([method, mean_val, std_val])\n",
    "\n",
    "print(\"## Summary Across All Runs\")\n",
    "print(tabulate(\n",
    "    summary_table,\n",
    "    headers=[\"Method\", \"Mean Macro F1\", \"Std Macro F1\"],\n",
    "    floatfmt=\".3f\",\n",
    "    tablefmt=\"github\"\n",
    "))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tabulate import tabulate\n",
    "from src.metrics import *\n",
    "\n",
    "def run_experiments(cancer, tnm, results_path, num_runs=5):\n",
    "\n",
    "    # Decide which metric function to use based on T vs. N\n",
    "    if tnm.lower().startswith('t'):\n",
    "        calculate_metrics = t14_calculate_metrics2\n",
    "    elif tnm.lower().startswith('n'):\n",
    "        calculate_metrics = n03_calculate_metrics2\n",
    "    else:\n",
    "        raise ValueError(\"Only T or N staging is supported in this template.\")\n",
    "\n",
    "    # Methods you want to evaluate:\n",
    "    # (keys = descriptive labels, values = column name in CSV)\n",
    "    methods = {\n",
    "        \"ZS\":                 f\"experiment_{tnm.lower()[0]}_stage_pred_zs\",\n",
    "        \"Rule1 (from nothing)\": f\"experiment_{tnm.lower()[0]}_stage_pred_llm_rulesonly\",\n",
    "        \"Memory1 (from Rule1)\": f\"experiment_{tnm.lower()[0]}_stage_pred_memonly\",\n",
    "        \"Rule1 + Memory1\":   f\"experiment_{tnm.lower()[0]}_stage_pred_llm_mem\",\n",
    "        \"RAG\":               f\"experiment_{tnm.lower()[0]}_stage_pred_rag_only\",\n",
    "        \"Rule2 (from RAG excerpt)\": f\"experiment_{tnm.lower()[0]}_stage_pred_rag_rulesonly\",\n",
    "        \"Memory2 (from Rule2)\": f\"experiment_{tnm.lower()[0]}_stage_pred_memonly2\",\n",
    "        \"Rule2 + Memory2\":   f\"experiment_{tnm.lower()[0]}_stage_pred_rag_mem\"\n",
    "    }\n",
    "\n",
    "    # We want to track each method's macro/micro precision, recall, and F1 across runs\n",
    "    # results_tracker[method][\"macro_precision\"] = [list of values across runs], etc.\n",
    "    metric_keys = [\"macro_precision\", \"macro_recall\", \"macro_f1\",\n",
    "                #    \"micro_precision\", \"micro_recall\", \"micro_f1\"\n",
    "                   ]\n",
    "    results_tracker = {\n",
    "        m: {mk: [] for mk in metric_keys} for m in methods.keys()\n",
    "    }\n",
    "\n",
    "    print(f\"{tnm.upper()}: {cancer} Cancer\")\n",
    "\n",
    "    # Loop over each run\n",
    "    for run_index in range(num_runs):\n",
    "        if run_index == 1:\n",
    "            continue\n",
    "        print(run_index)\n",
    "        df = pd.read_csv(results_path.format(i=run_index))\n",
    "        \n",
    "        # Evaluate each method's performance\n",
    "        method_results = {}\n",
    "        for method_label, col_name in methods.items():\n",
    "            # Make sure the column is in your dataframe\n",
    "            if col_name not in df.columns:\n",
    "                raise ValueError(f\"Column '{col_name}' not found in CSV for method '{method_label}'.\")\n",
    "\n",
    "            # Calculate metrics against ground truth: df[tnm.lower()[0]]\n",
    "            result = calculate_metrics(df[tnm.lower()[0]], df[col_name])['overall']\n",
    "            method_results[method_label] = result\n",
    "\n",
    "            # Store the metrics\n",
    "            for mk in metric_keys:\n",
    "                results_tracker[method_label][mk].append(result[mk])  \n",
    "\n",
    "        # Build a run-specific table (show macro & micro metrics)\n",
    "        table_data = []\n",
    "        for method_label in methods.keys():\n",
    "            res = method_results[method_label]\n",
    "            table_data.append([\n",
    "                method_label,\n",
    "                res[\"macro_precision\"], res[\"macro_recall\"], res[\"macro_f1\"],\n",
    "                # res[\"micro_precision\"], res[\"micro_recall\"], res[\"micro_f1\"]\n",
    "            ])\n",
    "\n",
    "        # print(f\"Run {run_index}\")\n",
    "        # print(tabulate(\n",
    "        #     table_data,\n",
    "        #     headers=[\"Method\", \n",
    "        #              \"Macro P\", \"Macro R\", \"Macro F1\", \n",
    "        #              \"Micro P\", \"Micro R\", \"Micro F1\"],\n",
    "        #     floatfmt=\".3f\",\n",
    "        #     tablefmt=\"github\"\n",
    "        # ))\n",
    "        # print()\n",
    "\n",
    "    # Build a final summary table (mean & std across all runs)\n",
    "    summary_table = []\n",
    "    for method_label in methods.keys():\n",
    "        row = [method_label]\n",
    "        for mk in metric_keys:\n",
    "            scores = results_tracker[method_label][mk]\n",
    "            mean_val = np.mean(scores)\n",
    "            std_val  = np.std(scores, ddof=1)  # sample standard deviation\n",
    "            row.append(f\"{mean_val:.3f}±{std_val:.3f}\")\n",
    "        summary_table.append(row)\n",
    "\n",
    "    # Print summary table\n",
    "    print(f\"## Summary Across {num_runs} Runs\")\n",
    "    print(tabulate(\n",
    "        summary_table,\n",
    "        headers=[\n",
    "            \"Method\", \n",
    "            \"Macro P (mean±std)\", \"Macro R (mean±std)\", \"Macro F1 (mean±std)\",\n",
    "            \"Micro P (mean±std)\", \"Micro R (mean±std)\", \"Micro F1 (mean±std)\"\n",
    "        ],\n",
    "        tablefmt=\"github\"\n",
    "    ))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cancer = \"Breast\"\n",
    "tnm = \"T14\"\n",
    "results_path = \"/home/yl3427/cylab/selfCorrectionAgent/result_breast/results_t_stage_run_{i}.csv\"\n",
    "run_experiments(cancer, tnm, results_path, num_runs=5)\n",
    "print(\"------------------------\"*4)\n",
    "print()\n",
    "cancer = \"Breast\"\n",
    "tnm = \"N03\"\n",
    "results_path = \"/home/yl3427/cylab/selfCorrectionAgent/result_breast/results_n_stage_run_{i}.csv\"\n",
    "run_experiments(cancer, tnm, results_path, num_runs=5)\n",
    "print(\"------------------------\"*4)\n",
    "print()\n",
    "cancer = \"Lung\"\n",
    "tnm = \"T14\"\n",
    "results_path = \"/home/yl3427/cylab/selfCorrectionAgent/results_t_stage_run_{i}_lung.csv\"\n",
    "run_experiments(cancer, tnm, results_path, num_runs=5)\n",
    "print(\"------------------------\"*4)\n",
    "print()\n",
    "cancer = \"Lung\"\n",
    "tnm = \"N03\"\n",
    "results_path = \"/home/yl3427/cylab/selfCorrectionAgent/results_n_stage_run_{i}_lung.csv\"\n",
    "run_experiments(cancer, tnm, results_path, num_runs=5)\n",
    "print(\"------------------------\"*4)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mixtral\n",
    "cancer = \"Lung\"\n",
    "tnm = \"T14\"\n",
    "results_path = \"/home/yl3427/cylab/selfCorrectionAgent/results_t_stage_run_{i}_lung_mixtral.csv\"\n",
    "run_experiments(cancer, tnm, results_path, num_runs=5)\n",
    "print(\"------------------------\"*4)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(results_path.format(i=0))\n",
    "'experiment_t_stage_pred_zs' in df.columns\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
