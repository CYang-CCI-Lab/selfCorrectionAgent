{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0 Model for testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "bat"
    }
   },
   "source": [
    "```bash\n",
    "CUDA_VISIBLE_DEVICES=\"4,5\", python -m vllm.entrypoints.openai.api_server \\\n",
    "--model mistralai/Mistral-7B-Instruct-v0.2 \\\n",
    "--disable-custom-all-reduce \\\n",
    "--enforce-eager \\\n",
    "--download-dir /secure/chiahsuan/hf_cache/ \\\n",
    "--tensor-parallel-size 2 \\\n",
    "--port 8085\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```bash\n",
    "CUDA_VISIBLE_DEVICES=\"0,1,2,3\", python -m vllm.entrypoints.openai.api_server \\\n",
    "--model mistralai/Mixtral-8x7B-Instruct-v0.1 \\\n",
    "--download-dir /secure/chiahsuan/hf_cache/ \\\n",
    "--tensor-parallel-size 4 \\\n",
    "--disable-custom-all-reduce \\\n",
    "--enforce-eager\n",
    "```\n",
    "mistralai/Mixtral-8x7B-Instruct-v0.1: always port 8000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 Load packages and define class\n",
    "== The following two cells can be organized into a separate py file as a module.=="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yl3427/miniconda3/envs/selfCorrection/lib/python3.9/site-packages/fuzzywuzzy/fuzz.py:11: UserWarning: Using slow pure-python SequenceMatcher. Install python-Levenshtein to remove this warning\n",
      "  warnings.warn('Using slow pure-python SequenceMatcher. Install python-Levenshtein to remove this warning')\n"
     ]
    }
   ],
   "source": [
    "from src.prompt import system_instruction\n",
    "from src.prompt import baseline_prompt_t14 as baseline_prompt\n",
    "from src.prompt import initial_predict_prompt_t14 as initial_predict_prompt\n",
    "from src.prompt import subsequent_predict_prompt_t14 as subsequent_predict_prompt\n",
    "from src.prompt import testing_predict_prompt_t14 as testing_predict_prompt\n",
    "from src.metrics import n03_performance_report\n",
    "\n",
    "import openai\n",
    "from openai import OpenAI\n",
    "from sklearn.model_selection import KFold\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import json\n",
    "from pydantic import BaseModel, Field\n",
    "from typing import List, Dict, Union\n",
    "from fuzzywuzzy import fuzz\n",
    "import re\n",
    "import ast"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Define ChoiceAgent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define class\n",
    "class ChoiceAgent:\n",
    "    \"\"\" the simplest agent, which is appropriate for zero-shot prompting\n",
    "    \"\"\"\n",
    "    def __init__(self, client: OpenAI, model: str, \n",
    "                 prompt_template: str, choices: dict, label: str) -> None:\n",
    "        self.client = client\n",
    "        self.model = model\n",
    "        self.prompt_template = prompt_template\n",
    "        self.choices = choices\n",
    "        self.label = label\n",
    "\n",
    "    def get_response_from_choices(self, messages: list, temperature:float) -> str:\n",
    "        response = self.client.chat.completions.create(\n",
    "            model=self.model,\n",
    "            messages=messages,\n",
    "            extra_body={\"guided_choice\":self.choices},\n",
    "            temperature = temperature\n",
    "        )\n",
    "        return response.choices[0].message.content\n",
    "\n",
    "    def run(self, dataset: pd.DataFrame, temperature: float = 0.0) -> pd.DataFrame:\n",
    "        pbar = tqdm(total=dataset.shape[0])\n",
    "        for idx, row in dataset.iterrows():\n",
    "            report = row[\"text\"]\n",
    "            prompt = self.prompt_template.format(report=report)\n",
    "            system_prompt = system_instruction+ \"\\n\" + prompt\n",
    "            messages = [{\"role\": \"user\", \"content\": system_prompt}]\n",
    "            answer = self.get_response_from_choices(messages, temperature)\n",
    "            dataset.loc[idx, f\"zs_{self.label}_ans_str\"] = answer\n",
    "            pbar.update(1)\n",
    "        pbar.close()\n",
    "\n",
    "        return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Define MemoryAgent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MemoryAgent:\n",
    "    \"\"\" the implementation of memory agent, which learn memory from training set and\n",
    "    utilize memory as contexts for testing set.\n",
    "    \"\"\"\n",
    "    def __init__(self, client: OpenAI, model: str, \n",
    "                 prompt_template_dict: dict[str, str], schema_dict: dict, label: str) -> None:\n",
    "        self.client = client\n",
    "        self.model = model\n",
    "        self.prompt_template_dict = prompt_template_dict\n",
    "        self.validate_prompt_template()\n",
    "        self.schema_dict = schema_dict\n",
    "        self.validate_schema()\n",
    "        self.memory = \"\"\n",
    "        self.label = label\n",
    "\n",
    "    def validate_prompt_template(self) -> None:\n",
    "        keys = self.prompt_template_dict.keys()\n",
    "        initial_prompt_exist = \"initialized_prompt\" in keys\n",
    "        learning_prompt_exist = \"learning_prompt\" in keys\n",
    "        testing_prompt_exist = \"testing_prompt\" in keys\n",
    "        assert True == initial_prompt_exist == learning_prompt_exist == testing_prompt_exist, \\\n",
    "        \"You should provide a dict with initialized_prompt, learning_prompt, and testing_prompt as keys.\"\n",
    "\n",
    "    def validate_schema(self) -> None:\n",
    "        keys = self.schema_dict.keys()\n",
    "        learning_schema_exist = \"learning_schema\" in keys\n",
    "        testing_schema_exist = \"testing_schema\" in keys\n",
    "        assert True == learning_schema_exist == testing_schema_exist, \\\n",
    "        \"You should provide a dict with learning_schema and testing_schema as keys.\"\n",
    "\n",
    "    def get_schema_followed_response(self, messages: list, schema:dict, temperature:float) -> Union[Dict, None]:\n",
    "        num_attempt = 3\n",
    "        for attempt in range(num_attempt):\n",
    "            try:\n",
    "                response = self.client.chat.completions.create(\n",
    "                    model=self.model,\n",
    "                    messages=messages,\n",
    "                    extra_body={\"guided_json\":schema},\n",
    "                    temperature = temperature\n",
    "                )\n",
    "                return json.loads(response.choices[0].message.content.replace(\"\\\\\", \"\\\\\\\\\"))\n",
    "            except json.JSONDecodeError:\n",
    "                print(\"Error decoding JSON response\")\n",
    "                return None\n",
    "            except openai.APITimeoutError:\n",
    "                if attempt < (num_attempt -1):\n",
    "                    wait_time = 2 * (attempt + 1)\n",
    "                    print(f\"Request timed out. Retrying in {wait_time} seconds...\")\n",
    "                    time.sleep(wait_time)\n",
    "                else:\n",
    "                    print(\"Max retries reached. Request faild.\")\n",
    "                    return None\n",
    "        \n",
    "    def train(self, training_dataset: pd.DataFrame, temperature: float = 0.0) -> pd.DataFrame:\n",
    "        pbar = tqdm(total=training_dataset.shape[0])\n",
    "        parsing_error = 0\n",
    "        for idx, row in training_dataset.iterrows():\n",
    "\n",
    "            report = row[\"text\"]\n",
    "\n",
    "            if self.memory == \"\":\n",
    "                prompt = self.prompt_template_dict[\"initialized_prompt\"].format(report=report)\n",
    "            else:\n",
    "                prompt = self.prompt_template_dict[\"learning_prompt\"].format(memory=self.memory, report=report)\n",
    "            \n",
    "            system_prompt = system_instruction+ \"\\n\" + prompt\n",
    "            messages = [{\"role\": \"user\", \"content\": system_prompt}]\n",
    "\n",
    "            json_output = self.get_schema_followed_response(messages, self.schema_dict[\"learning_schema\"], temperature)\n",
    "\n",
    "            if not json_output:\n",
    "                parsing_error += 1\n",
    "                continue\n",
    "            \n",
    "            self.memory = json_output['rules']\n",
    "            training_dataset.loc[idx, f\"mem_{self.label}_reasoning\"] = json_output['reasoning']\n",
    "            training_dataset.loc[idx, f\"mem_{self.label}_ans_str\"] = json_output['predictedStage']\n",
    "            \n",
    "            pbar.update(1)\n",
    "        pbar.close()\n",
    "        print(f\"Number of parsing errors: {parsing_error}\")\n",
    "        return training_dataset\n",
    "    \n",
    "    def test(self, testing_dataset: pd.DataFrame, temperature: float = 0.0) -> pd.DataFrame:\n",
    "        pbar = tqdm(total=testing_dataset.shape[0])\n",
    "        parsing_error = 0\n",
    "        for idx, row in testing_dataset.iterrows():\n",
    "\n",
    "            report = row[\"text\"]\n",
    "\n",
    "            prompt = self.prompt_template_dict[\"testing_prompt\"].format(memory=self.memory, report=report)\n",
    "            system_prompt = system_instruction+ \"\\n\" + prompt\n",
    "            messages = [{\"role\": \"user\", \"content\": system_prompt}]\n",
    "\n",
    "            json_output = self.get_schema_followed_response(messages, self.schema_dict[\"testing_schema\"], temperature)\n",
    "\n",
    "            if not json_output:\n",
    "                parsing_error += 1\n",
    "                continue\n",
    "            \n",
    "            testing_dataset.loc[idx, f\"mem_{self.label}_reasoning\"] = json_output['reasoning']\n",
    "            testing_dataset.loc[idx, f\"mem_{self.label}_ans_str\"] = json_output['predictedStage']\n",
    "            \n",
    "            pbar.update(1)\n",
    "        pbar.close()\n",
    "        print(f\"Number of parsing errors: {parsing_error}\")\n",
    "        return testing_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Define ConditionalMemoryAgent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConditionalMemoryAgent(MemoryAgent):\n",
    "  \n",
    "  def __init__(self, client: OpenAI, model: str, \n",
    "                 prompt_template_dict: dict[str, str], schema_dict: dict, label: str) -> None:\n",
    "    # inherit all properties and methods from MemoryAgent\n",
    "    super().__init__(client, model, prompt_template_dict, schema_dict, label)\n",
    "  \n",
    "  def is_updated(self, new_memory, threshold):\n",
    "    old_str = \"\\n\".join(self.memory)\n",
    "    new_str = \"\\n\".join(new_memory)\n",
    "    if fuzz.ratio(old_str, new_str) >= threshold : \n",
    "        return True # update memory\n",
    "    else:\n",
    "        return False\n",
    "    \n",
    "  def train(self, training_dataset: pd.DataFrame, temperature: float = 0.0, threshold: float = 80) -> pd.DataFrame:\n",
    "    # only overide this function because the rest parts are the same\n",
    "    pbar = tqdm(total=training_dataset.shape[0])\n",
    "    parsing_error = 0\n",
    "    num_update = 0\n",
    "    for idx, row in training_dataset.iterrows():\n",
    "\n",
    "        report = row[\"text\"]\n",
    "\n",
    "        if self.memory == \"\":\n",
    "            prompt = self.prompt_template_dict[\"initialized_prompt\"].format(report=report)\n",
    "        else:\n",
    "            prompt = self.prompt_template_dict[\"learning_prompt\"].format(memory=self.memory, report=report)\n",
    "        \n",
    "        system_prompt = system_instruction+ \"\\n\" + prompt\n",
    "        messages = [{\"role\": \"user\", \"content\": system_prompt}]\n",
    "\n",
    "        json_output = self.get_schema_followed_response(messages, self.schema_dict[\"learning_schema\"], temperature)\n",
    "\n",
    "        if not json_output:\n",
    "            parsing_error += 1\n",
    "            continue\n",
    "        \n",
    "        if self.memory == \"\":\n",
    "           self.memory = json_output['rules']\n",
    "        else:\n",
    "          current_memory_str = \"\\n\".join(self.memory)\n",
    "          new_memory_str = \"\\n\".join(json_output['rules'])\n",
    "          if fuzz.ratio(current_memory_str, new_memory_str) >= threshold :\n",
    "            self.memory = json_output['rules']\n",
    "            num_update += 1\n",
    "\n",
    "        training_dataset.loc[idx, f\"cmem_{self.label}_reasoning\"] = json_output['reasoning']\n",
    "        training_dataset.loc[idx, f\"cmem_{self.label}_ans_str\"] = json_output['predictedStage']\n",
    "        training_dataset.loc[idx, f\"cmem_{self.label}_num_update\"] = num_update\n",
    "        \n",
    "        pbar.update(1)\n",
    "    pbar.close()\n",
    "    print(f\"Number of memory updates: {num_update}\")\n",
    "    print(f\"Number of parsing errors: {parsing_error}\")\n",
    "    return training_dataset\n",
    "\n",
    "  def test(self, testing_dataset: pd.DataFrame, temperature: float = 0.0) -> pd.DataFrame:\n",
    "    pbar = tqdm(total=testing_dataset.shape[0])\n",
    "    parsing_error = 0\n",
    "    for idx, row in testing_dataset.iterrows():\n",
    "\n",
    "        report = row[\"text\"]\n",
    "\n",
    "        prompt = self.prompt_template_dict[\"testing_prompt\"].format(memory=self.memory, report=report)\n",
    "        system_prompt = system_instruction+ \"\\n\" + prompt\n",
    "        messages = [{\"role\": \"user\", \"content\": system_prompt}]\n",
    "\n",
    "        json_output = self.get_schema_followed_response(messages, self.schema_dict[\"testing_schema\"], temperature)\n",
    "\n",
    "        if not json_output:\n",
    "            parsing_error += 1\n",
    "            continue\n",
    "        \n",
    "        testing_dataset.loc[idx, f\"cmem_{self.label}_reasoning\"] = json_output['reasoning']\n",
    "        testing_dataset.loc[idx, f\"cmem_{self.label}_ans_str\"] = json_output['predictedStage']\n",
    "        \n",
    "        pbar.update(1)\n",
    "    pbar.close()\n",
    "    print(f\"Number of parsing errors: {parsing_error}\")\n",
    "    return testing_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4 To-be-Implemented"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiTaskMemoryAgent(MemoryAgent):\n",
    "    def __init__(self, client: OpenAI, model: str, \n",
    "                 prompt_template_dict: dict[str, str], schema_dict: dict) -> None:\n",
    "        # inherit all properties and methods from MemoryAgent\n",
    "        super().__init__(client, model, prompt_template_dict, schema_dict)\n",
    "  \n",
    "    def train(self, training_dataset: pd.DataFrame, temperature: float = 0.0) -> pd.DataFrame:\n",
    "        # overide this function\n",
    "        pass\n",
    "\n",
    "    def test(self, testing_dataset: pd.DataFrame, temperature: float = 0.0) -> pd.DataFrame:\n",
    "        # overide this function\n",
    "        pass\n",
    "\n",
    "class MemoryAgentWithVerifier(MemoryAgent): # inherit from MemoryAgent or MultiTaskMemoryAgent\n",
    "    def __init__(self, client: OpenAI, model: str, \n",
    "                 prompt_template_dict: dict[str, str], schema_dict: dict) -> None:\n",
    "        # inherit all properties and methods from MemoryAgent\n",
    "        super().__init__(client, model, prompt_template_dict, schema_dict)\n",
    "  \n",
    "    def train(self, training_dataset: pd.DataFrame, temperature: float = 0.0) -> pd.DataFrame:\n",
    "        # overide this function\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 Demonstration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1031\n",
      "800\n",
      "80\n",
      "720\n"
     ]
    }
   ],
   "source": [
    "openai_api_key = \"Empty\"\n",
    "openai_api_base = \"http://localhost:8000/v1\"\n",
    "\n",
    "client = OpenAI(\n",
    "    api_key=openai_api_key,\n",
    "    base_url=openai_api_base,\n",
    ")\n",
    "\n",
    "brca_report = pd.read_csv(\"/secure/shared_data/rag_tnm_results/summary/5_folds_summary/brca_df.csv\")\n",
    "print(len(brca_report))\n",
    "brca_report = brca_report[brca_report[\"n\"]!=-1]\n",
    "print(len(brca_report))\n",
    "\n",
    "df_training_samples = brca_report.iloc[:80, :]\n",
    "df_testing_samples = brca_report.iloc[80:, :]\n",
    "\n",
    "print(len(df_training_samples))\n",
    "print(len(df_testing_samples))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Initialize ChoiceAgent and use its instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# zs_agent = ChoiceAgent(client=client, model=\"mistralai/Mistral-7B-Instruct-v0.2\",\n",
    "#                  prompt_template=baseline_prompt,\n",
    "#                  choices=[\"T1\", \"T2\", \"T3\", \"T4\"])\n",
    "   \n",
    "zs_agent = ChoiceAgent(client=client, model=\"mistralai/Mixtral-8x7B-Instruct-v0.1\",\n",
    "                 prompt_template=baseline_prompt,\n",
    "                 choices=[\"T1\", \"T2\", \"T3\", \"T4\"], label = \"t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 800/800 [05:14<00:00,  2.54it/s]\n"
     ]
    }
   ],
   "source": [
    "zs_agent.run(dataset=brca_report).to_csv(\"brca_df_zs_t14.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "brca_report.to_csv(\"result/brca_df_zs_n03.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Initialize MemoryAgent and use its instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainingResponse(BaseModel):\n",
    "    predictedStage: str = Field(description=\"predicted cancer stage\")\n",
    "    reasoning: str = Field(description=\"reasoning to support predicted cancer stage\") \n",
    "    rules: List[str] = Field(description=\"list of rules\") \n",
    "training_schema = TrainingResponse.model_json_schema()\n",
    "\n",
    "class TestingResponse(BaseModel):\n",
    "    predictedStage: str = Field(description=\"predicted cancer stage\")\n",
    "    reasoning: str = Field(description=\"reasoning to support predicted cancer stage\") \n",
    "testing_schema = TestingResponse.model_json_schema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "memory_agent = MemoryAgent(client=client, model=\"mistralai/Mixtral-8x7B-Instruct-v0.1\",\n",
    "                           prompt_template_dict={\"initialized_prompt\":initial_predict_prompt,\n",
    "                                                 \"learning_prompt\":subsequent_predict_prompt,\n",
    "                                                 \"testing_prompt\":testing_predict_prompt},\n",
    "                           schema_dict={\"learning_schema\":training_schema,\n",
    "                                        \"testing_schema\":testing_schema},\n",
    "                                        label = \"n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "memory_agent.train(df_training_samples, temperature=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "memory_agent.memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(memory_agent.memory))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "memory_agent.test(df_testing_samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Initialize ConditionalMemoryAgent and use its instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conditional_memory_agent = ConditionalMemoryAgent(client=client, model=\"mistralai/Mixtral-8x7B-Instruct-v0.1\",\n",
    "                           prompt_template_dict={\"initialized_prompt\":initial_predict_prompt,\n",
    "                                                 \"learning_prompt\":subsequent_predict_prompt,\n",
    "                                                 \"testing_prompt\":testing_predict_prompt},\n",
    "                           schema_dict={\"learning_schema\":training_schema,\n",
    "                                        \"testing_schema\":testing_schema},\n",
    "                                        label = \"n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conditional_memory_agent.train(df_training_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conditional_memory_agent.memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(conditional_memory_agent.memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conditional_memory_agent.test(df_testing_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_training_samples.to_csv(\"df_training.csv\", index=False)\n",
    "df_testing_samples.to_csv(\"df_testing.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 Memory Saturation Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainingResponse(BaseModel):\n",
    "    predictedStage: str = Field(description=\"predicted cancer stage\")\n",
    "    reasoning: str = Field(description=\"reasoning to support predicted cancer stage\") \n",
    "    rules: List[str] = Field(description=\"list of rules\") \n",
    "training_schema = TrainingResponse.model_json_schema()\n",
    "\n",
    "class TestingResponse(BaseModel):\n",
    "    predictedStage: str = Field(description=\"predicted cancer stage\")\n",
    "    reasoning: str = Field(description=\"reasoning to support predicted cancer stage\") \n",
    "testing_schema = TestingResponse.model_json_schema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10 [00:00<?, ?it/s]/tmp/ipykernel_3850445/1676057217.py:48: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  training_dataset.loc[idx, f\"cmem_{self.label}_reasoning\"] = json_output['reasoning']\n",
      "/tmp/ipykernel_3850445/1676057217.py:49: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  training_dataset.loc[idx, f\"cmem_{self.label}_ans_str\"] = json_output['predictedStage']\n",
      "/tmp/ipykernel_3850445/1676057217.py:50: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  training_dataset.loc[idx, f\"cmem_{self.label}_num_update\"] = num_update\n",
      "100%|██████████| 10/10 [02:08<00:00, 12.88s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of memory updates: 9\n",
      "Number of parsing errors: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/790 [00:00<?, ?it/s]/tmp/ipykernel_3850445/1676057217.py:75: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  testing_dataset.loc[idx, f\"cmem_{self.label}_reasoning\"] = json_output['reasoning']\n",
      "/tmp/ipykernel_3850445/1676057217.py:76: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  testing_dataset.loc[idx, f\"cmem_{self.label}_ans_str\"] = json_output['predictedStage']\n",
      "100%|██████████| 790/790 [54:14<00:00,  4.12s/it]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parsing errors: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/20 [00:00<?, ?it/s]/tmp/ipykernel_3850445/1676057217.py:48: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  training_dataset.loc[idx, f\"cmem_{self.label}_reasoning\"] = json_output['reasoning']\n",
      "/tmp/ipykernel_3850445/1676057217.py:49: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  training_dataset.loc[idx, f\"cmem_{self.label}_ans_str\"] = json_output['predictedStage']\n",
      "/tmp/ipykernel_3850445/1676057217.py:50: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  training_dataset.loc[idx, f\"cmem_{self.label}_num_update\"] = num_update\n",
      " 10%|█         | 2/20 [00:23<03:35, 11.97s/it]"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 26\u001b[0m\n\u001b[1;32m     18\u001b[0m memory_agent \u001b[38;5;241m=\u001b[39m ConditionalMemoryAgent(client\u001b[38;5;241m=\u001b[39mclient, model\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmistralai/Mixtral-8x7B-Instruct-v0.1\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     19\u001b[0m                    prompt_template_dict\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minitialized_prompt\u001b[39m\u001b[38;5;124m\"\u001b[39m:initial_predict_prompt,\n\u001b[1;32m     20\u001b[0m                                          \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlearning_prompt\u001b[39m\u001b[38;5;124m\"\u001b[39m:subsequent_predict_prompt,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     23\u001b[0m                                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtesting_schema\u001b[39m\u001b[38;5;124m\"\u001b[39m:testing_schema},\n\u001b[1;32m     24\u001b[0m                                 label \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mn\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     25\u001b[0m df_train, df_test \u001b[38;5;241m=\u001b[39m sorted_df\u001b[38;5;241m.\u001b[39miloc[train_idx], sorted_df\u001b[38;5;241m.\u001b[39miloc[test_idx]\n\u001b[0;32m---> 26\u001b[0m train_result \u001b[38;5;241m=\u001b[39m \u001b[43mmemory_agent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     27\u001b[0m train_result\u001b[38;5;241m.\u001b[39mto_csv(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msaturation_train_result_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00msize\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m, index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m     28\u001b[0m test_result \u001b[38;5;241m=\u001b[39m memory_agent\u001b[38;5;241m.\u001b[39mtest(df_test)\n",
      "Cell \u001b[0;32mIn[4], line 33\u001b[0m, in \u001b[0;36mConditionalMemoryAgent.train\u001b[0;34m(self, training_dataset, temperature, threshold)\u001b[0m\n\u001b[1;32m     30\u001b[0m system_prompt \u001b[38;5;241m=\u001b[39m system_instruction\u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m prompt\n\u001b[1;32m     31\u001b[0m messages \u001b[38;5;241m=\u001b[39m [{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrole\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m: system_prompt}]\n\u001b[0;32m---> 33\u001b[0m json_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_schema_followed_response\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mschema_dict\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlearning_schema\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m json_output:\n\u001b[1;32m     36\u001b[0m     parsing_error \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "Cell \u001b[0;32mIn[3], line 35\u001b[0m, in \u001b[0;36mMemoryAgent.get_schema_followed_response\u001b[0;34m(self, messages, schema, temperature)\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m attempt \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_attempt):\n\u001b[1;32m     34\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 35\u001b[0m         response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchat\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompletions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     36\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     37\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmessages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     38\u001b[0m \u001b[43m            \u001b[49m\u001b[43mextra_body\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mguided_json\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43mschema\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     39\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtemperature\u001b[49m\n\u001b[1;32m     40\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     41\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m json\u001b[38;5;241m.\u001b[39mloads(response\u001b[38;5;241m.\u001b[39mchoices[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mmessage\u001b[38;5;241m.\u001b[39mcontent\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124m\"\u001b[39m))\n\u001b[1;32m     42\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m json\u001b[38;5;241m.\u001b[39mJSONDecodeError:\n",
      "File \u001b[0;32m~/miniconda3/envs/selfCorrection/lib/python3.9/site-packages/openai/_utils/_utils.py:277\u001b[0m, in \u001b[0;36mrequired_args.<locals>.inner.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    275\u001b[0m             msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMissing required argument: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquote(missing[\u001b[38;5;241m0\u001b[39m])\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    276\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(msg)\n\u001b[0;32m--> 277\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/selfCorrection/lib/python3.9/site-packages/openai/resources/chat/completions.py:590\u001b[0m, in \u001b[0;36mCompletions.create\u001b[0;34m(self, messages, model, frequency_penalty, function_call, functions, logit_bias, logprobs, max_tokens, n, presence_penalty, response_format, seed, stop, stream, stream_options, temperature, tool_choice, tools, top_logprobs, top_p, user, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[1;32m    558\u001b[0m \u001b[38;5;129m@required_args\u001b[39m([\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m], [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m    559\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcreate\u001b[39m(\n\u001b[1;32m    560\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    588\u001b[0m     timeout: \u001b[38;5;28mfloat\u001b[39m \u001b[38;5;241m|\u001b[39m httpx\u001b[38;5;241m.\u001b[39mTimeout \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m|\u001b[39m NotGiven \u001b[38;5;241m=\u001b[39m NOT_GIVEN,\n\u001b[1;32m    589\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ChatCompletion \u001b[38;5;241m|\u001b[39m Stream[ChatCompletionChunk]:\n\u001b[0;32m--> 590\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_post\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    591\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/chat/completions\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    592\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaybe_transform\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    593\u001b[0m \u001b[43m            \u001b[49m\u001b[43m{\u001b[49m\n\u001b[1;32m    594\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmessages\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    595\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmodel\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    596\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfrequency_penalty\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrequency_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    597\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfunction_call\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunction_call\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    598\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfunctions\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunctions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    599\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlogit_bias\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogit_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    600\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlogprobs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    601\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmax_tokens\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    602\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mn\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    603\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpresence_penalty\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpresence_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    604\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mresponse_format\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    605\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mseed\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    606\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstop\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    607\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstream\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    608\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstream_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    609\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtemperature\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    610\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtool_choice\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtool_choice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    611\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtools\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtools\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    612\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtop_logprobs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_logprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    613\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtop_p\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    614\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43muser\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43muser\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    615\u001b[0m \u001b[43m            \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    616\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcompletion_create_params\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCompletionCreateParams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    617\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    618\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmake_request_options\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    619\u001b[0m \u001b[43m            \u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_headers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_query\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_query\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_body\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_body\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\n\u001b[1;32m    620\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    621\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mChatCompletion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    622\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    623\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mStream\u001b[49m\u001b[43m[\u001b[49m\u001b[43mChatCompletionChunk\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    624\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/selfCorrection/lib/python3.9/site-packages/openai/_base_client.py:1240\u001b[0m, in \u001b[0;36mSyncAPIClient.post\u001b[0;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1226\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpost\u001b[39m(\n\u001b[1;32m   1227\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   1228\u001b[0m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1235\u001b[0m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1236\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ResponseT \u001b[38;5;241m|\u001b[39m _StreamT:\n\u001b[1;32m   1237\u001b[0m     opts \u001b[38;5;241m=\u001b[39m FinalRequestOptions\u001b[38;5;241m.\u001b[39mconstruct(\n\u001b[1;32m   1238\u001b[0m         method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpost\u001b[39m\u001b[38;5;124m\"\u001b[39m, url\u001b[38;5;241m=\u001b[39mpath, json_data\u001b[38;5;241m=\u001b[39mbody, files\u001b[38;5;241m=\u001b[39mto_httpx_files(files), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions\n\u001b[1;32m   1239\u001b[0m     )\n\u001b[0;32m-> 1240\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(ResponseT, \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m~/miniconda3/envs/selfCorrection/lib/python3.9/site-packages/openai/_base_client.py:921\u001b[0m, in \u001b[0;36mSyncAPIClient.request\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m    912\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrequest\u001b[39m(\n\u001b[1;32m    913\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    914\u001b[0m     cast_to: Type[ResponseT],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    919\u001b[0m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    920\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ResponseT \u001b[38;5;241m|\u001b[39m _StreamT:\n\u001b[0;32m--> 921\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    922\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    923\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    924\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    925\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    926\u001b[0m \u001b[43m        \u001b[49m\u001b[43mremaining_retries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mremaining_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    927\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/selfCorrection/lib/python3.9/site-packages/openai/_base_client.py:952\u001b[0m, in \u001b[0;36mSyncAPIClient._request\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m    949\u001b[0m log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSending HTTP Request: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, request\u001b[38;5;241m.\u001b[39mmethod, request\u001b[38;5;241m.\u001b[39murl)\n\u001b[1;32m    951\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 952\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    953\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    954\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_should_stream_response_body\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    955\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    956\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    957\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m httpx\u001b[38;5;241m.\u001b[39mTimeoutException \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m    958\u001b[0m     log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEncountered httpx.TimeoutException\u001b[39m\u001b[38;5;124m\"\u001b[39m, exc_info\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/miniconda3/envs/selfCorrection/lib/python3.9/site-packages/httpx/_client.py:914\u001b[0m, in \u001b[0;36mClient.send\u001b[0;34m(self, request, stream, auth, follow_redirects)\u001b[0m\n\u001b[1;32m    906\u001b[0m follow_redirects \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    907\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfollow_redirects\n\u001b[1;32m    908\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(follow_redirects, UseClientDefault)\n\u001b[1;32m    909\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m follow_redirects\n\u001b[1;32m    910\u001b[0m )\n\u001b[1;32m    912\u001b[0m auth \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_request_auth(request, auth)\n\u001b[0;32m--> 914\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_send_handling_auth\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    915\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    916\u001b[0m \u001b[43m    \u001b[49m\u001b[43mauth\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mauth\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    917\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    918\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhistory\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    919\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    920\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    921\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m stream:\n",
      "File \u001b[0;32m~/miniconda3/envs/selfCorrection/lib/python3.9/site-packages/httpx/_client.py:942\u001b[0m, in \u001b[0;36mClient._send_handling_auth\u001b[0;34m(self, request, auth, follow_redirects, history)\u001b[0m\n\u001b[1;32m    939\u001b[0m request \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(auth_flow)\n\u001b[1;32m    941\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 942\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_send_handling_redirects\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    943\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    944\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    945\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhistory\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhistory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    946\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    947\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    948\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/envs/selfCorrection/lib/python3.9/site-packages/httpx/_client.py:979\u001b[0m, in \u001b[0;36mClient._send_handling_redirects\u001b[0;34m(self, request, follow_redirects, history)\u001b[0m\n\u001b[1;32m    976\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_event_hooks[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrequest\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[1;32m    977\u001b[0m     hook(request)\n\u001b[0;32m--> 979\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_send_single_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    980\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    981\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_event_hooks[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresponse\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n",
      "File \u001b[0;32m~/miniconda3/envs/selfCorrection/lib/python3.9/site-packages/httpx/_client.py:1015\u001b[0m, in \u001b[0;36mClient._send_single_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m   1010\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m   1011\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAttempted to send an async request with a sync Client instance.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1012\u001b[0m     )\n\u001b[1;32m   1014\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m request_context(request\u001b[38;5;241m=\u001b[39mrequest):\n\u001b[0;32m-> 1015\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mtransport\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1017\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response\u001b[38;5;241m.\u001b[39mstream, SyncByteStream)\n\u001b[1;32m   1019\u001b[0m response\u001b[38;5;241m.\u001b[39mrequest \u001b[38;5;241m=\u001b[39m request\n",
      "File \u001b[0;32m~/miniconda3/envs/selfCorrection/lib/python3.9/site-packages/httpx/_transports/default.py:233\u001b[0m, in \u001b[0;36mHTTPTransport.handle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    220\u001b[0m req \u001b[38;5;241m=\u001b[39m httpcore\u001b[38;5;241m.\u001b[39mRequest(\n\u001b[1;32m    221\u001b[0m     method\u001b[38;5;241m=\u001b[39mrequest\u001b[38;5;241m.\u001b[39mmethod,\n\u001b[1;32m    222\u001b[0m     url\u001b[38;5;241m=\u001b[39mhttpcore\u001b[38;5;241m.\u001b[39mURL(\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    230\u001b[0m     extensions\u001b[38;5;241m=\u001b[39mrequest\u001b[38;5;241m.\u001b[39mextensions,\n\u001b[1;32m    231\u001b[0m )\n\u001b[1;32m    232\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m map_httpcore_exceptions():\n\u001b[0;32m--> 233\u001b[0m     resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_pool\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreq\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    235\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(resp\u001b[38;5;241m.\u001b[39mstream, typing\u001b[38;5;241m.\u001b[39mIterable)\n\u001b[1;32m    237\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m Response(\n\u001b[1;32m    238\u001b[0m     status_code\u001b[38;5;241m=\u001b[39mresp\u001b[38;5;241m.\u001b[39mstatus,\n\u001b[1;32m    239\u001b[0m     headers\u001b[38;5;241m=\u001b[39mresp\u001b[38;5;241m.\u001b[39mheaders,\n\u001b[1;32m    240\u001b[0m     stream\u001b[38;5;241m=\u001b[39mResponseStream(resp\u001b[38;5;241m.\u001b[39mstream),\n\u001b[1;32m    241\u001b[0m     extensions\u001b[38;5;241m=\u001b[39mresp\u001b[38;5;241m.\u001b[39mextensions,\n\u001b[1;32m    242\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/envs/selfCorrection/lib/python3.9/site-packages/httpcore/_sync/connection_pool.py:216\u001b[0m, in \u001b[0;36mConnectionPool.handle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    213\u001b[0m         closing \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_assign_requests_to_connections()\n\u001b[1;32m    215\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_close_connections(closing)\n\u001b[0;32m--> 216\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exc \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    218\u001b[0m \u001b[38;5;66;03m# Return the response. Note that in this case we still have to manage\u001b[39;00m\n\u001b[1;32m    219\u001b[0m \u001b[38;5;66;03m# the point at which the response is closed.\u001b[39;00m\n\u001b[1;32m    220\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response\u001b[38;5;241m.\u001b[39mstream, Iterable)\n",
      "File \u001b[0;32m~/miniconda3/envs/selfCorrection/lib/python3.9/site-packages/httpcore/_sync/connection_pool.py:196\u001b[0m, in \u001b[0;36mConnectionPool.handle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    192\u001b[0m connection \u001b[38;5;241m=\u001b[39m pool_request\u001b[38;5;241m.\u001b[39mwait_for_connection(timeout\u001b[38;5;241m=\u001b[39mtimeout)\n\u001b[1;32m    194\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    195\u001b[0m     \u001b[38;5;66;03m# Send the request on the assigned connection.\u001b[39;00m\n\u001b[0;32m--> 196\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mconnection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    197\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpool_request\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\n\u001b[1;32m    198\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    199\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m ConnectionNotAvailable:\n\u001b[1;32m    200\u001b[0m     \u001b[38;5;66;03m# In some cases a connection may initially be available to\u001b[39;00m\n\u001b[1;32m    201\u001b[0m     \u001b[38;5;66;03m# handle a request, but then become unavailable.\u001b[39;00m\n\u001b[1;32m    202\u001b[0m     \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[1;32m    203\u001b[0m     \u001b[38;5;66;03m# In this case we clear the connection and try again.\u001b[39;00m\n\u001b[1;32m    204\u001b[0m     pool_request\u001b[38;5;241m.\u001b[39mclear_connection()\n",
      "File \u001b[0;32m~/miniconda3/envs/selfCorrection/lib/python3.9/site-packages/httpcore/_sync/connection.py:101\u001b[0m, in \u001b[0;36mHTTPConnection.handle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m     98\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_connect_failed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m     99\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exc\n\u001b[0;32m--> 101\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_connection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/selfCorrection/lib/python3.9/site-packages/httpcore/_sync/http11.py:143\u001b[0m, in \u001b[0;36mHTTP11Connection.handle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    141\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m Trace(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresponse_closed\u001b[39m\u001b[38;5;124m\"\u001b[39m, logger, request) \u001b[38;5;28;01mas\u001b[39;00m trace:\n\u001b[1;32m    142\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_response_closed()\n\u001b[0;32m--> 143\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m exc\n",
      "File \u001b[0;32m~/miniconda3/envs/selfCorrection/lib/python3.9/site-packages/httpcore/_sync/http11.py:113\u001b[0m, in \u001b[0;36mHTTP11Connection.handle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    102\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m    104\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m Trace(\n\u001b[1;32m    105\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreceive_response_headers\u001b[39m\u001b[38;5;124m\"\u001b[39m, logger, request, kwargs\n\u001b[1;32m    106\u001b[0m ) \u001b[38;5;28;01mas\u001b[39;00m trace:\n\u001b[1;32m    107\u001b[0m     (\n\u001b[1;32m    108\u001b[0m         http_version,\n\u001b[1;32m    109\u001b[0m         status,\n\u001b[1;32m    110\u001b[0m         reason_phrase,\n\u001b[1;32m    111\u001b[0m         headers,\n\u001b[1;32m    112\u001b[0m         trailing_data,\n\u001b[0;32m--> 113\u001b[0m     ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_receive_response_headers\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    114\u001b[0m     trace\u001b[38;5;241m.\u001b[39mreturn_value \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    115\u001b[0m         http_version,\n\u001b[1;32m    116\u001b[0m         status,\n\u001b[1;32m    117\u001b[0m         reason_phrase,\n\u001b[1;32m    118\u001b[0m         headers,\n\u001b[1;32m    119\u001b[0m     )\n\u001b[1;32m    121\u001b[0m network_stream \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_network_stream\n",
      "File \u001b[0;32m~/miniconda3/envs/selfCorrection/lib/python3.9/site-packages/httpcore/_sync/http11.py:186\u001b[0m, in \u001b[0;36mHTTP11Connection._receive_response_headers\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    183\u001b[0m timeout \u001b[38;5;241m=\u001b[39m timeouts\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mread\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    185\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 186\u001b[0m     event \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_receive_event\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    187\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(event, h11\u001b[38;5;241m.\u001b[39mResponse):\n\u001b[1;32m    188\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/selfCorrection/lib/python3.9/site-packages/httpcore/_sync/http11.py:224\u001b[0m, in \u001b[0;36mHTTP11Connection._receive_event\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    221\u001b[0m     event \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_h11_state\u001b[38;5;241m.\u001b[39mnext_event()\n\u001b[1;32m    223\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m event \u001b[38;5;129;01mis\u001b[39;00m h11\u001b[38;5;241m.\u001b[39mNEED_DATA:\n\u001b[0;32m--> 224\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_network_stream\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    225\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mREAD_NUM_BYTES\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\n\u001b[1;32m    226\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    228\u001b[0m     \u001b[38;5;66;03m# If we feed this case through h11 we'll raise an exception like:\u001b[39;00m\n\u001b[1;32m    229\u001b[0m     \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[1;32m    230\u001b[0m     \u001b[38;5;66;03m#     httpcore.RemoteProtocolError: can't handle event type\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    234\u001b[0m     \u001b[38;5;66;03m# perspective. Instead we handle this case distinctly and treat\u001b[39;00m\n\u001b[1;32m    235\u001b[0m     \u001b[38;5;66;03m# it as a ConnectError.\u001b[39;00m\n\u001b[1;32m    236\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m data \u001b[38;5;241m==\u001b[39m \u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_h11_state\u001b[38;5;241m.\u001b[39mtheir_state \u001b[38;5;241m==\u001b[39m h11\u001b[38;5;241m.\u001b[39mSEND_RESPONSE:\n",
      "File \u001b[0;32m~/miniconda3/envs/selfCorrection/lib/python3.9/site-packages/httpcore/_backends/sync.py:126\u001b[0m, in \u001b[0;36mSyncStream.read\u001b[0;34m(self, max_bytes, timeout)\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m map_exceptions(exc_map):\n\u001b[1;32m    125\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sock\u001b[38;5;241m.\u001b[39msettimeout(timeout)\n\u001b[0;32m--> 126\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmax_bytes\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "class FixedTestSizeCV:\n",
    "    def __init__(self, num_test_points):\n",
    "        self.num_test_points = num_test_points\n",
    "\n",
    "    def split(self, X, y=None):\n",
    "        n_samples = len(X)\n",
    "        indices = np.arange(n_samples)\n",
    "        np.random.shuffle(indices)\n",
    "        test_indices = indices[:self.num_test_points]\n",
    "        train_indices = indices[self.num_test_points:]\n",
    "        yield train_indices, test_indices\n",
    "\n",
    "sorted_df = brca_report.reset_index(drop=True)\n",
    "\n",
    "for size in range(10, 101, 10):\n",
    "    cv = FixedTestSizeCV(num_test_points=size)\n",
    "    for test_idx, train_idx in cv.split(sorted_df):\n",
    "        memory_agent = ConditionalMemoryAgent(client=client, model=\"mistralai/Mixtral-8x7B-Instruct-v0.1\",\n",
    "                           prompt_template_dict={\"initialized_prompt\":initial_predict_prompt,\n",
    "                                                 \"learning_prompt\":subsequent_predict_prompt,\n",
    "                                                 \"testing_prompt\":testing_predict_prompt},\n",
    "                           schema_dict={\"learning_schema\":training_schema,\n",
    "                                        \"testing_schema\":testing_schema},\n",
    "                                        label = \"n\")\n",
    "        df_train, df_test = sorted_df.iloc[train_idx], sorted_df.iloc[test_idx]\n",
    "        train_result = memory_agent.train(df_train)\n",
    "        train_result.to_csv(f\"saturation_train_result_{size}.csv\", index=False)\n",
    "        test_result = memory_agent.test(df_test)\n",
    "        test_result.to_csv(f\"saturation_test_result_{size}.csv\", index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for size in [10, 20, 30, 40]:\n",
    "    result_df = pd.read_csv(f\"saturation_test_result_{size}.csv\")\n",
    "    n03_performance_report(df=result_df, ans_col=\"mem_n_ans_str\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 Multi-Turn Conversation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiTurnMemoryAgent:\n",
    "    def __init__(self, client: OpenAI, model: str, \n",
    "                 prompt_template_dict: dict[str, str], schema_dict: dict, label: str) -> None:\n",
    "        self.client = client\n",
    "        self.model = model\n",
    "        self.prompt_template_dict = prompt_template_dict\n",
    "        self.schema_dict = schema_dict\n",
    "        self.label = label\n",
    "        self.memory = \"\"\n",
    "    \n",
    "    def is_updated(self, new_memory):\n",
    "        old_str = \"\\n\".join(self.memory)\n",
    "        new_str = \"\\n\".join(new_memory)\n",
    "        if fuzz.ratio(old_str, new_str) >= 80 : \n",
    "            return True # update memory\n",
    "        else:\n",
    "            return False\n",
    "        \n",
    "    def get_schema_followed_response(self, messages: list, schema:dict, temperature:float) -> Union[Dict, None]:\n",
    "        response = self.client.chat.completions.create(\n",
    "            model=self.model,\n",
    "            messages=messages,\n",
    "            extra_body=schema,\n",
    "            temperature = temperature\n",
    "        )\n",
    "        return response.choices[0].message.content.replace(\"\\\\\", \"\\\\\\\\\")\n",
    "        \n",
    "    def train(self, training_dataset: pd.DataFrame, conditional_update: bool, temperature: float = 0.0) -> pd.DataFrame:\n",
    "        parsing_error = 0\n",
    "        num_update = 0\n",
    "        update_condition = True\n",
    "        for idx, row in training_dataset.iterrows():\n",
    "            report = row[\"text\"]\n",
    "            if self.memory == \"\":\n",
    "                prediction_prompt = self.prompt_template_dict[\"initialized_prompt\"].format(report=report)\n",
    "            else:\n",
    "                prediction_prompt = self.prompt_template_dict[\"prediction_prompt\"].format(memory=self.memory, report=report)\n",
    "\n",
    "            messages = [{\"role\": \"user\", \"content\": prediction_prompt}]\n",
    "            ans_format = {\"guided_choice\":self.schema_dict[\"predic_choice\"]}\n",
    "            pred = self.get_schema_followed_response(messages, ans_format, temperature)\n",
    "            training_dataset.loc[idx, f\"{self.label}_pred\"] = pred\n",
    " \n",
    "            reason_prompt = self.prompt_template_dict[\"reason_prompt\"]\n",
    "            messages.append({\"role\": \"assistant\", \"content\": pred}, \n",
    "                            {\"role\": \"user\", \"content\": reason_prompt})\n",
    "            ans_format = {\"guided_json\":self.schema_dict[\"reason_schema\"]}\n",
    "            try:\n",
    "                reason = json.loads(self.get_schema_followed_response(messages, ans_format, temperature))['reasoning']\n",
    "                training_dataset.loc[idx, f\"{self.label}_reason\"] = reason  \n",
    "            except json.JSONDecodeError:\n",
    "                parsing_error += 1\n",
    "                continue\n",
    "    \n",
    "            rule_prompt = self.prompt_template_dict[\"rule_prompt\"]\n",
    "            messages.append({\"role\": \"assistant\", \"content\": reason}, \n",
    "                            {\"role\": \"user\", \"content\": rule_prompt})\n",
    "            ans_format = {\"guided_json\":self.schema_dict[\"rule_schema\"]}\n",
    "            try:\n",
    "                rule = json.loads(self.get_schema_followed_response(messages, ans_format, temperature))['rules']\n",
    "                training_dataset.loc[idx, f\"{self.label}_rule\"] = rule\n",
    "            except json.JSONDecodeError:\n",
    "                parsing_error += 1\n",
    "                continue\n",
    "                \n",
    "            if conditional_update == True:\n",
    "                update_condition = self.is_updated(rule)\n",
    "\n",
    "            if self.memory == \"\" or update_condition:\n",
    "                self.memory = rule\n",
    "                num_update += 1\n",
    "                \n",
    "        print(f\"Number of parsing errors: {parsing_error}\")\n",
    "        print(f\"Number of memory updates: {num_update}\")\n",
    "        return training_dataset\n",
    "    \n",
    "    def test(self, testing_dataset: pd.DataFrame, temperature: float = 0.0) -> pd.DataFrame:\n",
    "        parsing_error = 0\n",
    "        for idx, row in testing_dataset.iterrows():\n",
    "            report = row[\"text\"]\n",
    "            prediction_prompt = self.prompt_template_dict[\"prediction_prompt\"].format(memory=self.memory, report=report)\n",
    "            messages = [{\"role\": \"user\", \"content\": prediction_prompt}]\n",
    "            ans_format = {\"guided_choice\":self.schema_dict[\"predic_choice\"]}\n",
    "            pred = self.get_schema_followed_response(messages, ans_format, temperature)\n",
    "            testing_dataset.loc[idx, f\"{self.label}_pred\"] = pred\n",
    "            \n",
    "            reason_prompt = self.prompt_template_dict[\"reason_prompt\"]\n",
    "            messages.append({\"role\": \"assistant\", \"content\": pred}, \n",
    "                            {\"role\": \"user\", \"content\": reason_prompt})\n",
    "            ans_format = {\"guided_json\":self.schema_dict[\"reason_schema\"]}\n",
    "            try:\n",
    "                reason = json.loads(self.get_schema_followed_response(messages, ans_format, temperature))['reasoning']\n",
    "                testing_dataset.loc[idx, f\"{self.label}_reason\"] = reason  \n",
    "            except json.JSONDecodeError:\n",
    "                parsing_error += 1\n",
    "                continue\n",
    "       \n",
    "        print(f\"Number of parsing errors: {parsing_error}\")\n",
    "        return testing_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_choices=[\"N0\", \"N1\", \"N2\", \"N3\"]\n",
    "\n",
    "class ReasonSchema(BaseModel):\n",
    "    reasoning: str = Field(description=\"reasoning to support predicted cancer stage\")     \n",
    "reason_schema = ReasonSchema.model_json_schema()\n",
    "\n",
    "class RuleSchema(BaseModel):\n",
    "    rules: List[str] = Field(description=\"list of rules\")\n",
    "rule_schema = RuleSchema.model_json_schema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multi_turn_agent = MultiTurMemoryAgent(client=client, model=\"mistralai/Mixtral-8x7B-Instruct-v0.1\",\n",
    "                           prompt_template_dict={\"initialized_prompt\":initial_predict_prompt,\n",
    "                                                 \"prediction_prompt\":subsequent_predict_prompt,\n",
    "                                                 \"reason_prompt\":reason_prompt,\n",
    "                                                 \"rule_prompt\":rule_prompt},\n",
    "                           schema_dict={\"predic_choice\":pred_choices,\n",
    "                                        \"reason_schema\":reason_schema,\n",
    "                                        \"rule_schema\":rule_schema},\n",
    "                                        label = \"n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 998 Experiemntal Plan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run T task only\n",
    "T_memory_agent = MemoryAgent(client=client, model=\"mistralai/Mixtral-8x7B-Instruct-v0.1\",\n",
    "                           prompt_template_dict={\"initialized_prompt\":initial_predict_prompt,\n",
    "                                                 \"learning_prompt\":subsequent_predict_prompt,\n",
    "                                                 \"testing_prompt\":testing_predict_prompt},\n",
    "                           schema_dict={\"learning_schema\":training_schema,\n",
    "                                        \"testing_schema\":testing_schema})\n",
    "\n",
    "# run N task only\n",
    "N_memory_agent = MemoryAgent(client=client, model=...,\n",
    "                           prompt_template_dict=...,\n",
    "                           schema_dict=...)\n",
    "\n",
    "# run T and N task simultaneously\n",
    "TN_memory_agent = MultiTaskMemoryAgent(client=client, model=...,\n",
    "                           prompt_template_dict=...,\n",
    "                           schema_dict=...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "\"\"\" the following pseudo codes are for checking number of instances are sufficient\n",
    "\"\"\"\n",
    "def stratified_sampling_function(pd.dataframe, target_column, size):\n",
    "    pass\n",
    "\n",
    "def eval():\n",
    "    pass\n",
    "\n",
    "def report():\n",
    "    pass\n",
    "\n",
    "performances = {}\n",
    "for size in [5, 10, 20, 25, 30, 35, 40]:\n",
    "    evaluated_scores = []\n",
    "    # the process should be evaluated on K different splits and take the average performance\n",
    "    for i, (train_indexes, test_indexes) in stratified_sampling_function(df): \n",
    "        \n",
    "        # the memory is unique for each split\n",
    "        N_memory_agent = MemoryAgent(client=client, model=...,\n",
    "                           prompt_template_dict=...,\n",
    "                           schema_dict=...)\n",
    "\n",
    "        df_train = df.iloc[train_indexes,:]\n",
    "        N_memory_agent.train(df_train)\n",
    "        df_test = df.iloc[test_indexes,:]\n",
    "        df_results = N_memory_agent.test(df_test)\n",
    "        evaluated_scores.append(eval(df_results))\n",
    "\n",
    "    performances[size] = report(evaluated_scores)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 999 (temporary) Memory saturation test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_memory(file_path):\n",
    "    with open(file_path, 'r') as file:\n",
    "        content = file.read()\n",
    "    matches = re.findall(r\" memory: (\\[.*?\\])\", content, re.DOTALL)\n",
    "    memory_lst = [ast.literal_eval(match) for match in matches]\n",
    "    return memory_lst\n",
    "\n",
    "file_path = '/home/yl3427/cylab/selfCorrectionAgent/memory.txt'\n",
    "\n",
    "memories = find_memory(file_path)\n",
    "\n",
    "print(len(memories))\n",
    "print(memories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TestAgent:\n",
    "    def __init__(self, client: OpenAI, model: str, \n",
    "                 prompt_template: str, schema, memory_lst: List[str]) -> None:\n",
    "        self.client = client\n",
    "        self.model = model\n",
    "        self.prompt_template = prompt_template\n",
    "        self.schema = schema\n",
    "        self.memory_lst = memory_lst\n",
    "\n",
    "\n",
    "    def get_schema_followed_response(self, messages: list, schema:dict, temperature:float) -> Union[Dict, None]:\n",
    "        try:\n",
    "            response = self.client.chat.completions.create(\n",
    "                model=self.model,\n",
    "                messages=messages,\n",
    "                extra_body={\"guided_json\":schema},\n",
    "                temperature = temperature\n",
    "            )\n",
    "            return json.loads(response.choices[0].message.content.replace(\"\\\\\", \"\\\\\\\\\"))\n",
    "        except json.JSONDecodeError:\n",
    "            return None\n",
    "\n",
    "    \n",
    "    def test(self, testing_dataset: pd.DataFrame, temperature: float = 0.0):\n",
    "        for mem_idx, memory in enumerate(self.memory_lst):\n",
    "            print(f\"memory index: {mem_idx}\")\n",
    "            parsing_error = 0\n",
    "            correct_count = 0\n",
    "            incorrect_count = 0\n",
    "            for report_idx, row in testing_dataset.iterrows():\n",
    "                # print(report_idx)\n",
    "                report = row[\"text\"]\n",
    "                label = row[\"t\"]\n",
    "\n",
    "                prompt = self.prompt_template.format(memory=memory, report=report)\n",
    "                system_prompt = system_instruction+ \"\\n\" + prompt\n",
    "                messages = [{\"role\": \"user\", \"content\": system_prompt}]\n",
    "\n",
    "                json_output = self.get_schema_followed_response(messages, self.schema, temperature)\n",
    "\n",
    "                if not json_output:\n",
    "                    parsing_error += 1\n",
    "                    continue\n",
    "                \n",
    "                if f\"T{label+1}\" == json_output[\"predictedStage\"]:\n",
    "                    correct_count += 1\n",
    "                else:\n",
    "                    incorrect_count +=1\n",
    "\n",
    "            print(f\"\\tcorrect: {correct_count}\")\n",
    "            print(f\"\\twrong: {incorrect_count}\")\n",
    "            print(f\"\\tparsing error: {parsing_error}\")\n",
    "            print()\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "openai_api_key = \"Empty\"\n",
    "openai_api_base = \"http://localhost:8000/v1\"\n",
    "\n",
    "client = OpenAI(\n",
    "    api_key=openai_api_key,\n",
    "    base_url=openai_api_base,\n",
    ")\n",
    "\n",
    "brca_report = pd.read_csv(\"/secure/shared_data/rag_tnm_results/summary/5_folds_summary/brca_df.csv\")\n",
    "df_testing = brca_report.iloc[500:600, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df_testing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TestingResponse(BaseModel):\n",
    "    predictedStage: str = Field(description=\"predicted cancer stage\")\n",
    "    reasoning: str = Field(description=\"reasoning to support predicted cancer stage\") \n",
    "testing_schema = TestingResponse.model_json_schema()\n",
    "\n",
    "memory_agent = TestAgent(client=client, model=\"mistralai/Mixtral-8x7B-Instruct-v0.1\",\n",
    "                           prompt_template = testing_predict_prompt,\n",
    "                           schema = testing_schema,memory_lst = memories)\n",
    "memory_agent.test(df_testing)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "reflection",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.-1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
