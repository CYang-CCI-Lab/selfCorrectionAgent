{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0 Model for testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "bat"
    }
   },
   "source": [
    "```bash\n",
    "CUDA_VISIBLE_DEVICES=\"4,5\", python -m vllm.entrypoints.openai.api_server \\\n",
    "--model mistralai/Mistral-7B-Instruct-v0.2 \\\n",
    "--disable-custom-all-reduce \\\n",
    "--enforce-eager \\\n",
    "--download-dir /secure/chiahsuan/hf_cache/ \\\n",
    "--tensor-parallel-size 2 \\\n",
    "--port 8085\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```bash\n",
    "CUDA_VISIBLE_DEVICES=\"0,1,2,3\", python -m vllm.entrypoints.openai.api_server \\\n",
    "--model mistralai/Mixtral-8x7B-Instruct-v0.1 \\\n",
    "--download-dir /secure/chiahsuan/hf_cache/ \\\n",
    "--tensor-parallel-size 4\n",
    "```\n",
    "mistralai/Mixtral-8x7B-Instruct-v0.1: always port 8080"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 Load packages and define class\n",
    "== The following two cells can be organized into a separate py file as a module.=="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yl3427/miniconda3/envs/selfCorrection/lib/python3.9/site-packages/fuzzywuzzy/fuzz.py:11: UserWarning: Using slow pure-python SequenceMatcher. Install python-Levenshtein to remove this warning\n",
      "  warnings.warn('Using slow pure-python SequenceMatcher. Install python-Levenshtein to remove this warning')\n"
     ]
    }
   ],
   "source": [
    "from prompt import system_instruction\n",
    "from prompt import baseline_prompt_n03 as baseline_prompt\n",
    "from prompt import initial_predict_prompt_n03 as initial_predict_prompt\n",
    "from prompt import subsequent_predict_prompt_n03 as subsequent_predict_prompt\n",
    "from prompt import testing_predict_prompt_n03 as testing_predict_prompt\n",
    "from metrics import n03_performance_report\n",
    "\n",
    "import openai\n",
    "from openai import OpenAI\n",
    "from sklearn.model_selection import KFold\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import json\n",
    "from pydantic import BaseModel, Field\n",
    "from typing import List, Dict, Union\n",
    "from fuzzywuzzy import fuzz\n",
    "import re\n",
    "import ast"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Define ChoiceAgent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define class\n",
    "class ChoiceAgent:\n",
    "    \"\"\" the simplest agent, which is appropriate for zero-shot prompting\n",
    "    \"\"\"\n",
    "    def __init__(self, client: OpenAI, model: str, \n",
    "                 prompt_template: str, choices: dict, label: str) -> None:\n",
    "        self.client = client\n",
    "        self.model = model\n",
    "        self.prompt_template = prompt_template\n",
    "        self.choices = choices\n",
    "        self.label = label\n",
    "\n",
    "    def get_response_from_choices(self, messages: list, temperature:float) -> str:\n",
    "        response = self.client.chat.completions.create(\n",
    "            model=self.model,\n",
    "            messages=messages,\n",
    "            extra_body={\"guided_choice\":self.choices},\n",
    "            temperature = temperature\n",
    "        )\n",
    "        return response.choices[0].message.content\n",
    "\n",
    "    def run(self, dataset: pd.DataFrame, temperature: float = 0.0) -> pd.DataFrame:\n",
    "        pbar = tqdm(total=dataset.shape[0])\n",
    "        for idx, row in dataset.iterrows():\n",
    "            report = row[\"text\"]\n",
    "            prompt = self.prompt_template.format(report=report)\n",
    "            system_prompt = system_instruction+ \"\\n\" + prompt\n",
    "            messages = [{\"role\": \"user\", \"content\": system_prompt}]\n",
    "            answer = self.get_response_from_choices(messages, temperature)\n",
    "            dataset.loc[idx, f\"zs_{self.label}_ans_str\"] = answer\n",
    "            pbar.update(1)\n",
    "        pbar.close()\n",
    "\n",
    "        return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Define MemoryAgent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MemoryAgent:\n",
    "    \"\"\" the implementation of memory agent, which learn memory from training set and\n",
    "    utilize memory as contexts for testing set.\n",
    "    \"\"\"\n",
    "    def __init__(self, client: OpenAI, model: str, \n",
    "                 prompt_template_dict: dict[str, str], schema_dict: dict, label: str) -> None:\n",
    "        self.client = client\n",
    "        self.model = model\n",
    "        self.prompt_template_dict = prompt_template_dict\n",
    "        self.validate_prompt_template()\n",
    "        self.schema_dict = schema_dict\n",
    "        self.validate_schema()\n",
    "        self.memory = \"\"\n",
    "        self.label = label\n",
    "\n",
    "    def validate_prompt_template(self) -> None:\n",
    "        keys = self.prompt_template_dict.keys()\n",
    "        initial_prompt_exist = \"initialized_prompt\" in keys\n",
    "        learning_prompt_exist = \"learning_prompt\" in keys\n",
    "        testing_prompt_exist = \"testing_prompt\" in keys\n",
    "        assert True == initial_prompt_exist == learning_prompt_exist == testing_prompt_exist, \\\n",
    "        \"You should provide a dict with initialized_prompt, learning_prompt, and testing_prompt as keys.\"\n",
    "\n",
    "    def validate_schema(self) -> None:\n",
    "        keys = self.schema_dict.keys()\n",
    "        learning_schema_exist = \"learning_schema\" in keys\n",
    "        testing_schema_exist = \"testing_schema\" in keys\n",
    "        assert True == learning_schema_exist == testing_schema_exist, \\\n",
    "        \"You should provide a dict with learning_schema and testing_schema as keys.\"\n",
    "\n",
    "    def get_schema_followed_response(self, messages: list, schema:dict, temperature:float) -> Union[Dict, None]:\n",
    "        num_attempt = 3\n",
    "        for attempt in range(num_attempt):\n",
    "            try:\n",
    "                response = self.client.chat.completions.create(\n",
    "                    model=self.model,\n",
    "                    messages=messages,\n",
    "                    extra_body={\"guided_json\":schema},\n",
    "                    temperature = temperature\n",
    "                )\n",
    "                return json.loads(response.choices[0].message.content.replace(\"\\\\\", \"\\\\\\\\\"))\n",
    "            except json.JSONDecodeError:\n",
    "                print(\"Error decoding JSON response\")\n",
    "                return None\n",
    "            except openai.APITimeoutError:\n",
    "                if attempt < (num_attempt -1):\n",
    "                    wait_time = 2 * (attempt + 1)\n",
    "                    print(f\"Request timed out. Retrying in {wait_time} seconds...\")\n",
    "                    time.sleep(wait_time)\n",
    "                else:\n",
    "                    print(\"Max retries reached. Request faild.\")\n",
    "                    return None\n",
    "        \n",
    "    def train(self, training_dataset: pd.DataFrame, temperature: float = 0.0) -> pd.DataFrame:\n",
    "        pbar = tqdm(total=training_dataset.shape[0])\n",
    "        parsing_error = 0\n",
    "        for idx, row in training_dataset.iterrows():\n",
    "\n",
    "            report = row[\"text\"]\n",
    "\n",
    "            if self.memory == \"\":\n",
    "                prompt = self.prompt_template_dict[\"initialized_prompt\"].format(report=report)\n",
    "            else:\n",
    "                prompt = self.prompt_template_dict[\"learning_prompt\"].format(memory=self.memory, report=report)\n",
    "            \n",
    "            system_prompt = system_instruction+ \"\\n\" + prompt\n",
    "            messages = [{\"role\": \"user\", \"content\": system_prompt}]\n",
    "\n",
    "            json_output = self.get_schema_followed_response(messages, self.schema_dict[\"learning_schema\"], temperature)\n",
    "\n",
    "            if not json_output:\n",
    "                parsing_error += 1\n",
    "                continue\n",
    "            \n",
    "            self.memory = json_output['rules']\n",
    "            training_dataset.loc[idx, f\"mem_{self.label}_reasoning\"] = json_output['reasoning']\n",
    "            training_dataset.loc[idx, f\"mem_{self.label}_ans_str\"] = json_output['predictedStage']\n",
    "            \n",
    "            pbar.update(1)\n",
    "        pbar.close()\n",
    "        print(f\"Number of parsing errors: {parsing_error}\")\n",
    "        return training_dataset\n",
    "    \n",
    "    def test(self, testing_dataset: pd.DataFrame, temperature: float = 0.0) -> pd.DataFrame:\n",
    "        pbar = tqdm(total=testing_dataset.shape[0])\n",
    "        parsing_error = 0\n",
    "        for idx, row in testing_dataset.iterrows():\n",
    "\n",
    "            report = row[\"text\"]\n",
    "\n",
    "            prompt = self.prompt_template_dict[\"testing_prompt\"].format(memory=self.memory, report=report)\n",
    "            system_prompt = system_instruction+ \"\\n\" + prompt\n",
    "            messages = [{\"role\": \"user\", \"content\": system_prompt}]\n",
    "\n",
    "            json_output = self.get_schema_followed_response(messages, self.schema_dict[\"testing_schema\"], temperature)\n",
    "\n",
    "            if not json_output:\n",
    "                parsing_error += 1\n",
    "                continue\n",
    "            \n",
    "            testing_dataset.loc[idx, f\"mem_{self.label}_reasoning\"] = json_output['reasoning']\n",
    "            testing_dataset.loc[idx, f\"mem_{self.label}_ans_str\"] = json_output['predictedStage']\n",
    "            \n",
    "            pbar.update(1)\n",
    "        pbar.close()\n",
    "        print(f\"Number of parsing errors: {parsing_error}\")\n",
    "        return testing_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Define ConditionalMemoryAgent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConditionalMemoryAgent(MemoryAgent):\n",
    "  \n",
    "  def __init__(self, client: OpenAI, model: str, \n",
    "                 prompt_template_dict: dict[str, str], schema_dict: dict, label: str) -> None:\n",
    "    # inherit all properties and methods from MemoryAgent\n",
    "    super().__init__(client, model, prompt_template_dict, schema_dict, label)\n",
    "  \n",
    "  def is_updated(self, new_memory, threshold):\n",
    "    old_str = \"\\n\".join(self.memory)\n",
    "    new_str = \"\\n\".join(new_memory)\n",
    "    if fuzz.ratio(old_str, new_str) >= threshold : \n",
    "        return True # update memory\n",
    "    else:\n",
    "        return False\n",
    "    \n",
    "  def train(self, training_dataset: pd.DataFrame, temperature: float = 0.0, threshold: float = 80) -> pd.DataFrame:\n",
    "    # only overide this function because the rest parts are the same\n",
    "    pbar = tqdm(total=training_dataset.shape[0])\n",
    "    parsing_error = 0\n",
    "    num_update = 0\n",
    "    for idx, row in training_dataset.iterrows():\n",
    "\n",
    "        report = row[\"text\"]\n",
    "\n",
    "        if self.memory == \"\":\n",
    "            prompt = self.prompt_template_dict[\"initialized_prompt\"].format(report=report)\n",
    "        else:\n",
    "            prompt = self.prompt_template_dict[\"learning_prompt\"].format(memory=self.memory, report=report)\n",
    "        \n",
    "        system_prompt = system_instruction+ \"\\n\" + prompt\n",
    "        messages = [{\"role\": \"user\", \"content\": system_prompt}]\n",
    "\n",
    "        json_output = self.get_schema_followed_response(messages, self.schema_dict[\"learning_schema\"], temperature)\n",
    "\n",
    "        if not json_output:\n",
    "            parsing_error += 1\n",
    "            continue\n",
    "        \n",
    "        if self.memory == \"\":\n",
    "           self.memory = json_output['rules']\n",
    "        else:\n",
    "          current_memory_str = \"\\n\".join(self.memory)\n",
    "          new_memory_str = \"\\n\".join(json_output['rules'])\n",
    "          if fuzz.ratio(current_memory_str, new_memory_str) >= threshold :\n",
    "            self.memory = json_output['rules']\n",
    "            num_update += 1\n",
    "\n",
    "        training_dataset.loc[idx, f\"cmem_{self.label}_reasoning\"] = json_output['reasoning']\n",
    "        training_dataset.loc[idx, f\"cmem_{self.label}_ans_str\"] = json_output['predictedStage']\n",
    "        training_dataset.loc[idx, f\"cmem_{self.label}_num_update\"] = num_update\n",
    "        \n",
    "        pbar.update(1)\n",
    "    pbar.close()\n",
    "    print(f\"Number of memory updates: {num_update}\")\n",
    "    print(f\"Number of parsing errors: {parsing_error}\")\n",
    "    return training_dataset\n",
    "\n",
    "  def test(self, testing_dataset: pd.DataFrame, temperature: float = 0.0) -> pd.DataFrame:\n",
    "    pbar = tqdm(total=testing_dataset.shape[0])\n",
    "    parsing_error = 0\n",
    "    for idx, row in testing_dataset.iterrows():\n",
    "\n",
    "        report = row[\"text\"]\n",
    "\n",
    "        prompt = self.prompt_template_dict[\"testing_prompt\"].format(memory=self.memory, report=report)\n",
    "        system_prompt = system_instruction+ \"\\n\" + prompt\n",
    "        messages = [{\"role\": \"user\", \"content\": system_prompt}]\n",
    "\n",
    "        json_output = self.get_schema_followed_response(messages, self.schema_dict[\"testing_schema\"], temperature)\n",
    "\n",
    "        if not json_output:\n",
    "            parsing_error += 1\n",
    "            continue\n",
    "        \n",
    "        testing_dataset.loc[idx, f\"cmem_{self.label}_reasoning\"] = json_output['reasoning']\n",
    "        testing_dataset.loc[idx, f\"cmem_{self.label}_ans_str\"] = json_output['predictedStage']\n",
    "        \n",
    "        pbar.update(1)\n",
    "    pbar.close()\n",
    "    print(f\"Number of parsing errors: {parsing_error}\")\n",
    "    return testing_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4 To-be-Implemented"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiTaskMemoryAgent(MemoryAgent):\n",
    "    def __init__(self, client: OpenAI, model: str, \n",
    "                 prompt_template_dict: dict[str, str], schema_dict: dict) -> None:\n",
    "        # inherit all properties and methods from MemoryAgent\n",
    "        super().__init__(client, model, prompt_template_dict, schema_dict)\n",
    "  \n",
    "    def train(self, training_dataset: pd.DataFrame, temperature: float = 0.0) -> pd.DataFrame:\n",
    "        # overide this function\n",
    "        pass\n",
    "\n",
    "    def test(self, testing_dataset: pd.DataFrame, temperature: float = 0.0) -> pd.DataFrame:\n",
    "        # overide this function\n",
    "        pass\n",
    "\n",
    "class MemoryAgentWithVerifier(MemoryAgent): # inherit from MemoryAgent or MultiTaskMemoryAgent\n",
    "    def __init__(self, client: OpenAI, model: str, \n",
    "                 prompt_template_dict: dict[str, str], schema_dict: dict) -> None:\n",
    "        # inherit all properties and methods from MemoryAgent\n",
    "        super().__init__(client, model, prompt_template_dict, schema_dict)\n",
    "  \n",
    "    def train(self, training_dataset: pd.DataFrame, temperature: float = 0.0) -> pd.DataFrame:\n",
    "        # overide this function\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 Demonstration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "openai_api_key = \"Empty\"\n",
    "openai_api_base = \"http://localhost:8085/v1\"\n",
    "\n",
    "client = OpenAI(\n",
    "    api_key=openai_api_key,\n",
    "    base_url=openai_api_base,\n",
    ")\n",
    "\n",
    "brca_report = pd.read_csv(\"/secure/shared_data/rag_tnm_results/summary/5_folds_summary/brca_df.csv\")\n",
    "print(len(brca_report))\n",
    "brca_report = brca_report[brca_report[\"n\"]!=-1]\n",
    "print(len(brca_report))\n",
    "\n",
    "df_training_samples = brca_report.iloc[:200, :]\n",
    "df_testing_samples = brca_report.iloc[200:, :]\n",
    "\n",
    "print(len(df_training_samples))\n",
    "print(len(df_testing_samples))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Initialize ChoiceAgent and use its instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# zs_agent = ChoiceAgent(client=client, model=\"mistralai/Mistral-7B-Instruct-v0.2\",\n",
    "#                  prompt_template=baseline_prompt,\n",
    "#                  choices=[\"T1\", \"T2\", \"T3\", \"T4\"])\n",
    "   \n",
    "zs_agent = ChoiceAgent(client=client, model=\"mistralai/Mixtral-8x7B-Instruct-v0.1\",\n",
    "                 prompt_template=baseline_prompt,\n",
    "                 choices=[\"N0\", \"N1\", \"N2\", \"N3\"], label = \"n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zs_agent.run(dataset=df_testing_samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Initialize MemoryAgent and use its instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainingResponse(BaseModel):\n",
    "    predictedStage: str = Field(description=\"predicted cancer stage\")\n",
    "    reasoning: str = Field(description=\"reasoning to support predicted cancer stage\") \n",
    "    rules: List[str] = Field(description=\"list of rules\") \n",
    "training_schema = TrainingResponse.model_json_schema()\n",
    "\n",
    "class TestingResponse(BaseModel):\n",
    "    predictedStage: str = Field(description=\"predicted cancer stage\")\n",
    "    reasoning: str = Field(description=\"reasoning to support predicted cancer stage\") \n",
    "testing_schema = TestingResponse.model_json_schema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "memory_agent = MemoryAgent(client=client, model=\"mistralai/Mistral-7B-Instruct-v0.2\",\n",
    "                           prompt_template_dict={\"initialized_prompt\":initial_predict_prompt,\n",
    "                                                 \"learning_prompt\":subsequent_predict_prompt,\n",
    "                                                 \"testing_prompt\":testing_predict_prompt},\n",
    "                           schema_dict={\"learning_schema\":training_schema,\n",
    "                                        \"testing_schema\":testing_schema},\n",
    "                                        label = \"n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "memory_agent.train(df_training_samples, temperature=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "memory_agent.memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(memory_agent.memory))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "memory_agent.test(df_testing_samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Initialize ConditionalMemoryAgent and use its instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conditional_memory_agent = ConditionalMemoryAgent(client=client, model=\"mistralai/Mistral-7B-Instruct-v0.2\",\n",
    "                           prompt_template_dict={\"initialized_prompt\":initial_predict_prompt,\n",
    "                                                 \"learning_prompt\":subsequent_predict_prompt,\n",
    "                                                 \"testing_prompt\":testing_predict_prompt},\n",
    "                           schema_dict={\"learning_schema\":training_schema,\n",
    "                                        \"testing_schema\":testing_schema},\n",
    "                                        label = \"n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conditional_memory_agent.train(df_training_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conditional_memory_agent.memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(conditional_memory_agent.memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conditional_memory_agent.test(df_testing_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_training_samples.to_csv(\"df_training.csv\", index=False)\n",
    "df_testing_samples.to_csv(\"df_testing.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 Memory Saturation Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainingResponse(BaseModel):\n",
    "    predictedStage: str = Field(description=\"predicted cancer stage\")\n",
    "    reasoning: str = Field(description=\"reasoning to support predicted cancer stage\") \n",
    "    rules: List[str] = Field(description=\"list of rules\") \n",
    "training_schema = TrainingResponse.model_json_schema()\n",
    "\n",
    "class TestingResponse(BaseModel):\n",
    "    predictedStage: str = Field(description=\"predicted cancer stage\")\n",
    "    reasoning: str = Field(description=\"reasoning to support predicted cancer stage\") \n",
    "testing_schema = TestingResponse.model_json_schema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FixedTestSizeCV:\n",
    "    def __init__(self, num_test_points):\n",
    "        self.num_test_points = num_test_points\n",
    "\n",
    "    def split(self, X, y=None):\n",
    "        n_samples = len(X)\n",
    "        indices = np.arange(n_samples)\n",
    "        np.random.shuffle(indices)\n",
    "        test_indices = indices[:self.num_test_points]\n",
    "        train_indices = indices[self.num_test_points:]\n",
    "        yield train_indices, test_indices\n",
    "\n",
    "sorted_df = brca_report.reset_index(drop=True)\n",
    "\n",
    "for size in range(10, 101, 10):\n",
    "    cv = FixedTestSizeCV(num_test_points=size)\n",
    "    for test_idx, train_idx in cv.split(sorted_df):\n",
    "        memory_agent = ConditionalMemoryAgent(client=client, model=\"mistralai/Mistral-7B-Instruct-v0.2\",\n",
    "                           prompt_template_dict={\"initialized_prompt\":initial_predict_prompt,\n",
    "                                                 \"learning_prompt\":subsequent_predict_prompt,\n",
    "                                                 \"testing_prompt\":testing_predict_prompt},\n",
    "                           schema_dict={\"learning_schema\":training_schema,\n",
    "                                        \"testing_schema\":testing_schema},\n",
    "                                        label = \"n\")\n",
    "        df_train, df_test = sorted_df.iloc[train_idx], sorted_df.iloc[test_idx]\n",
    "        train_result = memory_agent.train(df_train)\n",
    "        train_result.to_csv(f\"saturation_train_result_{size}.csv\", index=False)\n",
    "        test_result = memory_agent.test(df_test)\n",
    "        test_result.to_csv(f\"saturation_test_result_{size}.csv\", index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for size in [10, 20, 30, 40]:\n",
    "    result_df = pd.read_csv(f\"saturation_test_result_{size}.csv\")\n",
    "    n03_performance_report(df=result_df, ans_col=\"mem_n_ans_str\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 Experiemntal Plan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run T task only\n",
    "T_memory_agent = MemoryAgent(client=client, model=\"mistralai/Mistral-7B-Instruct-v0.2\",\n",
    "                           prompt_template_dict={\"initialized_prompt\":initial_predict_prompt,\n",
    "                                                 \"learning_prompt\":subsequent_predict_prompt,\n",
    "                                                 \"testing_prompt\":testing_predict_prompt},\n",
    "                           schema_dict={\"learning_schema\":training_schema,\n",
    "                                        \"testing_schema\":testing_schema})\n",
    "\n",
    "# run N task only\n",
    "N_memory_agent = MemoryAgent(client=client, model=...,\n",
    "                           prompt_template_dict=...,\n",
    "                           schema_dict=...)\n",
    "\n",
    "# run T and N task simultaneously\n",
    "TN_memory_agent = MultiTaskMemoryAgent(client=client, model=...,\n",
    "                           prompt_template_dict=...,\n",
    "                           schema_dict=...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "\"\"\" the following pseudo codes are for checking number of instances are sufficient\n",
    "\"\"\"\n",
    "def stratified_sampling_function(pd.dataframe, target_column, size):\n",
    "    pass\n",
    "\n",
    "def eval():\n",
    "    pass\n",
    "\n",
    "def report():\n",
    "    pass\n",
    "\n",
    "performances = {}\n",
    "for size in [5, 10, 20, 25, 30, 35, 40]:\n",
    "    evaluated_scores = []\n",
    "    # the process should be evaluated on K different splits and take the average performance\n",
    "    for i, (train_indexes, test_indexes) in stratified_sampling_function(df): \n",
    "        \n",
    "        # the memory is unique for each split\n",
    "        N_memory_agent = MemoryAgent(client=client, model=...,\n",
    "                           prompt_template_dict=...,\n",
    "                           schema_dict=...)\n",
    "\n",
    "        df_train = df.iloc[train_indexes,:]\n",
    "        N_memory_agent.train(df_train)\n",
    "        df_test = df.iloc[test_indexes,:]\n",
    "        df_results = N_memory_agent.test(df_test)\n",
    "        evaluated_scores.append(eval(df_results))\n",
    "\n",
    "    performances[size] = report(evaluated_scores)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 999 (temporary) Memory saturation test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_memory(file_path):\n",
    "    with open(file_path, 'r') as file:\n",
    "        content = file.read()\n",
    "    matches = re.findall(r\" memory: (\\[.*?\\])\", content, re.DOTALL)\n",
    "    memory_lst = [ast.literal_eval(match) for match in matches]\n",
    "    return memory_lst\n",
    "\n",
    "file_path = '/home/yl3427/cylab/selfCorrectionAgent/memory.txt'\n",
    "\n",
    "memories = find_memory(file_path)\n",
    "\n",
    "print(len(memories))\n",
    "print(memories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TestAgent:\n",
    "    def __init__(self, client: OpenAI, model: str, \n",
    "                 prompt_template: str, schema, memory_lst: List[str]) -> None:\n",
    "        self.client = client\n",
    "        self.model = model\n",
    "        self.prompt_template = prompt_template\n",
    "        self.schema = schema\n",
    "        self.memory_lst = memory_lst\n",
    "\n",
    "\n",
    "    def get_schema_followed_response(self, messages: list, schema:dict, temperature:float) -> Union[Dict, None]:\n",
    "        try:\n",
    "            response = self.client.chat.completions.create(\n",
    "                model=self.model,\n",
    "                messages=messages,\n",
    "                extra_body={\"guided_json\":schema},\n",
    "                temperature = temperature\n",
    "            )\n",
    "            return json.loads(response.choices[0].message.content.replace(\"\\\\\", \"\\\\\\\\\"))\n",
    "        except json.JSONDecodeError:\n",
    "            return None\n",
    "\n",
    "    \n",
    "    def test(self, testing_dataset: pd.DataFrame, temperature: float = 0.0):\n",
    "        for mem_idx, memory in enumerate(self.memory_lst):\n",
    "            print(f\"memory index: {mem_idx}\")\n",
    "            parsing_error = 0\n",
    "            correct_count = 0\n",
    "            incorrect_count = 0\n",
    "            for report_idx, row in testing_dataset.iterrows():\n",
    "                # print(report_idx)\n",
    "                report = row[\"text\"]\n",
    "                label = row[\"t\"]\n",
    "\n",
    "                prompt = self.prompt_template.format(memory=memory, report=report)\n",
    "                system_prompt = system_instruction+ \"\\n\" + prompt\n",
    "                messages = [{\"role\": \"user\", \"content\": system_prompt}]\n",
    "\n",
    "                json_output = self.get_schema_followed_response(messages, self.schema, temperature)\n",
    "\n",
    "                if not json_output:\n",
    "                    parsing_error += 1\n",
    "                    continue\n",
    "                \n",
    "                if f\"T{label+1}\" == json_output[\"predictedStage\"]:\n",
    "                    correct_count += 1\n",
    "                else:\n",
    "                    incorrect_count +=1\n",
    "\n",
    "            print(f\"\\tcorrect: {correct_count}\")\n",
    "            print(f\"\\twrong: {incorrect_count}\")\n",
    "            print(f\"\\tparsing error: {parsing_error}\")\n",
    "            print()\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "openai_api_key = \"Empty\"\n",
    "openai_api_base = \"http://localhost:8000/v1\"\n",
    "\n",
    "client = OpenAI(\n",
    "    api_key=openai_api_key,\n",
    "    base_url=openai_api_base,\n",
    ")\n",
    "\n",
    "brca_report = pd.read_csv(\"/secure/shared_data/rag_tnm_results/summary/5_folds_summary/brca_df.csv\")\n",
    "df_testing = brca_report.iloc[500:600, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df_testing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TestingResponse(BaseModel):\n",
    "    predictedStage: str = Field(description=\"predicted cancer stage\")\n",
    "    reasoning: str = Field(description=\"reasoning to support predicted cancer stage\") \n",
    "testing_schema = TestingResponse.model_json_schema()\n",
    "\n",
    "memory_agent = TestAgent(client=client, model=\"mistralai/Mistral-7B-Instruct-v0.2\",\n",
    "                           prompt_template = testing_predict_prompt,\n",
    "                           schema = testing_schema,memory_lst = memories)\n",
    "memory_agent.test(df_testing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Seperate Prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiTurMemoryAgent:\n",
    "    def __init__(self, client: OpenAI, model: str, \n",
    "                 prompt_template_dict: dict[str, str], schema_dict: dict, label: str) -> None:\n",
    "        self.client = client\n",
    "        self.model = model\n",
    "        self.prompt_template_dict = prompt_template_dict\n",
    "        self.schema_dict = schema_dict\n",
    "        self.label = label\n",
    "        self.memory = \"\"\n",
    "    \n",
    "    def is_updated(self, new_memory):\n",
    "        old_str = \"\\n\".join(self.memory)\n",
    "        new_str = \"\\n\".join(new_memory)\n",
    "        if fuzz.ratio(old_str, new_str) >= 80 : \n",
    "            return True # update memory\n",
    "        else:\n",
    "            return False\n",
    "        \n",
    "    def get_schema_followed_response(self, messages: list, schema:dict, temperature:float) -> Union[Dict, None]:\n",
    "        response = self.client.chat.completions.create(\n",
    "            model=self.model,\n",
    "            messages=messages,\n",
    "            extra_body=schema,\n",
    "            temperature = temperature\n",
    "        )\n",
    "        return response.choices[0].message.content.replace(\"\\\\\", \"\\\\\\\\\")\n",
    "        \n",
    "    def train(self, training_dataset: pd.DataFrame, conditional_update: bool, temperature: float = 0.0) -> pd.DataFrame:\n",
    "        parsing_error = 0\n",
    "        num_update = 0\n",
    "        update_condition = True\n",
    "        for idx, row in training_dataset.iterrows():\n",
    "            report = row[\"text\"]\n",
    "            if self.memory == \"\":\n",
    "                prediction_prompt = self.prompt_template_dict[\"initialized_prompt\"].format(report=report)\n",
    "            else:\n",
    "                prediction_prompt = self.prompt_template_dict[\"prediction_prompt\"].format(memory=self.memory, report=report)\n",
    "\n",
    "            messages = [{\"role\": \"user\", \"content\": prediction_prompt}]\n",
    "            ans_format = {\"guided_choice\":self.schema_dict[\"predic_choice\"]}\n",
    "            pred = self.get_schema_followed_response(messages, ans_format, temperature)\n",
    "            training_dataset.loc[idx, f\"{self.label}_pred\"] = pred\n",
    " \n",
    "            reason_prompt = self.prompt_template_dict[\"reason_prompt\"]\n",
    "            messages.append({\"role\": \"system\", \"content\": pred}, \n",
    "                            {\"role\": \"user\", \"content\": reason_prompt})\n",
    "            ans_format = {\"guided_json\":self.schema_dict[\"reason_schema\"]}\n",
    "            try:\n",
    "                reason = json.loads(self.get_schema_followed_response(messages, ans_format, temperature))['reasoning']\n",
    "                training_dataset.loc[idx, f\"{self.label}_reason\"] = reason  \n",
    "            except json.JSONDecodeError:\n",
    "                parsing_error += 1\n",
    "                continue\n",
    "    \n",
    "            rule_prompt = self.prompt_template_dict[\"rule_prompt\"]\n",
    "            messages.append({\"role\": \"system\", \"content\": reason}, \n",
    "                            {\"role\": \"user\", \"content\": rule_prompt})\n",
    "            ans_format = {\"guided_json\":self.schema_dict[\"rule_schema\"]}\n",
    "            try:\n",
    "                rule = json.loads(self.get_schema_followed_response(messages, ans_format, temperature))['rules']\n",
    "                training_dataset.loc[idx, f\"{self.label}_rule\"] = rule\n",
    "            except json.JSONDecodeError:\n",
    "                parsing_error += 1\n",
    "                continue\n",
    "                \n",
    "            if conditional_update == True:\n",
    "                update_condition = self.is_updated(rule)\n",
    "\n",
    "            if self.memory == \"\" or update_condition:\n",
    "                self.memory = rule\n",
    "                num_update += 1\n",
    "                \n",
    "        print(f\"Number of parsing errors: {parsing_error}\")\n",
    "        print(f\"Number of memory updates: {num_update}\")\n",
    "        return training_dataset\n",
    "    \n",
    "    def test(self, testing_dataset: pd.DataFrame, temperature: float = 0.0) -> pd.DataFrame:\n",
    "        parsing_error = 0\n",
    "        for idx, row in testing_dataset.iterrows():\n",
    "            report = row[\"text\"]\n",
    "            prediction_prompt = self.prompt_template_dict[\"prediction_prompt\"].format(memory=self.memory, report=report)\n",
    "            messages = [{\"role\": \"user\", \"content\": prediction_prompt}]\n",
    "            ans_format = {\"guided_choice\":self.schema_dict[\"predic_choice\"]}\n",
    "            pred = self.get_schema_followed_response(messages, ans_format, temperature)\n",
    "            testing_dataset.loc[idx, f\"{self.label}_pred\"] = pred\n",
    "            \n",
    "            reason_prompt = self.prompt_template_dict[\"reason_prompt\"]\n",
    "            messages.append({\"role\": \"system\", \"content\": pred}, \n",
    "                            {\"role\": \"user\", \"content\": reason_prompt})\n",
    "            ans_format = {\"guided_json\":self.schema_dict[\"reason_schema\"]}\n",
    "            try:\n",
    "                reason = json.loads(self.get_schema_followed_response(messages, ans_format, temperature))['reasoning']\n",
    "                testing_dataset.loc[idx, f\"{self.label}_reason\"] = reason  \n",
    "            except json.JSONDecodeError:\n",
    "                parsing_error += 1\n",
    "                continue\n",
    "       \n",
    "        print(f\"Number of parsing errors: {parsing_error}\")\n",
    "        return testing_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_choices=[\"N0\", \"N1\", \"N2\", \"N3\"]\n",
    "\n",
    "class ReasonSchema(BaseModel):\n",
    "    reasoning: str = Field(description=\"reasoning to support predicted cancer stage\")     \n",
    "reason_schema = ReasonSchema.model_json_schema()\n",
    "\n",
    "class RuleSchema(BaseModel):\n",
    "    rules: List[str] = Field(description=\"list of rules\")\n",
    "rule_schema = RuleSchema.model_json_schema()\n",
    "\n",
    "\n",
    "agent = MultiTurMemoryAgent(client=client, model=\"mistralai/Mistral-7B-Instruct-v0.2\",\n",
    "                           prompt_template_dict={\"initialized_prompt\":initial_predict_prompt,\n",
    "                                                 \"prediction_prompt\":subsequent_predict_prompt,\n",
    "                                                 \"reason_prompt\":reason_prompt,\n",
    "                                                 \"rule_prompt\":rule_prompt},\n",
    "                           schema_dict={\"predic_choice\":pred_choices,\n",
    "                                        \"reason_schema\":reason_schema,\n",
    "                                        \"rule_schema\":rule_schema},\n",
    "                                        label = \"n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "reflection",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
