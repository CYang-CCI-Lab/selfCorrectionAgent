{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from metrics import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_mean_std(results, cat):\n",
    "    precision_list = [result[cat][\"precision\"] for result in results]\n",
    "    recall_list = [result[cat][\"recall\"] for result in results]\n",
    "    f1_list = [result[cat][\"f1\"] for result in results]\n",
    "    support_list = [result[cat][\"support\"] for result in results]\n",
    "    num_errors_list = [result[cat][\"num_errors\"] for result in results]\n",
    "\n",
    "    mean_precision = sum(precision_list) / len(precision_list)\n",
    "    mean_recall = sum(recall_list) / len(recall_list)\n",
    "    mean_f1 = sum(f1_list) / len(f1_list)\n",
    "\n",
    "    std_precision = (\n",
    "        sum([(x - mean_precision) ** 2 for x in precision_list]) / len(precision_list)\n",
    "    ) ** 0.5\n",
    "    std_recall = (\n",
    "        sum([(x - mean_recall) ** 2 for x in recall_list]) / len(recall_list)\n",
    "    ) ** 0.5\n",
    "    std_f1 = (sum([(x - mean_f1) ** 2 for x in f1_list]) / len(f1_list)) ** 0.5\n",
    "\n",
    "    return {\n",
    "        \"mean_precision\": round(mean_precision, 3),\n",
    "        \"mean_recall\": round(mean_recall, 3),\n",
    "        \"mean_f1\": round(mean_f1, 3),\n",
    "        \"std_precision\": round(std_precision, 3),\n",
    "        \"std_recall\": round(std_recall, 3),\n",
    "        \"std_f1\": round(std_f1, 3),\n",
    "        \"sum_support\": sum(support_list),\n",
    "        \"sum_num_errors\": sum(num_errors_list),\n",
    "        \"raw_mean_precision\": mean_precision,\n",
    "        \"raw_mean_recall\": mean_recall,\n",
    "        \"raw_mean_f1\": mean_f1,\n",
    "    }\n",
    "\n",
    "\n",
    "def output_tabular_performance(results, categories=[\"T1\", \"T2\", \"T3\", \"T4\"]):\n",
    "    precisions = []\n",
    "    recalls = []\n",
    "    f1s = []\n",
    "\n",
    "    for category in categories:\n",
    "        eval = calculate_mean_std(results, category)\n",
    "        print(\n",
    "            \"{} {:.3f}({:.3f}) {:.3f}({:.3f}) {:.3f}({:.3f})\".format(\n",
    "                category,\n",
    "                eval[\"mean_precision\"],\n",
    "                eval[\"std_precision\"],\n",
    "                eval[\"mean_recall\"],\n",
    "                eval[\"std_recall\"],\n",
    "                eval[\"mean_f1\"],\n",
    "                eval[\"std_f1\"],\n",
    "            )\n",
    "        )\n",
    "\n",
    "        # for calculating macro average\n",
    "        precisions.append(eval[\"raw_mean_precision\"])\n",
    "        recalls.append(eval[\"raw_mean_recall\"])\n",
    "        f1s.append(eval[\"raw_mean_f1\"])\n",
    "\n",
    "    print(\n",
    "        \"MacroAvg. {:.3f} {:.3f} {:.3f}\".format(\n",
    "            round(sum(precisions) / len(precisions), 3),\n",
    "            round(sum(recalls) / len(recalls), 3),\n",
    "            round(sum(f1s) / len(f1s), 3),\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# kepa (reported in the draft)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kepa_t_results = []\n",
    "zs_t_results = []\n",
    "zscot_t_results = []\n",
    "\n",
    "\n",
    "kepa_run_lst = [0, 1, 2, 3, 4, 5, 6, 8]\n",
    "\n",
    "for run in kepa_run_lst:\n",
    "    # print(f\"Run {run}, memory 40\")\n",
    "    pred_column = \"cmem_t_40reports_ans_str\"\n",
    "\n",
    "    t_test_df = pd.read_csv(\n",
    "        f\"/home/yl3427/cylab/selfCorrectionAgent/result/0718_t14_dynamic_test_{run}_outof_10runs.csv\"\n",
    "    ).sort_values(by=\"patient_filename\")\n",
    "    t_zs_df = pd.read_csv(\n",
    "        \"/home/yl3427/cylab/selfCorrectionAgent/result/0716_t14_zs_test_800.csv\"\n",
    "    ).sort_values(by=\"patient_filename\")\n",
    "    t_zscot_df = pd.read_csv(\n",
    "        \"/home/yl3427/cylab/selfCorrectionAgent/result/0716_t14_zscot_test_800.csv\"\n",
    "    ).sort_values(by=\"patient_filename\")\n",
    "\n",
    "    split_ids = t_test_df.patient_filename\n",
    "\n",
    "    label_column = t_test_df[\"t\"]\n",
    "    t_test_pred_df = t_test_df[t_test_df.patient_filename.isin(split_ids)][pred_column]\n",
    "    kepa_t_results.append(\n",
    "        t14_calculate_metrics(true_labels=label_column, predictions=t_test_pred_df)\n",
    "    )\n",
    "\n",
    "    t_zs_pred_df = t_zs_df[t_zs_df.patient_filename.isin(split_ids)][\"zs_t_ans_str\"]\n",
    "    zs_t_results.append(\n",
    "        t14_calculate_metrics(true_labels=label_column, predictions=t_zs_pred_df)\n",
    "    )\n",
    "\n",
    "    t_zscot_pred_df = t_zscot_df[t_zscot_df.patient_filename.isin(split_ids)][\n",
    "        \"zs_t_ans_str\"\n",
    "    ]\n",
    "    zscot_t_results.append(\n",
    "        t14_calculate_metrics(true_labels=label_column, predictions=t_zscot_pred_df)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_tabular_performance(kepa_t_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kepa_n_results = []\n",
    "zs_n_results = []\n",
    "zscot_n_results = []\n",
    "\n",
    "kepa_run_lst = [0, 1, 3, 4, 5, 6, 7, 9]\n",
    "\n",
    "for run in kepa_run_lst:\n",
    "    # print(f\"Run {run}, memory 40\")\n",
    "    pred_column = \"cmem_n_40reports_ans_str\"\n",
    "\n",
    "    n_test_df = pd.read_csv(\n",
    "        f\"/home/yl3427/cylab/selfCorrectionAgent/result/0718_n03_dynamic_test_{run}_outof_10runs.csv\"\n",
    "    ).sort_values(by=\"patient_filename\")\n",
    "    n_zs_df = pd.read_csv(\n",
    "        \"/home/yl3427/cylab/selfCorrectionAgent/result/0716_n03_zs_test_800.csv\"\n",
    "    ).sort_values(by=\"patient_filename\")\n",
    "    n_zscot_df = pd.read_csv(\n",
    "        \"/home/yl3427/cylab/selfCorrectionAgent/result/0716_n03_zscot_test_800.csv\"\n",
    "    ).sort_values(by=\"patient_filename\")\n",
    "\n",
    "    split_ids = n_test_df.patient_filename\n",
    "\n",
    "    label_column = n_test_df[\"n\"]\n",
    "    n_test_pred_df = n_test_df[n_test_df.patient_filename.isin(split_ids)][pred_column]\n",
    "    kepa_n_results.append(\n",
    "        n03_calculate_metrics(true_labels=label_column, predictions=n_test_pred_df)\n",
    "    )\n",
    "\n",
    "    n_zs_pred_df = n_zs_df[n_zs_df.patient_filename.isin(split_ids)][\"zs_n_ans_str\"]\n",
    "    zs_n_results.append(\n",
    "        n03_calculate_metrics(true_labels=label_column, predictions=n_zs_pred_df)\n",
    "    )\n",
    "\n",
    "    n_zscot_pred_df = n_zscot_df[n_zscot_df.patient_filename.isin(split_ids)][\n",
    "        \"zs_n_ans_str\"\n",
    "    ]\n",
    "    zscot_n_results.append(\n",
    "        n03_calculate_metrics(true_labels=label_column, predictions=n_zscot_pred_df)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_tabular_performance(kepa_n_results, categories=[\"N0\", \"N1\", \"N2\", \"N3\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_test_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kepa_t_results = []\n",
    "\n",
    "kepa_run_lst = [0, 1, 2, 3, 4, 5, 6, 8]\n",
    "\n",
    "for run in kepa_run_lst:\n",
    "    # print(f\"Run {run}, memory 40\")\n",
    "    pred_column = \"gpt4o_t_stage\"\n",
    "\n",
    "    t_test_df = pd.read_csv(\n",
    "        f\"/home/yl3427/cylab/selfCorrectionAgent/result/1112_t14_gpt_test_{run}_outof_8runs.csv\"\n",
    "    ).sort_values(by=\"patient_filename\")\n",
    "\n",
    "    label_column = t_test_df[\"t\"]\n",
    "    t_test_pred_df = t_test_df[pred_column]\n",
    "    kepa_t_results.append(\n",
    "        t14_calculate_metrics(true_labels=label_column, predictions=t_test_pred_df)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_tabular_performance(kepa_t_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_test_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kepa_n_results = []\n",
    "\n",
    "kepa_run_lst = [0, 1, 3, 4, 5, 6, 7, 9]\n",
    "\n",
    "for run in kepa_run_lst:\n",
    "    # print(f\"Run {run}, memory 40\")\n",
    "    pred_column = \"gpt4o_n_stage\"\n",
    "\n",
    "    n_test_df = pd.read_csv(\n",
    "        f\"/home/yl3427/cylab/selfCorrectionAgent/result/1112_n03_gpt_test_{run}_outof_8runs.csv\"\n",
    "    ).sort_values(by=\"patient_filename\")\n",
    "\n",
    "    label_column = n_test_df[\"n\"]\n",
    "    n_test_pred_df = n_test_df[pred_column]\n",
    "    kepa_n_results.append(\n",
    "        n03_calculate_metrics(true_labels=label_column, predictions=n_test_pred_df)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_tabular_performance(kepa_n_results, categories=[\"N0\", \"N1\", \"N2\", \"N3\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# other models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "kepa_t_results = []\n",
    "\n",
    "kepa_run_lst = [0, 1, 2, 3, 4, 5, 6, 8]\n",
    "\n",
    "for run in kepa_run_lst:\n",
    "    # print(f\"Run {run}, memory 40\")\n",
    "    pred_column = \"kepa_t_ans_str\"\n",
    "\n",
    "    t_test_df = pd.read_csv(\n",
    "        f\"/home/yl3427/cylab/selfCorrectionAgent/result/1114_t14_med42_v2_test_{run}_outof_10runs.csv\"\n",
    "    ).sort_values(by=\"patient_filename\")\n",
    "\n",
    "    label_column = t_test_df[\"t\"]\n",
    "    t_test_pred_df = t_test_df[pred_column]\n",
    "    kepa_t_results.append(\n",
    "        t14_calculate_metrics(true_labels=label_column, predictions=t_test_pred_df)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "T1 0.813(0.073) 0.759(0.076) 0.783(0.064)\n",
      "T2 0.855(0.031) 0.913(0.023) 0.882(0.016)\n",
      "T3 0.869(0.063) 0.703(0.099) 0.770(0.065)\n",
      "T4 0.630(0.046) 0.615(0.057) 0.621(0.042)\n",
      "MacroAvg. 0.792 0.747 0.764\n"
     ]
    }
   ],
   "source": [
    "output_tabular_performance(kepa_t_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "kepa_n_results = []\n",
    "\n",
    "kepa_run_lst = [0, 1, 3, 4, 5, 6, 7, 9]\n",
    "\n",
    "for run in kepa_run_lst:\n",
    "    # print(f\"Run {run}, memory 40\")\n",
    "    pred_column = \"kepa_n_ans_str\"\n",
    "\n",
    "    n_test_df = pd.read_csv(\n",
    "        f\"/home/yl3427/cylab/selfCorrectionAgent/result/1114_n03_med42_v2_test_{run}_outof_10runs.csv\"\n",
    "    ).sort_values(by=\"patient_filename\")\n",
    "\n",
    "    label_column = n_test_df[\"n\"]\n",
    "    n_test_pred_df = n_test_df[pred_column]\n",
    "    kepa_n_results.append(\n",
    "        n03_calculate_metrics(true_labels=label_column, predictions=n_test_pred_df)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N0 0.950(0.011) 0.821(0.059) 0.879(0.032)\n",
      "N1 0.775(0.056) 0.821(0.032) 0.795(0.022)\n",
      "N2 0.657(0.067) 0.711(0.076) 0.675(0.018)\n",
      "N3 0.759(0.103) 0.858(0.029) 0.800(0.062)\n",
      "MacroAvg. 0.785 0.803 0.787\n"
     ]
    }
   ],
   "source": [
    "output_tabular_performance(kepa_n_results, categories=[\"N0\", \"N1\", \"N2\", \"N3\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Re-Run Parsing Error Cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yl3427/miniconda3/envs/vllm_env/lib/python3.9/site-packages/fuzzywuzzy/fuzz.py:11: UserWarning: Using slow pure-python SequenceMatcher. Install python-Levenshtein to remove this warning\n",
      "  warnings.warn('Using slow pure-python SequenceMatcher. Install python-Levenshtein to remove this warning')\n"
     ]
    }
   ],
   "source": [
    "from agent import *\n",
    "from prompt import *\n",
    "from openai import OpenAI\n",
    "import pandas as pd\n",
    "from pydantic import BaseModel, Field\n",
    "from typing import Literal\n",
    "import logging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KEPA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Response_T(BaseModel):\n",
    "    reasoning: str = Field(\n",
    "        description=\"Step-by-step explanation of how you interpreted the report to determine the T stage.\"\n",
    "    )\n",
    "    stage: Literal[\"T1\", \"T2\", \"T3\", \"T4\"] = Field(\n",
    "        description=\"The T stage determined from the report. Stage must be one of 'T1', 'T2', 'T3' or 'T4.'\"\n",
    "    )\n",
    "\n",
    "\n",
    "class Response_N(BaseModel):\n",
    "    reasoning: str = Field(\n",
    "        description=\"Step-by-step explanation of how you interpreted the report to determine the N stage.\"\n",
    "    )\n",
    "    stage: Literal[\"N0\", \"N1\", \"N2\", \"N3\"] = Field(\n",
    "        description=\"The N stage determined from the report. Stage must be one of 'N0', 'N1', 'N2' or 'N3.'\"\n",
    "    )\n",
    "\n",
    "\n",
    "testing_schema_t14 = Response_T.model_json_schema()\n",
    "testing_schema_n03 = Response_N.model_json_schema()\n",
    "\n",
    "client = OpenAI(api_key=\"empty\", base_url=\"http://localhost:8000/v1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_individual_report(\n",
    "    dataset: pd.DataFrame,\n",
    "    patient_filename: str,\n",
    "    memory: str,\n",
    "    label: str,\n",
    "    testing_schema: dict,\n",
    "    model: str = \"m42-health/Llama3-Med42-70B\",\n",
    "):\n",
    "\n",
    "    report = dataset[dataset.patient_filename == patient_filename][\"text\"].values[0]\n",
    "\n",
    "    if label.lower()[0] == \"n\":\n",
    "        prompt = testing_predict_prompt_n03.format(memory=memory, report=report)\n",
    "    else:\n",
    "        prompt = testing_predict_prompt_t14.format(memory=memory, report=report)\n",
    "\n",
    "    filled_prompt = system_instruction + \"\\n\" + prompt\n",
    "    messages = [{\"role\": \"user\", \"content\": filled_prompt}]\n",
    "\n",
    "    response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "        extra_body={\"guided_json\": testing_schema},\n",
    "        temperature=0.5,  # 0.3, 0.5, 0.7, 0.9\n",
    "    )\n",
    "\n",
    "    try:\n",
    "        response = json.loads(response.choices[0].message.content)\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "        return None\n",
    "\n",
    "    dataset.loc[\n",
    "        dataset[\"patient_filename\"] == patient_filename, f\"kepa_{label}_is_parsed\"\n",
    "    ] = True\n",
    "    dataset.loc[\n",
    "        dataset[\"patient_filename\"] == patient_filename, f\"kepa_{label}_ans_str\"\n",
    "    ] = response[\"stage\"]\n",
    "    dataset.loc[\n",
    "        dataset[\"patient_filename\"] == patient_filename, f\"kepa_{label}_reasoning\"\n",
    "    ] = response[\"reasoning\"]\n",
    "\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0th split\n",
      "1th split\n",
      "2th split\n",
      "3th split\n",
      "4th split\n",
      "5th split\n",
      "6th split\n",
      "8th split\n"
     ]
    }
   ],
   "source": [
    "# T14  [0, 1, 2, 3, 4, 5, 6, 8]\n",
    "for run in [0, 1, 2, 3, 4, 5, 6, 8]:\n",
    "    print(f\"{run}th split\")\n",
    "\n",
    "    # Extract memory for t14\n",
    "    t_train_file_path = (\n",
    "        f\"/home/yl3427/cylab/selfCorrectionAgent/result/t14_memory_dataset{run}.csv\"\n",
    "    )\n",
    "    t_train_data = pd.read_csv(t_train_file_path)\n",
    "\n",
    "    t_memory_dict = {}\n",
    "    for idx, row in t_train_data.iterrows():\n",
    "        t_memory_dict[idx + 1] = row[\"cmem_t_memory_str\"]\n",
    "    t_memory = t_memory_dict.get(40, \"\")  # Use .get to avoid KeyError\n",
    "\n",
    "    test_df = pd.read_csv(\n",
    "        f\"/home/yl3427/cylab/selfCorrectionAgent/result/1114_t14_med42_v2_test_{run}_outof_10runs.csv\"\n",
    "    )\n",
    "    unparsed_df = test_df[\n",
    "        ~test_df[\"kepa_t_is_parsed\"].astype(bool) | test_df[\"kepa_t_ans_str\"].isna()\n",
    "    ]\n",
    "\n",
    "    for idx, row in unparsed_df.iterrows():\n",
    "        # if run == 8 and idx <= 149:\n",
    "        #     continue\n",
    "        patient_filename = row[\"patient_filename\"]\n",
    "        print(f\"Processing patient: {patient_filename} (Index: {idx})\")\n",
    "\n",
    "        print(f\"Before: {row['kepa_t_ans_str']}\")\n",
    "\n",
    "        updated_df = test_individual_report(\n",
    "            dataset=test_df,\n",
    "            patient_filename=patient_filename,\n",
    "            memory=t_memory,\n",
    "            label=\"t\",\n",
    "            testing_schema=testing_schema_t14,\n",
    "        )\n",
    "\n",
    "        if updated_df is None:\n",
    "            print(f\"Failed to process patient: {patient_filename}. Skipping...\")\n",
    "            continue\n",
    "        else:\n",
    "            test_df = updated_df  # Only assign if not None\n",
    "\n",
    "        after_stage = test_df.loc[\n",
    "            test_df[\"patient_filename\"] == patient_filename, \"kepa_t_ans_str\"\n",
    "        ].values\n",
    "        after_reasoning = test_df.loc[\n",
    "            test_df[\"patient_filename\"] == patient_filename, \"kepa_t_reasoning\"\n",
    "        ].values\n",
    "        label_value = test_df.loc[\n",
    "            test_df[\"patient_filename\"] == patient_filename, \"t\"\n",
    "        ].values\n",
    "\n",
    "        print(f\"After Stage: {after_stage}\")\n",
    "        print(f\"After Reasoning: {after_reasoning}\")\n",
    "        print(f\"Label: {label_value}\")\n",
    "\n",
    "        rerun_df_path = f\"/home/yl3427/cylab/selfCorrectionAgent/result/1114_t14_med42_v2_test_{run}_outof_10runs.csv\"\n",
    "        test_df.to_csv(rerun_df_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0th split\n",
      "1th split\n",
      "3th split\n",
      "4th split\n",
      "5th split\n",
      "6th split\n",
      "7th split\n",
      "9th split\n"
     ]
    }
   ],
   "source": [
    "# N03 [0, 1, 3, 4, 5, 6, 7, 9]\n",
    "for run in [0, 1, 3, 4, 5, 6, 7, 9]:\n",
    "    print(f\"{run}th split\")\n",
    "\n",
    "    # extract memory for t14\n",
    "    n_train_file_path = (\n",
    "        f\"/home/yl3427/cylab/selfCorrectionAgent/result/n03_memory_dataset{run}.csv\"\n",
    "    )\n",
    "    n_train_data = pd.read_csv(n_train_file_path)\n",
    "\n",
    "    n_memory_dict = {}\n",
    "    for idx, row in n_train_data.iterrows():\n",
    "        n_memory_dict[idx + 1] = row[\"cmem_n_memory_str\"]\n",
    "    n_memory = n_memory_dict[40]\n",
    "\n",
    "    test_df = pd.read_csv(\n",
    "        f\"/home/yl3427/cylab/selfCorrectionAgent/result/1114_n03_med42_v2_test_{run}_outof_10runs.csv\"\n",
    "    )\n",
    "    unparsed_df = test_df[\n",
    "        ~test_df[\"kepa_n_is_parsed\"].astype(bool) | test_df[\"kepa_n_ans_str\"].isna()\n",
    "    ]\n",
    "\n",
    "    for idx, row in unparsed_df.iterrows():\n",
    "        # if run == 8 and idx <= 149:\n",
    "        #     continue\n",
    "\n",
    "        patient_filename = row[\"patient_filename\"]\n",
    "        print(f\"Processing patient: {patient_filename} (Index: {idx})\")\n",
    "\n",
    "        print(f\"Before: {row['kepa_n_ans_str']}\")\n",
    "\n",
    "        updated_df = test_individual_report(\n",
    "            dataset=test_df,\n",
    "            patient_filename=patient_filename,\n",
    "            memory=n_memory,\n",
    "            label=\"n\",\n",
    "            testing_schema=testing_schema_n03,\n",
    "        )\n",
    "\n",
    "        if updated_df is None:\n",
    "            print(f\"Failed to process patient: {patient_filename}. Skipping...\")\n",
    "            continue\n",
    "        else:\n",
    "            test_df = updated_df\n",
    "\n",
    "        after_stage = test_df.loc[\n",
    "            test_df[\"patient_filename\"] == patient_filename, \"kepa_n_ans_str\"\n",
    "        ].values\n",
    "        after_reasoning = test_df.loc[\n",
    "            test_df[\"patient_filename\"] == patient_filename, \"kepa_n_reasoning\"\n",
    "        ].values\n",
    "        label_value = test_df.loc[\n",
    "            test_df[\"patient_filename\"] == patient_filename, \"n\"\n",
    "        ].values\n",
    "\n",
    "        print(f\"After Stage: {after_stage}\")\n",
    "        print(f\"After Reasoning: {after_reasoning}\")\n",
    "        print(f\"Label: {label_value}\")\n",
    "\n",
    "        rerun_df_path = f\"/home/yl3427/cylab/selfCorrectionAgent/result/1114_n03_med42_v2_test_{run}_outof_10runs.csv\"\n",
    "        test_df.to_csv(rerun_df_path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ZSCOT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Response_T(BaseModel):\n",
    "    reasoning: str = Field(\n",
    "        description=\"Step-by-step explanation of how you interpreted the report to determine the T stage.\"\n",
    "    )\n",
    "    stage: Literal[\"T1\", \"T2\", \"T3\", \"T4\"] = Field(\n",
    "        description=\"The T stage determined from the report. Stage must be one of 'T1', 'T2', 'T3' or 'T4.'\"\n",
    "    )\n",
    "\n",
    "\n",
    "class Response_N(BaseModel):\n",
    "    reasoning: str = Field(\n",
    "        description=\"Step-by-step explanation of how you interpreted the report to determine the N stage.\"\n",
    "    )\n",
    "    stage: Literal[\"N0\", \"N1\", \"N2\", \"N3\"] = Field(\n",
    "        description=\"The N stage determined from the report. Stage must be one of 'N0', 'N1', 'N2' or 'N3.'\"\n",
    "    )\n",
    "\n",
    "\n",
    "testing_schema_t14 = Response_T.model_json_schema()\n",
    "testing_schema_n03 = Response_N.model_json_schema()\n",
    "\n",
    "client = OpenAI(api_key=\"empty\", base_url=\"http://localhost:8000/v1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_individual_report(\n",
    "    dataset: pd.DataFrame,\n",
    "    patient_filename: str,\n",
    "    label: str,\n",
    "    testing_schema: dict,\n",
    "    model: str = \"m42-health/Llama3-Med42-70B\",\n",
    "):\n",
    "\n",
    "    report = dataset[dataset.patient_filename == patient_filename][\"text\"].values[0]\n",
    "\n",
    "    if label.lower()[0] == \"n\":\n",
    "        prompt = zscot_predict_prompt_n03.format(report=report)\n",
    "    else:\n",
    "        prompt = zscot_predict_prompt_t14.format(report=report)\n",
    "\n",
    "    filled_prompt = system_instruction + \"\\n\" + prompt\n",
    "    messages = [{\"role\": \"user\", \"content\": filled_prompt}]\n",
    "\n",
    "    response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "        extra_body={\"guided_json\": testing_schema},\n",
    "        temperature=0.1,  # 0.3, 0.5, 0.7, 0.9\n",
    "    )\n",
    "\n",
    "    try:\n",
    "        response = json.loads(response.choices[0].message.content)\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "        return None\n",
    "\n",
    "    dataset.loc[\n",
    "        dataset[\"patient_filename\"] == patient_filename, f\"zscot_{label}_is_parsed\"\n",
    "    ] = True\n",
    "    dataset.loc[\n",
    "        dataset[\"patient_filename\"] == patient_filename, f\"zscot_{label}_ans_str\"\n",
    "    ] = response[\"stage\"]\n",
    "    dataset.loc[\n",
    "        dataset[\"patient_filename\"] == patient_filename, f\"zscot_{label}_reasoning\"\n",
    "    ] = response[\"reasoning\"]\n",
    "\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### T14\n",
    "test_df = pd.read_csv(\n",
    "    f\"/home/yl3427/cylab/selfCorrectionAgent/result/1118_t14_med42_v2_test_800.csv\"\n",
    ")\n",
    "unparsed_df = test_df[\n",
    "    ~test_df[\"zscot_t_is_parsed\"].astype(bool) | test_df[\"zscot_t_ans_str\"].isna()\n",
    "]\n",
    "\n",
    "for idx, row in unparsed_df.iterrows():\n",
    "\n",
    "    patient_filename = row[\"patient_filename\"]\n",
    "    print(f\"Processing patient: {patient_filename} (Index: {idx})\")\n",
    "\n",
    "    print(f\"Before: {row['zscot_t_ans_str']}\")\n",
    "\n",
    "    updated_df = test_individual_report(\n",
    "        dataset=test_df,\n",
    "        patient_filename=patient_filename,\n",
    "        label=\"t\",\n",
    "        testing_schema=testing_schema_t14,\n",
    "    )\n",
    "\n",
    "    if updated_df is None:\n",
    "        print(f\"Failed to process patient: {patient_filename}. Skipping...\")\n",
    "        continue\n",
    "    else:\n",
    "        test_df = updated_df  # Only assign if not None\n",
    "\n",
    "    after_stage = test_df.loc[\n",
    "        test_df[\"patient_filename\"] == patient_filename, \"zscot_t_ans_str\"\n",
    "    ].values\n",
    "    after_reasoning = test_df.loc[\n",
    "        test_df[\"patient_filename\"] == patient_filename, \"zscot_t_reasoning\"\n",
    "    ].values\n",
    "    label_value = test_df.loc[\n",
    "        test_df[\"patient_filename\"] == patient_filename, \"t\"\n",
    "    ].values\n",
    "\n",
    "    print(f\"After Stage: {after_stage}\")\n",
    "    print(f\"After Reasoning: {after_reasoning}\")\n",
    "    print(f\"Label: {label_value}\")\n",
    "\n",
    "    rerun_df_path = (\n",
    "        f\"/home/yl3427/cylab/selfCorrectionAgent/result/1118_t14_med42_v2_test_800.csv\"\n",
    "    )\n",
    "    test_df.to_csv(rerun_df_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### N03\n",
    "test_df = pd.read_csv(\n",
    "    f\"/home/yl3427/cylab/selfCorrectionAgent/result/1118_n03_med42_v2_test_800.csv\"\n",
    ")\n",
    "unparsed_df = test_df[\n",
    "    ~test_df[\"zscot_n_is_parsed\"].astype(bool) | test_df[\"zscot_n_ans_str\"].isna()\n",
    "]\n",
    "\n",
    "for idx, row in unparsed_df.iterrows():\n",
    "\n",
    "    patient_filename = row[\"patient_filename\"]\n",
    "    print(f\"Processing patient: {patient_filename} (Index: {idx})\")\n",
    "\n",
    "    print(f\"Before: {row['zscot_n_ans_str']}\")\n",
    "\n",
    "    updated_df = test_individual_report(\n",
    "        dataset=test_df,\n",
    "        patient_filename=patient_filename,\n",
    "        label=\"n\",\n",
    "        testing_schema=testing_schema_n03,\n",
    "    )\n",
    "\n",
    "    if updated_df is None:\n",
    "        print(f\"Failed to process patient: {patient_filename}. Skipping...\")\n",
    "        continue\n",
    "    else:\n",
    "        test_df = updated_df  # Only assign if not None\n",
    "\n",
    "    after_stage = test_df.loc[\n",
    "        test_df[\"patient_filename\"] == patient_filename, \"zscot_n_ans_str\"\n",
    "    ].values\n",
    "    after_reasoning = test_df.loc[\n",
    "        test_df[\"patient_filename\"] == patient_filename, \"zscot_n_reasoning\"\n",
    "    ].values\n",
    "    label_value = test_df.loc[\n",
    "        test_df[\"patient_filename\"] == patient_filename, \"n\"\n",
    "    ].values\n",
    "\n",
    "    print(f\"After Stage: {after_stage}\")\n",
    "    print(f\"After Reasoning: {after_reasoning}\")\n",
    "    print(f\"Label: {label_value}\")\n",
    "\n",
    "    rerun_df_path = (\n",
    "        f\"/home/yl3427/cylab/selfCorrectionAgent/result/1118_n03_med42_v2_test_800.csv\"\n",
    "    )\n",
    "    test_df.to_csv(rerun_df_path, index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vllm_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
