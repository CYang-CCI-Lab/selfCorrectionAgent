{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "39515827",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "###############################################################################\n",
    "# T‑stage metrics                                                             #\n",
    "###############################################################################\n",
    "def t14_calculate_metrics(true_labels: pd.Series, predictions: pd.Series) -> dict:\n",
    "    \"\"\"\n",
    "    Calculate per‑class and overall precision / recall / F1 for T‑stage\n",
    "    predictions.  Adds *num_errors* at the “overall” level that counts each\n",
    "    wrong row exactly once.\n",
    "    \"\"\"\n",
    "    # ------------------------------------------------------------------ #\n",
    "    # Sanity checks                                                      #\n",
    "    # ------------------------------------------------------------------ #\n",
    "    if len(true_labels) != len(predictions):\n",
    "        raise ValueError(\"true_labels and predictions must be the same length\")\n",
    "    if not isinstance(true_labels, pd.Series) or not isinstance(predictions, pd.Series):\n",
    "        raise TypeError(\"true_labels and predictions must be pandas Series\")\n",
    "    if any(pd.isna(pred) or not isinstance(pred, str) for pred in predictions):\n",
    "        raise ValueError(\"All predictions must be non‑null strings\")\n",
    "\n",
    "    true_labels = true_labels.apply(lambda x: f\"T{int(x) + 1}\")          # T1‑T4 …\n",
    "    unique_true_labels = sorted(set(true_labels))\n",
    "\n",
    "    metrics = {lab: {\"tp\": 0, \"fp\": 0, \"fn\": 0} for lab in unique_true_labels}\n",
    "    label_counts = {lab: 0 for lab in unique_true_labels}\n",
    "\n",
    "    # ------------------------------------------------------------------ #\n",
    "    # Count TP / FP / FN                                                 #\n",
    "    # ------------------------------------------------------------------ #\n",
    "    for true_lab, pred in zip(true_labels, predictions):\n",
    "        pred_u = str(pred).upper()\n",
    "\n",
    "        label_counts[true_lab] += 1\n",
    "        if true_lab in pred_u:                                           # TP test\n",
    "            metrics[true_lab][\"tp\"] += 1\n",
    "        else:\n",
    "            metrics[true_lab][\"fn\"] += 1\n",
    "\n",
    "        for lab in unique_true_labels:                                  # FP test\n",
    "            if lab in pred_u and lab != true_lab:\n",
    "                metrics[lab][\"fp\"] += 1\n",
    "\n",
    "    # ------------------------------------------------------------------ #\n",
    "    # Derive precision/recall/F1 per class                               #\n",
    "    # ------------------------------------------------------------------ #\n",
    "    results = {}\n",
    "    total_tp = total_fp = total_fn = 0\n",
    "    macro_prec = macro_rec = macro_f1 = 0.0\n",
    "    N = len(true_labels)\n",
    "\n",
    "    for lab in unique_true_labels:\n",
    "        tp, fp, fn = (metrics[lab][k] for k in (\"tp\", \"fp\", \"fn\"))\n",
    "\n",
    "        prec = tp / (tp + fp) if tp + fp else 0.0\n",
    "        rec  = tp / (tp + fn) if tp + fn else 0.0\n",
    "        f1   = 2 * prec * rec / (prec + rec) if prec + rec else 0.0\n",
    "\n",
    "        results[lab] = {\n",
    "            \"precision\": round(prec, 3),\n",
    "            \"recall\":    round(rec, 3),\n",
    "            \"f1\":        round(f1, 3),\n",
    "            \"support\":   label_counts[lab],\n",
    "            \"tp\": tp,\n",
    "            \"fp\": fp,\n",
    "            \"fn\": fn,\n",
    "            \"num_errors\": fp + fn,            # class‑level (kept as before)\n",
    "        }\n",
    "\n",
    "        total_tp += tp\n",
    "        total_fp += fp\n",
    "        total_fn += fn\n",
    "        macro_prec += prec\n",
    "        macro_rec  += rec\n",
    "        macro_f1   += f1\n",
    "\n",
    "    # ------------------------------------------------------------------ #\n",
    "    # Micro / macro + row‑level error count                              #\n",
    "    # ------------------------------------------------------------------ #\n",
    "    micro_prec = total_tp / (total_tp + total_fp) if total_tp + total_fp else 0.0\n",
    "    micro_rec  = total_tp / (total_tp + total_fn) if total_tp + total_fn else 0.0\n",
    "    micro_f1   = 2 * micro_prec * micro_rec / (micro_prec + micro_rec) if micro_prec + micro_rec else 0.0\n",
    "\n",
    "    macro_prec /= len(unique_true_labels)\n",
    "    macro_rec  /= len(unique_true_labels)\n",
    "    macro_f1   /= len(unique_true_labels)\n",
    "\n",
    "    weighted_f1 = sum(results[lab][\"f1\"] * label_counts[lab] for lab in unique_true_labels) / N\n",
    "\n",
    "    results[\"overall\"] = {\n",
    "        \"micro_precision\": round(micro_prec, 3),\n",
    "        \"micro_recall\":    round(micro_rec, 3),\n",
    "        \"micro_f1\":        round(micro_f1, 3),\n",
    "        \"macro_precision\": round(macro_prec, 3),\n",
    "        \"macro_recall\":    round(macro_rec, 3),\n",
    "        \"macro_f1\":        round(macro_f1, 3),\n",
    "        \"weighted_f1\":     round(weighted_f1, 3),\n",
    "        \"support\":         N,\n",
    "        \"total_tp\":        total_tp,\n",
    "        \"total_fp\":        total_fp,\n",
    "        \"total_fn\":        total_fn,\n",
    "        # wrong rows counted once:\n",
    "        \"num_errors\":      N - total_tp,\n",
    "    }\n",
    "    return results\n",
    "\n",
    "\n",
    "###############################################################################\n",
    "# N‑stage metrics (same changes as above)                                     #\n",
    "###############################################################################\n",
    "def n03_calculate_metrics(true_labels: pd.Series, predictions: pd.Series) -> dict:\n",
    "    \"\"\"Same as t14_calculate_metrics but for N‑stage labels.\"\"\"\n",
    "    if len(true_labels) != len(predictions):\n",
    "        raise ValueError(\"true_labels and predictions must be the same length\")\n",
    "    if not isinstance(true_labels, pd.Series) or not isinstance(predictions, pd.Series):\n",
    "        raise TypeError(\"true_labels and predictions must be pandas Series\")\n",
    "    if any(pd.isna(pred) or not isinstance(pred, str) for pred in predictions):\n",
    "        raise ValueError(\"All predictions must be non‑null strings\")\n",
    "\n",
    "    true_labels = true_labels.apply(lambda x: f\"N{int(x)}\")\n",
    "    unique_true_labels = sorted(set(true_labels))\n",
    "\n",
    "    metrics = {lab: {\"tp\": 0, \"fp\": 0, \"fn\": 0} for lab in unique_true_labels}\n",
    "    label_counts = {lab: 0 for lab in unique_true_labels}\n",
    "\n",
    "    for true_lab, pred in zip(true_labels, predictions):\n",
    "        pred_u = str(pred).upper().replace(\"NO\", \"N0\").replace(\"NL\", \"N1\")\n",
    "\n",
    "        label_counts[true_lab] += 1\n",
    "        if true_lab in pred_u:\n",
    "            metrics[true_lab][\"tp\"] += 1\n",
    "        else:\n",
    "            metrics[true_lab][\"fn\"] += 1\n",
    "\n",
    "        for lab in unique_true_labels:\n",
    "            if lab in pred_u and lab != true_lab:\n",
    "                metrics[lab][\"fp\"] += 1\n",
    "\n",
    "    results = {}\n",
    "    total_tp = total_fp = total_fn = 0\n",
    "    macro_prec = macro_rec = macro_f1 = 0.0\n",
    "    N = len(true_labels)\n",
    "\n",
    "    for lab in unique_true_labels:\n",
    "        tp, fp, fn = (metrics[lab][k] for k in (\"tp\", \"fp\", \"fn\"))\n",
    "        prec = tp / (tp + fp) if tp + fp else 0.0\n",
    "        rec  = tp / (tp + fn) if tp + fn else 0.0\n",
    "        f1   = 2 * prec * rec / (prec + rec) if prec + rec else 0.0\n",
    "\n",
    "        results[lab] = {\n",
    "            \"precision\": round(prec, 3),\n",
    "            \"recall\":    round(rec, 3),\n",
    "            \"f1\":        round(f1, 3),\n",
    "            \"support\":   label_counts[lab],\n",
    "            \"tp\": tp,\n",
    "            \"fp\": fp,\n",
    "            \"fn\": fn,\n",
    "            \"num_errors\": fp + fn,\n",
    "        }\n",
    "\n",
    "        total_tp += tp\n",
    "        total_fp += fp\n",
    "        total_fn += fn\n",
    "        macro_prec += prec\n",
    "        macro_rec  += rec\n",
    "        macro_f1   += f1\n",
    "\n",
    "    micro_prec = total_tp / (total_tp + total_fp) if total_tp + total_fp else 0.0\n",
    "    micro_rec  = total_tp / (total_tp + total_fn) if total_tp + total_fn else 0.0\n",
    "    micro_f1   = 2 * micro_prec * micro_rec / (micro_prec + micro_rec) if micro_prec + micro_rec else 0.0\n",
    "\n",
    "    macro_prec /= len(unique_true_labels)\n",
    "    macro_rec  /= len(unique_true_labels)\n",
    "    macro_f1   /= len(unique_true_labels)\n",
    "\n",
    "    weighted_f1 = sum(results[lab][\"f1\"] * label_counts[lab] for lab in unique_true_labels) / N\n",
    "\n",
    "    results[\"overall\"] = {\n",
    "        \"micro_precision\": round(micro_prec, 3),\n",
    "        \"micro_recall\":    round(micro_rec, 3),\n",
    "        \"micro_f1\":        round(micro_f1, 3),\n",
    "        \"macro_precision\": round(macro_prec, 3),\n",
    "        \"macro_recall\":    round(macro_rec, 3),\n",
    "        \"macro_f1\":        round(macro_f1, 3),\n",
    "        \"weighted_f1\":     round(weighted_f1, 3),\n",
    "        \"support\":         N,\n",
    "        \"total_tp\":        total_tp,\n",
    "        \"total_fp\":        total_fp,\n",
    "        \"total_fn\":        total_fn,\n",
    "        \"num_errors\":      N - total_tp,\n",
    "    }\n",
    "    return results\n",
    "\n",
    "\n",
    "###############################################################################\n",
    "# Helper: print error counts per split                                        #\n",
    "###############################################################################\n",
    "def print_error_counts(results):\n",
    "    fp_list, fn_list, row_err_list = [], [], []\n",
    "\n",
    "    print(\"Run  WrongRows  FN  FP  (FP+FN)\")\n",
    "    for i, res in enumerate(results):\n",
    "        fp  = res[\"overall\"][\"total_fp\"]\n",
    "        fn  = res[\"overall\"][\"total_fn\"]\n",
    "        err = res[\"overall\"][\"num_errors\"]          # each row once\n",
    "\n",
    "        fp_list.append(fp)\n",
    "        fn_list.append(fn)\n",
    "        row_err_list.append(err)\n",
    "\n",
    "        print(f\"{i:<4} {err:<10} {fn:<3} {fp:<3} {fp+fn}\")\n",
    "\n",
    "    print(\"\\nMean ± Std across splits\")\n",
    "    print(f\"Wrong rows: {np.mean(row_err_list):.2f} ± {np.std(row_err_list):.2f}\")\n",
    "    print(f\"FN:         {np.mean(fn_list):.2f} ± {np.std(fn_list):.2f}\")\n",
    "    print(f\"FP:         {np.mean(fp_list):.2f} ± {np.std(fp_list):.2f}\")\n",
    "\n",
    "\n",
    "###############################################################################\n",
    "# calculate_mean_std and output_tabular_performance remain unchanged          #\n",
    "# (they already pick up the updated overall['num_errors'])                    #\n",
    "###############################################################################\n",
    "# … (rest of your notebook cells stay exactly as before)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13d8bd98",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b28967b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "\n",
    "# def t14_calculate_metrics(true_labels: pd.Series, predictions: pd.Series) -> dict:\n",
    "#     \"\"\"\n",
    "#     Calculates precision, recall, F1-score, and support for T-stage predictions.\n",
    "#     Includes both per-label, micro-average, and macro-average scores.\n",
    "\n",
    "#     Args:\n",
    "#         true_labels: A pandas Series of true labels (e.g., integers from 0 to N-1).\n",
    "#         predictions: A pandas Series of predicted labels (strings).\n",
    "\n",
    "#     Returns:\n",
    "#         A dictionary containing the metrics for each label and overall scores.\n",
    "#     \"\"\"\n",
    "#     # Check for valid inputs\n",
    "#     if len(true_labels) != len(predictions):\n",
    "#         raise ValueError(\"The length of true_labels and predictions must be the same.\")\n",
    "\n",
    "#     if not isinstance(true_labels, pd.Series) or not isinstance(predictions, pd.Series):\n",
    "#         raise TypeError(\"true_labels and predictions must be pandas Series.\")\n",
    "\n",
    "#     if any(pd.isna(pred) or not isinstance(pred, str) for pred in predictions):\n",
    "#         raise ValueError(\"All predictions must be non-null strings.\")\n",
    "\n",
    "#     # Standardize true labels to \"T{x+1}\" format\n",
    "#     true_labels = true_labels.apply(lambda x: f\"T{int(x)+1}\")\n",
    "\n",
    "#     metrics = {}\n",
    "#     label_counts = {}\n",
    "#     unique_true_labels = sorted(list(set(true_labels))) # Ensure consistent order\n",
    "\n",
    "#     for label in unique_true_labels:\n",
    "#         metrics[label] = {\"tp\": 0, \"fp\": 0, \"fn\": 0}\n",
    "#         label_counts[label] = 0\n",
    "\n",
    "#     for true_label, prediction in zip(true_labels, predictions):\n",
    "#         # Ensure prediction is a string and convert to uppercase\n",
    "#         prediction_str = str(prediction).upper()\n",
    "        \n",
    "#         label_counts[true_label] += 1\n",
    "#         if true_label in prediction_str:\n",
    "#             metrics[true_label][\"tp\"] += 1\n",
    "#         else:\n",
    "#             metrics[true_label][\"fn\"] += 1\n",
    "\n",
    "#         # Calculate false positives\n",
    "#         # A prediction is a false positive for a label if:\n",
    "#         # 1. The label is present in the prediction string.\n",
    "#         # 2. The label is NOT the true_label.\n",
    "#         for label_to_check_fp in unique_true_labels:\n",
    "#             if label_to_check_fp in prediction_str and label_to_check_fp != true_label:\n",
    "#                 metrics[label_to_check_fp][\"fp\"] += 1\n",
    "    \n",
    "#     results = {}\n",
    "#     # Variables for micro-averaging\n",
    "#     total_tp_micro = 0\n",
    "#     total_fp_micro = 0\n",
    "#     total_fn_micro = 0\n",
    "    \n",
    "#     # Variables for macro-averaging\n",
    "#     macro_precision_sum = 0.0\n",
    "#     macro_recall_sum = 0.0\n",
    "#     macro_f1_sum = 0.0\n",
    "    \n",
    "#     total_instances = len(true_labels)\n",
    "\n",
    "#     for label in unique_true_labels: # Iterate in a defined order\n",
    "#         counts = metrics[label]\n",
    "#         tp = counts[\"tp\"]\n",
    "#         fp = counts[\"fp\"]\n",
    "#         fn = counts[\"fn\"]\n",
    "\n",
    "#         # Precision: TP / (TP + FP)\n",
    "#         precision = tp / (tp + fp) if (tp + fp) > 0 else 0.0\n",
    "#         # Recall: TP / (TP + FN)\n",
    "#         recall = tp / (tp + fn) if (tp + fn) > 0 else 0.0\n",
    "#         # F1-Score: 2 * (Precision * Recall) / (Precision + Recall)\n",
    "#         f1 = (\n",
    "#             2 * precision * recall / (precision + recall)\n",
    "#             if (precision + recall) > 0\n",
    "#             else 0.0\n",
    "#         )\n",
    "#         support = label_counts[label]\n",
    "\n",
    "#         results[label] = {\n",
    "#             \"precision\": round(precision, 3),\n",
    "#             \"recall\": round(recall, 3),\n",
    "#             \"f1\": round(f1, 3),\n",
    "#             \"support\": support,\n",
    "#             \"tp\": tp,\n",
    "#             \"fp\": fp,\n",
    "#             \"fn\": fn,\n",
    "#             \"num_errors\": fp + fn, # Sum of false positives and false negatives for this class\n",
    "#         }\n",
    "\n",
    "#         # Accumulate for micro-averages\n",
    "#         total_tp_micro += tp\n",
    "#         total_fp_micro += fp\n",
    "#         total_fn_micro += fn\n",
    "        \n",
    "#         # Accumulate for macro-averages\n",
    "#         macro_precision_sum += precision\n",
    "#         macro_recall_sum += recall\n",
    "#         macro_f1_sum += f1\n",
    "\n",
    "#     # Calculate macro-averaged metrics\n",
    "#     num_labels = len(unique_true_labels)\n",
    "#     macro_precision = macro_precision_sum / num_labels if num_labels > 0 else 0.0\n",
    "#     macro_recall = macro_recall_sum / num_labels if num_labels > 0 else 0.0\n",
    "#     macro_f1 = macro_f1_sum / num_labels if num_labels > 0 else 0.0 # Often calculated as harmonic mean of macro_precision and macro_recall\n",
    "\n",
    "#     # Calculate micro-averaged (overall) precision, recall, and F1 score\n",
    "#     # Micro-Precision: Sum of all TPs / (Sum of all TPs + Sum of all FPs)\n",
    "#     micro_precision = (\n",
    "#         total_tp_micro / (total_tp_micro + total_fp_micro) if (total_tp_micro + total_fp_micro) > 0 else 0.0\n",
    "#     )\n",
    "#     # Micro-Recall: Sum of all TPs / (Sum of all TPs + Sum of all FNs)\n",
    "#     micro_recall = (\n",
    "#         total_tp_micro / (total_tp_micro + total_fn_micro) if (total_tp_micro + total_fn_micro) > 0 else 0.0\n",
    "#     )\n",
    "#     # Micro-F1: 2 * (Micro-Precision * Micro-Recall) / (Micro-Precision + Micro-Recall)\n",
    "#     micro_f1 = (\n",
    "#         2 * micro_precision * micro_recall / (micro_precision + micro_recall)\n",
    "#         if (micro_precision + micro_recall) > 0\n",
    "#         else 0.0\n",
    "#     )\n",
    "\n",
    "#     # Calculate weighted F1 score\n",
    "#     weighted_f1_sum = 0.0\n",
    "#     for label in unique_true_labels:\n",
    "#         weighted_f1_sum += results[label][\"f1\"] * label_counts[label]\n",
    "#     weighted_f1 = weighted_f1_sum / total_instances if total_instances > 0 else 0.0\n",
    "\n",
    "#     results[\"overall\"] = {\n",
    "#         \"micro_precision\": round(micro_precision, 3),\n",
    "#         \"micro_recall\": round(micro_recall, 3),\n",
    "#         \"micro_f1\": round(micro_f1, 3),\n",
    "#         \"macro_precision\": round(macro_precision, 3),\n",
    "#         \"macro_recall\": round(macro_recall, 3),\n",
    "#         \"macro_f1\": round(macro_f1, 3),\n",
    "#         \"weighted_f1\": round(weighted_f1, 3),\n",
    "#         \"support\": total_instances,\n",
    "#         \"total_tp\": total_tp_micro,\n",
    "#         \"total_fp\": total_fp_micro,\n",
    "#         \"total_fn\": total_fn_micro,\n",
    "#         \"num_errors\": total_fp_micro + total_fn_micro, # Sum of all false positives and false negatives\n",
    "#     }\n",
    "#     return results\n",
    "\n",
    "\n",
    "# def n03_calculate_metrics(true_labels: pd.Series, predictions: pd.Series) -> dict:\n",
    "#     \"\"\"\n",
    "#     Calculates precision, recall, F1-score, and support for N-stage predictions.\n",
    "#     Includes both per-label, micro-average, and macro-average scores.\n",
    "#     Handles specific label replacements: \"NO\" to \"N0\", \"NL\" to \"N1\".\n",
    "\n",
    "#     Args:\n",
    "#         true_labels: A pandas Series of true labels (e.g., integers from 0 to N-1).\n",
    "#         predictions: A pandas Series of predicted labels (strings).\n",
    "\n",
    "#     Returns:\n",
    "#         A dictionary containing the metrics for each label and overall scores.\n",
    "#     \"\"\"\n",
    "#     # Check for valid inputs\n",
    "#     if len(true_labels) != len(predictions):\n",
    "#         raise ValueError(\"The length of true_labels and predictions must be the same.\")\n",
    "\n",
    "#     if not isinstance(true_labels, pd.Series) or not isinstance(predictions, pd.Series):\n",
    "#         raise TypeError(\"true_labels and predictions must be pandas Series.\")\n",
    "\n",
    "#     if any(pd.isna(pred) or not isinstance(pred, str) for pred in predictions):\n",
    "#         raise ValueError(\"All predictions must be non-null strings.\")\n",
    "\n",
    "#     # Standardize true labels to \"N{x}\" format\n",
    "#     true_labels = true_labels.apply(lambda x: f\"N{int(x)}\")\n",
    "\n",
    "#     metrics = {}\n",
    "#     label_counts = {}\n",
    "#     unique_true_labels = sorted(list(set(true_labels))) # Ensure consistent order\n",
    "\n",
    "#     for label in unique_true_labels:\n",
    "#         metrics[label] = {\"tp\": 0, \"fp\": 0, \"fn\": 0}\n",
    "#         label_counts[label] = 0\n",
    "\n",
    "#     for true_label, prediction in zip(true_labels, predictions):\n",
    "#         # Ensure prediction is a string, convert to uppercase, and apply replacements\n",
    "#         prediction_str = str(prediction).upper()\n",
    "#         prediction_str = prediction_str.replace(\"NO\", \"N0\").replace(\"NL\", \"N1\")\n",
    "        \n",
    "#         label_counts[true_label] += 1\n",
    "#         if true_label in prediction_str:\n",
    "#             metrics[true_label][\"tp\"] += 1\n",
    "#         else:\n",
    "#             metrics[true_label][\"fn\"] += 1\n",
    "\n",
    "#         # Calculate false positives\n",
    "#         for label_to_check_fp in unique_true_labels:\n",
    "#             if label_to_check_fp in prediction_str and label_to_check_fp != true_label:\n",
    "#                 metrics[label_to_check_fp][\"fp\"] += 1\n",
    "\n",
    "#     results = {}\n",
    "#     # Variables for micro-averaging\n",
    "#     total_tp_micro = 0\n",
    "#     total_fp_micro = 0\n",
    "#     total_fn_micro = 0\n",
    "    \n",
    "#     # Variables for macro-averaging\n",
    "#     macro_precision_sum = 0.0\n",
    "#     macro_recall_sum = 0.0\n",
    "#     macro_f1_sum = 0.0\n",
    "\n",
    "#     total_instances = len(true_labels)\n",
    "\n",
    "#     for label in unique_true_labels: # Iterate in a defined order\n",
    "#         counts = metrics[label]\n",
    "#         tp = counts[\"tp\"]\n",
    "#         fp = counts[\"fp\"]\n",
    "#         fn = counts[\"fn\"]\n",
    "\n",
    "#         precision = tp / (tp + fp) if (tp + fp) > 0 else 0.0\n",
    "#         recall = tp / (tp + fn) if (tp + fn) > 0 else 0.0\n",
    "#         f1 = (\n",
    "#             2 * precision * recall / (precision + recall)\n",
    "#             if (precision + recall) > 0\n",
    "#             else 0.0\n",
    "#         )\n",
    "#         support = label_counts[label]\n",
    "\n",
    "#         results[label] = {\n",
    "#             \"precision\": round(precision, 3),\n",
    "#             \"recall\": round(recall, 3),\n",
    "#             \"f1\": round(f1, 3),\n",
    "#             \"support\": support,\n",
    "#             \"tp\": tp,\n",
    "#             \"fp\": fp,\n",
    "#             \"fn\": fn,\n",
    "#             \"num_errors\": fp + fn,\n",
    "#         }\n",
    "\n",
    "#         total_tp_micro += tp\n",
    "#         total_fp_micro += fp\n",
    "#         total_fn_micro += fn\n",
    "\n",
    "#         macro_precision_sum += precision\n",
    "#         macro_recall_sum += recall\n",
    "#         macro_f1_sum += f1\n",
    "\n",
    "#     # Calculate macro-averaged metrics\n",
    "#     num_labels = len(unique_true_labels)\n",
    "#     macro_precision = macro_precision_sum / num_labels if num_labels > 0 else 0.0\n",
    "#     macro_recall = macro_recall_sum / num_labels if num_labels > 0 else 0.0\n",
    "#     macro_f1 = macro_f1_sum / num_labels if num_labels > 0 else 0.0\n",
    "\n",
    "#     # Calculate micro-averaged (overall) precision, recall, and F1 score\n",
    "#     micro_precision = (\n",
    "#         total_tp_micro / (total_tp_micro + total_fp_micro) if (total_tp_micro + total_fp_micro) > 0 else 0.0\n",
    "#     )\n",
    "#     micro_recall = (\n",
    "#         total_tp_micro / (total_tp_micro + total_fn_micro) if (total_tp_micro + total_fn_micro) > 0 else 0.0\n",
    "#     )\n",
    "#     micro_f1 = (\n",
    "#         2 * micro_precision * micro_recall / (micro_precision + micro_recall)\n",
    "#         if (micro_precision + micro_recall) > 0\n",
    "#         else 0.0\n",
    "#     )\n",
    "\n",
    "#     # Calculate weighted F1 score\n",
    "#     weighted_f1_sum = 0.0\n",
    "#     for label in unique_true_labels:\n",
    "#         weighted_f1_sum += results[label][\"f1\"] * label_counts[label]\n",
    "#     weighted_f1 = weighted_f1_sum / total_instances if total_instances > 0 else 0.0\n",
    "    \n",
    "#     results[\"overall\"] = {\n",
    "#         \"micro_precision\": round(micro_precision, 3),\n",
    "#         \"micro_recall\": round(micro_recall, 3),\n",
    "#         \"micro_f1\": round(micro_f1, 3),\n",
    "#         \"macro_precision\": round(macro_precision, 3),\n",
    "#         \"macro_recall\": round(macro_recall, 3),\n",
    "#         \"macro_f1\": round(macro_f1, 3),\n",
    "#         \"weighted_f1\": round(weighted_f1, 3),\n",
    "#         \"support\": total_instances,\n",
    "#         \"total_tp\": total_tp_micro,\n",
    "#         \"total_fp\": total_fp_micro,\n",
    "#         \"total_fn\": total_fn_micro,\n",
    "#         \"num_errors\": total_fp_micro + total_fn_micro,\n",
    "#     }\n",
    "\n",
    "#     return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6c3871d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def print_error_counts(results):\n",
    "#     fp_list = []\n",
    "#     fn_list = []\n",
    "#     num_errors_list = []\n",
    "\n",
    "#     print(\"Run   FP    FN    FP+FN\")\n",
    "#     for i, res in enumerate(results):\n",
    "#         fp = res[\"overall\"][\"total_fp\"]\n",
    "#         fn = res[\"overall\"][\"total_fn\"]\n",
    "#         num_errors = res[\"overall\"][\"num_errors\"]\n",
    "#         fp_list.append(fp)\n",
    "#         fn_list.append(fn)\n",
    "#         num_errors_list.append(num_errors)\n",
    "#         print(f\"{i:<5} {fp:<5} {fn:<5} {num_errors:<6}\")\n",
    "\n",
    "#     print(\"\\nMean ± Std across splits:\")\n",
    "#     print(f\"FP:    {np.mean(fp_list):.2f} ± {np.std(fp_list):.2f}\")\n",
    "#     print(f\"FN:    {np.mean(fn_list):.2f} ± {np.std(fn_list):.2f}\")\n",
    "#     print(f\"FP+FN: {np.mean(num_errors_list):.2f} ± {np.std(num_errors_list):.2f}\")\n",
    "\n",
    "def calculate_mean_std(\n",
    "        results: list[dict],\n",
    "        cat: str | None,\n",
    "        level: str = \"label\"     # \"label\", \"micro\", or \"macro\"\n",
    "    ) -> dict:\n",
    "    \"\"\"\n",
    "    Compute mean ± std for a single class (\"label\" level) or for the\n",
    "    overall micro / macro aggregates produced by *t14_calculate_metrics*.\n",
    "\n",
    "    Args\n",
    "    ----\n",
    "    results : list of metrics dicts (output of t14_calculate_metrics)\n",
    "    cat     : class label (e.g. \"T1\") – ignored for micro/macro levels\n",
    "    level   : \"label\" | \"micro\" | \"macro\"\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    dict with keys:\n",
    "        mean_precision, mean_recall, mean_f1,\n",
    "        std_precision,  std_recall,  std_f1,\n",
    "        (plus sums and raw means used elsewhere)\n",
    "    \"\"\"\n",
    "    # ------------------------------------------------------------------ #\n",
    "    # Gather the three score lists                                       #\n",
    "    # ------------------------------------------------------------------ #\n",
    "    precision_list, recall_list, f1_list = [], [], []\n",
    "    support_list, num_errors_list = [], []\n",
    "\n",
    "    for res in results:\n",
    "        if level == \"label\":\n",
    "            src = res[cat]                                 # per‑class block\n",
    "        elif level == \"micro\":\n",
    "            src = {                                        # NEW ↓\n",
    "                \"precision\": res[\"overall\"][\"micro_precision\"],\n",
    "                \"recall\":    res[\"overall\"][\"micro_recall\"],\n",
    "                \"f1\":        res[\"overall\"][\"micro_f1\"],\n",
    "                \"support\":   res[\"overall\"][\"support\"],\n",
    "                \"num_errors\": res[\"overall\"][\"num_errors\"],\n",
    "            }\n",
    "        elif level == \"macro\":\n",
    "            src = {                                        # NEW ↓\n",
    "                \"precision\": res[\"overall\"][\"macro_precision\"],\n",
    "                \"recall\":    res[\"overall\"][\"macro_recall\"],\n",
    "                \"f1\":        res[\"overall\"][\"macro_f1\"],\n",
    "                \"support\":   res[\"overall\"][\"support\"],\n",
    "                \"num_errors\": res[\"overall\"][\"num_errors\"],\n",
    "            }\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown level: {level}\")\n",
    "\n",
    "        precision_list.append(src[\"precision\"])\n",
    "        recall_list.append(src[\"recall\"])\n",
    "        f1_list.append(src[\"f1\"])\n",
    "        support_list.append(src[\"support\"])\n",
    "        num_errors_list.append(src[\"num_errors\"])\n",
    "\n",
    "    # ------------------------------------------------------------------ #\n",
    "    # Mean / std                                                         #\n",
    "    # ------------------------------------------------------------------ #\n",
    "    mean_p = sum(precision_list) / len(precision_list)\n",
    "    mean_r = sum(recall_list)    / len(recall_list)\n",
    "    mean_f = sum(f1_list)        / len(f1_list)\n",
    "\n",
    "    std_p = (sum((x - mean_p) ** 2 for x in precision_list) / len(precision_list)) ** 0.5\n",
    "    std_r = (sum((x - mean_r) ** 2 for x in recall_list)    / len(recall_list))    ** 0.5\n",
    "    std_f = (sum((x - mean_f) ** 2 for x in f1_list)        / len(f1_list))        ** 0.5\n",
    "\n",
    "    return {\n",
    "        \"mean_precision\": round(mean_p, 3),\n",
    "        \"mean_recall\":    round(mean_r, 3),\n",
    "        \"mean_f1\":        round(mean_f, 3),\n",
    "        \"std_precision\":  round(std_p, 3),\n",
    "        \"std_recall\":     round(std_r, 3),\n",
    "        \"std_f1\":         round(std_f, 3),\n",
    "        \"sum_support\":    sum(support_list),\n",
    "        \"sum_num_errors\": sum(num_errors_list),\n",
    "        \"raw_mean_precision\": mean_p,   # keep raw for higher‑level macro\n",
    "        \"raw_mean_recall\":    mean_r,\n",
    "        \"raw_mean_f1\":        mean_f,\n",
    "    }\n",
    "\n",
    "\n",
    "###############################################################################\n",
    "# Updated helper: output_tabular_performance                                  #\n",
    "###############################################################################\n",
    "def output_tabular_performance(\n",
    "        results: list[dict],\n",
    "        categories: list[str] = (\"T1\", \"T2\", \"T3\", \"T4\"),\n",
    "        show_overall: bool = True         # NEW flag\n",
    "    ) -> None:\n",
    "    \"\"\"\n",
    "    Print mean ± std precision/recall/F1 for each class, followed by:\n",
    "      • category‑macro average (same as before)\n",
    "      • micro‑average (overall)\n",
    "      • macro‑average (overall)\n",
    "    \"\"\"\n",
    "    # ------------------------------------------------------------------ #\n",
    "    # Per‑class lines                                                    #\n",
    "    # ------------------------------------------------------------------ #\n",
    "    label_means_p, label_means_r, label_means_f = [], [], []\n",
    "\n",
    "    for cat in categories:\n",
    "        stats = calculate_mean_std(results, cat, level=\"label\")\n",
    "        print(f\"{cat:8s} \"\n",
    "              f\"{stats['mean_precision']:.3f}({stats['std_precision']:.3f}) \"\n",
    "              f\"{stats['mean_recall']:.3f}({stats['std_recall']:.3f}) \"\n",
    "              f\"{stats['mean_f1']:.3f}({stats['std_f1']:.3f})\")\n",
    "\n",
    "        label_means_p.append(stats[\"raw_mean_precision\"])\n",
    "        label_means_r.append(stats[\"raw_mean_recall\"])\n",
    "        label_means_f.append(stats[\"raw_mean_f1\"])\n",
    "\n",
    "    # ------------------------------------------------------------------ #\n",
    "    # Category‑macro (average of label means)                            #\n",
    "    # ------------------------------------------------------------------ #\n",
    "    print(f\"{'Cat‑Macro':8s} \"\n",
    "          f\"{sum(label_means_p)/len(label_means_p):.3f} \"\n",
    "          f\"{sum(label_means_r)/len(label_means_r):.3f} \"\n",
    "          f\"{sum(label_means_f)/len(label_means_f):.3f}\")\n",
    "\n",
    "    # ------------------------------------------------------------------ #\n",
    "    # Overall micro / macro                                              #\n",
    "    # ------------------------------------------------------------------ #\n",
    "    if show_overall:\n",
    "        micro = calculate_mean_std(results, None, level=\"micro\")\n",
    "        macro = calculate_mean_std(results, None, level=\"macro\")\n",
    "\n",
    "        print(f\"{'MicroAvg.':8s} \"\n",
    "              f\"{micro['mean_precision']:.3f}({micro['std_precision']:.3f}) \"\n",
    "              f\"{micro['mean_recall']:.3f}({micro['std_recall']:.3f}) \"\n",
    "              f\"{micro['mean_f1']:.3f}({micro['std_f1']:.3f})\")\n",
    "\n",
    "        print(f\"{'MacroAvg.':8s} \"\n",
    "              f\"{macro['mean_precision']:.3f}({macro['std_precision']:.3f}) \"\n",
    "              f\"{macro['mean_recall']:.3f}({macro['std_recall']:.3f}) \"\n",
    "              f\"{macro['mean_f1']:.3f}({macro['std_f1']:.3f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e1f60051",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mixtral T-stage metrics:\n",
      "ZSCOT T-stage:\n",
      "{'micro_precision': 0.85, 'micro_recall': 0.863, 'micro_f1': 0.856, 'macro_precision': 0.831, 'macro_recall': 0.765, 'macro_f1': 0.792, 'weighted_f1': 0.854, 'support': 800, 'total_tp': 690, 'total_fp': 122, 'total_fn': 110, 'num_errors': 110}\n",
      "\n",
      "RAG T-stage:\n",
      "{'micro_precision': 0.81, 'micro_recall': 0.815, 'micro_f1': 0.812, 'macro_precision': 0.771, 'macro_recall': 0.73, 'macro_f1': 0.743, 'weighted_f1': 0.812, 'support': 800, 'total_tp': 652, 'total_fp': 153, 'total_fn': 148, 'num_errors': 148}\n",
      "\n",
      "KEwLTM T-stage:\n",
      "T1       0.904(0.017) 0.812(0.040) 0.855(0.018)\n",
      "T2       0.882(0.022) 0.938(0.018) 0.909(0.005)\n",
      "T3       0.834(0.054) 0.810(0.058) 0.818(0.018)\n",
      "T4       0.807(0.082) 0.634(0.038) 0.707(0.029)\n",
      "Cat‑Macro 0.857 0.799 0.822\n",
      "MicroAvg. 0.876(0.006) 0.878(0.007) 0.877(0.007)\n",
      "MacroAvg. 0.857(0.022) 0.799(0.020) 0.822(0.010)\n",
      "Run  WrongRows  FN  FP  (FP+FN)\n",
      "0    79         79  81  160\n",
      "1    88         88  89  177\n",
      "2    95         95  95  190\n",
      "3    83         83  86  169\n",
      "4    84         84  86  170\n",
      "5    88         88  91  179\n",
      "6    86         86  87  173\n",
      "7    81         81  81  162\n",
      "\n",
      "Mean ± Std across splits\n",
      "Wrong rows: 85.50 ± 4.66\n",
      "FN:         85.50 ± 4.66\n",
      "FP:         87.00 ± 4.44\n",
      "\n",
      "KEwRAG T-stage:\n",
      "{'micro_precision': 0.853, 'micro_recall': 0.848, 'micro_f1': 0.85, 'macro_precision': 0.792, 'macro_recall': 0.728, 'macro_f1': 0.746, 'weighted_f1': 0.846, 'support': 800, 'total_tp': 678, 'total_fp': 117, 'total_fn': 122, 'num_errors': 122}\n"
     ]
    }
   ],
   "source": [
    "mixtral_df = pd.read_csv('/home/yl3427/cylab/selfCorrectionAgent/mixtral_rag_result/0929_ltm_rag2.csv') # 이거야. 여기에 mixtral결과 다있어\n",
    "\n",
    "print(\"Mixtral T-stage metrics:\")\n",
    "\n",
    "print(\"ZSCOT T-stage:\")\n",
    "print(t14_calculate_metrics(mixtral_df['t'], mixtral_df['zscot_t_stage'])['overall'])\n",
    "print()\n",
    "print(\"RAG T-stage:\")\n",
    "print(t14_calculate_metrics(mixtral_df['t'], mixtral_df['rag_raw_t_stage'])['overall'])\n",
    "print()\n",
    "kewltm_t_results = []\n",
    "t_label = 't'\n",
    "kewltm_t_stage = \"cmem_t_40reports_ans_str\"\n",
    "run_lst = [0, 1, 2, 3, 4, 5, 6, 8]\n",
    "\n",
    "for run in run_lst:\n",
    "    t_test_df = pd.read_csv(\n",
    "        f\"/home/yl3427/cylab/selfCorrectionAgent/result/0718_t14_dynamic_test_{run}_outof_10runs.csv\"\n",
    "    ).sort_values(by=\"patient_filename\")\n",
    "\n",
    "    kewltm_t_results.append(\n",
    "        t14_calculate_metrics(t_test_df[t_label], t_test_df[kewltm_t_stage])\n",
    "    )\n",
    "print(\"KEwLTM T-stage:\")\n",
    "output_tabular_performance(kewltm_t_results)\n",
    "print_error_counts(kewltm_t_results)\n",
    "\n",
    "print()\n",
    "print(\"KEwRAG T-stage:\")\n",
    "print(t14_calculate_metrics(mixtral_df['t'], mixtral_df['ltm_rag1_t_stage'])['overall'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "15173e88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mixtral N-stage metrics:\n",
      "ZSCOT N-stage:\n",
      "{'micro_precision': 0.874, 'micro_recall': 0.873, 'micro_f1': 0.873, 'macro_precision': 0.843, 'macro_recall': 0.822, 'macro_f1': 0.832, 'weighted_f1': 0.872, 'support': 800, 'total_tp': 698, 'total_fp': 101, 'total_fn': 102, 'num_errors': 102}\n",
      "\n",
      "RAG N-stage:\n",
      "{'micro_precision': 0.841, 'micro_recall': 0.835, 'micro_f1': 0.838, 'macro_precision': 0.803, 'macro_recall': 0.799, 'macro_f1': 0.797, 'weighted_f1': 0.84, 'support': 800, 'total_tp': 668, 'total_fp': 126, 'total_fn': 132, 'num_errors': 132}\n",
      "\n",
      "KEwLTM N-stage:\n",
      "N0       0.944(0.008) 0.952(0.018) 0.948(0.011)\n",
      "N1       0.885(0.020) 0.883(0.026) 0.884(0.010)\n",
      "N2       0.713(0.031) 0.745(0.054) 0.727(0.022)\n",
      "N3       0.886(0.058) 0.784(0.042) 0.830(0.017)\n",
      "Cat‑Macro 0.857 0.841 0.847\n",
      "MicroAvg. 0.883(0.007) 0.883(0.007) 0.883(0.007)\n",
      "MacroAvg. 0.857(0.011) 0.841(0.011) 0.847(0.008)\n",
      "Run  WrongRows  FN  FP  (FP+FN)\n",
      "0    85         85  85  170\n",
      "1    78         78  78  156\n",
      "2    85         85  84  169\n",
      "3    83         83  83  166\n",
      "4    80         80  80  160\n",
      "5    81         81  81  162\n",
      "6    74         74  73  147\n",
      "7    91         91  91  182\n",
      "\n",
      "Mean ± Std across splits\n",
      "Wrong rows: 82.12 ± 4.81\n",
      "FN:         82.12 ± 4.81\n",
      "FP:         81.88 ± 4.96\n",
      "\n",
      "KEwRAG N-stage:\n",
      "{'micro_precision': 0.853, 'micro_recall': 0.859, 'micro_f1': 0.856, 'macro_precision': 0.807, 'macro_recall': 0.814, 'macro_f1': 0.81, 'weighted_f1': 0.856, 'support': 800, 'total_tp': 687, 'total_fp': 118, 'total_fn': 113, 'num_errors': 113}\n"
     ]
    }
   ],
   "source": [
    "print(\"Mixtral N-stage metrics:\")\n",
    "\n",
    "print(\"ZSCOT N-stage:\")\n",
    "print(n03_calculate_metrics(mixtral_df['n'], mixtral_df['zscot_n_stage'])['overall'])\n",
    "print()\n",
    "print(\"RAG N-stage:\")\n",
    "print(n03_calculate_metrics(mixtral_df['n'], mixtral_df['rag_raw_n_stage'])['overall'])\n",
    "print()\n",
    "kewltm_n_results = []\n",
    "n_label = 'n'\n",
    "kewltm_n_stage = \"cmem_n_40reports_ans_str\"\n",
    "run_lst = [0, 1, 3, 4, 5, 6, 7, 9]\n",
    "for run in run_lst:\n",
    "    n_test_df = pd.read_csv(\n",
    "        f\"/home/yl3427/cylab/selfCorrectionAgent/result/0718_n03_dynamic_test_{run}_outof_10runs.csv\"\n",
    "    ).sort_values(by=\"patient_filename\")\n",
    "\n",
    "    kewltm_n_results.append(\n",
    "        n03_calculate_metrics(n_test_df[n_label], n_test_df[kewltm_n_stage])\n",
    "    )\n",
    "print(\"KEwLTM N-stage:\")\n",
    "output_tabular_performance(kewltm_n_results, categories=[\"N0\", \"N1\", \"N2\", \"N3\"])\n",
    "print_error_counts(kewltm_n_results)\n",
    "\n",
    "print()\n",
    "print(\"KEwRAG N-stage:\")\n",
    "print(n03_calculate_metrics(mixtral_df['n'], mixtral_df['ltm_rag1_n_stage'])['overall'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69268edf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5f7b164a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Med42 T-stage metrics:\n",
      "ZSCOT T-stage:\n",
      "{'micro_precision': 0.769, 'micro_recall': 0.769, 'micro_f1': 0.769, 'macro_precision': 0.746, 'macro_recall': 0.678, 'macro_f1': 0.703, 'weighted_f1': 0.77, 'support': 800, 'total_tp': 615, 'total_fp': 185, 'total_fn': 185, 'num_errors': 185}\n",
      "\n",
      "RAG T-stage:\n",
      "{'micro_precision': 0.836, 'micro_recall': 0.836, 'micro_f1': 0.836, 'macro_precision': 0.786, 'macro_recall': 0.748, 'macro_f1': 0.764, 'weighted_f1': 0.834, 'support': 800, 'total_tp': 669, 'total_fp': 131, 'total_fn': 131, 'num_errors': 131}\n",
      "\n",
      "KEwLTM T-stage:\n",
      "T1       0.813(0.073) 0.759(0.076) 0.783(0.064)\n",
      "T2       0.855(0.031) 0.913(0.023) 0.882(0.016)\n",
      "T3       0.869(0.063) 0.703(0.099) 0.770(0.065)\n",
      "T4       0.630(0.046) 0.615(0.057) 0.621(0.042)\n",
      "Cat‑Macro 0.792 0.747 0.764\n",
      "MicroAvg. 0.835(0.025) 0.835(0.025) 0.835(0.025)\n",
      "MacroAvg. 0.792(0.032) 0.747(0.043) 0.764(0.034)\n",
      "Run  WrongRows  FN  FP  (FP+FN)\n",
      "0    92         92  92  184\n",
      "1    143        143 143 286\n",
      "2    127        127 127 254\n",
      "3    115        115 115 230\n",
      "4    93         93  93  186\n",
      "5    100        100 100 200\n",
      "6    124        124 124 248\n",
      "7    130        130 130 260\n",
      "\n",
      "Mean ± Std across splits\n",
      "Wrong rows: 115.50 ± 17.57\n",
      "FN:         115.50 ± 17.57\n",
      "FP:         115.50 ± 17.57\n",
      "\n",
      "KEwRAG T-stage:\n",
      "{'micro_precision': 0.879, 'micro_recall': 0.879, 'micro_f1': 0.879, 'macro_precision': 0.838, 'macro_recall': 0.793, 'macro_f1': 0.812, 'weighted_f1': 0.876, 'support': 800, 'total_tp': 703, 'total_fp': 97, 'total_fn': 97, 'num_errors': 97}\n"
     ]
    }
   ],
   "source": [
    "kewltm_t_stage = 'kepa_t_ans_str'\n",
    "zscot_t_df = pd.read_csv('/home/yl3427/cylab/selfCorrectionAgent/result/1118_t14_med42_v2_test_800.csv').sort_values(by=\"patient_filename\")[[\"patient_filename\", 't', 'zscot_t_ans_str']]\n",
    "rag_t_df = pd.read_csv('/home/yl3427/cylab/selfCorrectionAgent/result/1120_t14_rag_raw_med42_v2_800.csv').sort_values(by=\"patient_filename\")[[\"patient_filename\", 't', 't14_rag_raw_t_pred']]\n",
    "ltm_rag_t_df = pd.read_csv('/home/yl3427/cylab/selfCorrectionAgent/result/1128_t14_ltm_rag1_med42_v2_800.csv').sort_values(by=\"patient_filename\")[[\"patient_filename\", 't', 't14_ltm_rag1_t_pred']]\n",
    "\n",
    "print(\"Med42 T-stage metrics:\")\n",
    "\n",
    "print(\"ZSCOT T-stage:\")\n",
    "print(t14_calculate_metrics(zscot_t_df['t'], zscot_t_df['zscot_t_ans_str'])['overall'])\n",
    "print()\n",
    "print(\"RAG T-stage:\")\n",
    "print(t14_calculate_metrics(rag_t_df['t'], rag_t_df['t14_rag_raw_t_pred'])['overall'])\n",
    "print()\n",
    "kewltm_t_results = []\n",
    "t_label = 't'\n",
    "run_lst = [0, 1, 2, 3, 4, 5, 6, 8]\n",
    "\n",
    "for run in run_lst:\n",
    "    t_test_df = pd.read_csv(\n",
    "        f\"/home/yl3427/cylab/selfCorrectionAgent/result/1114_t14_med42_v2_test_{run}_outof_10runs.csv\"\n",
    "    ).sort_values(by=\"patient_filename\")\n",
    "\n",
    "    kewltm_t_results.append(\n",
    "        t14_calculate_metrics(t_test_df[t_label], t_test_df[kewltm_t_stage])\n",
    "    )\n",
    "print(\"KEwLTM T-stage:\")\n",
    "output_tabular_performance(kewltm_t_results)\n",
    "print_error_counts(kewltm_t_results)\n",
    "\n",
    "print()\n",
    "print(\"KEwRAG T-stage:\")\n",
    "print(t14_calculate_metrics(ltm_rag_t_df['t'], ltm_rag_t_df['t14_ltm_rag1_t_pred'])['overall'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8635d140",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Med42 N-stage metrics:\n",
      "ZSCOT N-stage:\n",
      "{'micro_precision': 0.738, 'micro_recall': 0.738, 'micro_f1': 0.738, 'macro_precision': 0.748, 'macro_recall': 0.723, 'macro_f1': 0.724, 'weighted_f1': 0.742, 'support': 800, 'total_tp': 590, 'total_fp': 210, 'total_fn': 210, 'num_errors': 210}\n",
      "\n",
      "RAG N-stage:\n",
      "{'micro_precision': 0.79, 'micro_recall': 0.79, 'micro_f1': 0.79, 'macro_precision': 0.76, 'macro_recall': 0.799, 'macro_f1': 0.759, 'weighted_f1': 0.796, 'support': 800, 'total_tp': 632, 'total_fp': 168, 'total_fn': 168, 'num_errors': 168}\n",
      "\n",
      "KEwLTM N-stage:\n",
      "N0       0.950(0.011) 0.821(0.059) 0.879(0.032)\n",
      "N1       0.775(0.056) 0.821(0.032) 0.795(0.022)\n",
      "N2       0.657(0.067) 0.711(0.076) 0.675(0.018)\n",
      "N3       0.759(0.103) 0.858(0.029) 0.800(0.062)\n",
      "Cat‑Macro 0.785 0.803 0.787\n",
      "MicroAvg. 0.809(0.024) 0.809(0.024) 0.809(0.024)\n",
      "MacroAvg. 0.785(0.021) 0.803(0.025) 0.788(0.026)\n",
      "Run  WrongRows  FN  FP  (FP+FN)\n",
      "0    142        142 142 284\n",
      "1    140        140 140 280\n",
      "2    93         93  93  186\n",
      "3    148        148 148 296\n",
      "4    136        136 136 272\n",
      "5    133        133 133 266\n",
      "6    127        127 127 254\n",
      "7    149        149 149 298\n",
      "\n",
      "Mean ± Std across splits\n",
      "Wrong rows: 133.50 ± 16.79\n",
      "FN:         133.50 ± 16.79\n",
      "FP:         133.50 ± 16.79\n",
      "\n",
      "KEwRAG N-stage:\n",
      "{'micro_precision': 0.88, 'micro_recall': 0.88, 'micro_f1': 0.88, 'macro_precision': 0.845, 'macro_recall': 0.849, 'macro_f1': 0.846, 'weighted_f1': 0.881, 'support': 800, 'total_tp': 704, 'total_fp': 96, 'total_fn': 96, 'num_errors': 96}\n"
     ]
    }
   ],
   "source": [
    "kewltm_n_stage = 'kepa_n_ans_str'\n",
    "zscot_n_df = pd.read_csv('/home/yl3427/cylab/selfCorrectionAgent/result/1118_n03_med42_v2_test_800.csv').sort_values(by=\"patient_filename\")[[\"patient_filename\", 'n', 'zscot_n_ans_str']]\n",
    "rag_n_df = pd.read_csv('/home/yl3427/cylab/selfCorrectionAgent/result/1120_n03_rag_raw_med42_v2_800.csv').sort_values(by=\"patient_filename\")[[\"patient_filename\", 'n', 'n03_rag_raw_n_pred']]\n",
    "ltm_rag_n_df = pd.read_csv('/home/yl3427/cylab/selfCorrectionAgent/result/1128_n03_ltm_rag1_med42_v2_800.csv').sort_values(by=\"patient_filename\")[[\"patient_filename\", 'n', 'n03_ltm_rag1_n_pred']]\n",
    "\n",
    "print(\"Med42 N-stage metrics:\")\n",
    "\n",
    "print(\"ZSCOT N-stage:\")\n",
    "print(n03_calculate_metrics(zscot_n_df['n'], zscot_n_df['zscot_n_ans_str'])['overall'])\n",
    "print()\n",
    "print(\"RAG N-stage:\")\n",
    "print(n03_calculate_metrics(rag_n_df['n'], rag_n_df['n03_rag_raw_n_pred'])['overall'])\n",
    "print()\n",
    "kewltm_n_results = []\n",
    "n_label = 'n'\n",
    "run_lst = [0, 1, 3, 4, 5, 6, 7, 9]\n",
    "\n",
    "for run in run_lst:\n",
    "    n_test_df = pd.read_csv(\n",
    "        f\"/home/yl3427/cylab/selfCorrectionAgent/result/1114_n03_med42_v2_test_{run}_outof_10runs.csv\"\n",
    "    ).sort_values(by=\"patient_filename\")\n",
    "\n",
    "    kewltm_n_results.append(\n",
    "        n03_calculate_metrics(n_test_df[n_label], n_test_df[kewltm_n_stage])\n",
    "    )\n",
    "print(\"KEwLTM N-stage:\")\n",
    "output_tabular_performance(kewltm_n_results, categories=[\"N0\", \"N1\", \"N2\", \"N3\"])\n",
    "print_error_counts(kewltm_n_results)\n",
    "\n",
    "print()\n",
    "print(\"KEwRAG N-stage:\")\n",
    "print(n03_calculate_metrics(ltm_rag_n_df['n'], ltm_rag_n_df['n03_ltm_rag1_n_pred'])['overall'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66c22b92",
   "metadata": {},
   "source": [
    "# Error analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e76f7d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('/home/yl3427/cylab/selfCorrectionAgent/mixtral_rag_result/0929_ltm_rag2.csv')\n",
    "df.columns\n",
    "# new_df2 = pd.read_csv(f\"/home/yl3427/cylab/selfCorrectionAgent/result/0718_t14_dynamic_test_0_outof_10runs.csv\")\n",
    "# new_df2.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from typing import Tuple, Optional\n",
    "\n",
    "\n",
    "###############################################################################\n",
    "# Helper – map the integer label to the string used in the predictions\n",
    "###############################################################################\n",
    "def _canon_label(x, stage: str) -> str:\n",
    "    \"\"\"\n",
    "    Convert the integer‑coded ground‑truth to the canonical string that appears\n",
    "    in prediction strings.\n",
    "\n",
    "        T‑stage: 0 → \"T1\", 1 → \"T2\", …\n",
    "        N‑stage: 0 → \"N0\", 1 → \"N1\", …\n",
    "    \"\"\"\n",
    "    stage = stage.lower()\n",
    "    if stage == \"t\":\n",
    "        return f\"T{int(x) + 1}\"\n",
    "    elif stage == \"n\":\n",
    "        return f\"N{int(x)}\"\n",
    "    else:\n",
    "        raise ValueError(f'Unknown stage \"{stage}\". Use \"t\" or \"n\".')\n",
    "\n",
    "\n",
    "###############################################################################\n",
    "# Core – compare two methods and write three CSVs                             #\n",
    "###############################################################################\n",
    "def compare_error_cases(\n",
    "    df_baseline: pd.DataFrame,\n",
    "    df_method: pd.DataFrame,\n",
    "    *,\n",
    "    stage: str,\n",
    "    true_col: str,\n",
    "    base_pred_col: str,\n",
    "    base_reason_col: Optional[str],\n",
    "    meth_pred_col: str,\n",
    "    meth_reason_col: Optional[str],\n",
    "    patient_id_col: str = \"patient_filename\",\n",
    "    out_common_csv: str | Path,\n",
    "    out_base_only_csv: str | Path,\n",
    "    out_meth_only_csv: str | Path,\n",
    ") -> Tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Create three CSVs: common errors, baseline‑only errors, method‑only errors.\n",
    "    Returns the three data frames.\n",
    "    \"\"\"\n",
    "\n",
    "    # -------- flag errors row‑wise ----------------------------------- #\n",
    "    def _flag(df: pd.DataFrame, pred_col: str) -> pd.DataFrame:\n",
    "        canon = df[true_col].apply(lambda v: _canon_label(v, stage))\n",
    "        wrong = ~df.apply(\n",
    "            lambda r: canon.loc[r.name] in str(r[pred_col]).upper(), axis=1\n",
    "        )\n",
    "        out = df.copy()\n",
    "        out[\"canon_truth\"] = canon\n",
    "        out[\"is_error\"] = wrong\n",
    "        return out\n",
    "\n",
    "    base_f = _flag(df_baseline, base_pred_col)\n",
    "    meth_f = _flag(df_method, meth_pred_col)\n",
    "\n",
    "    # -------- build id sets ------------------------------------------ #\n",
    "    base_err_ids = set(base_f.loc[base_f.is_error, patient_id_col])\n",
    "    meth_err_ids = set(meth_f.loc[meth_f.is_error, patient_id_col])\n",
    "\n",
    "    common_ids    = base_err_ids & meth_err_ids\n",
    "    base_only_ids = base_err_ids - meth_err_ids\n",
    "    meth_only_ids = meth_err_ids - base_err_ids\n",
    "\n",
    "    # -------- helpers to pick & merge -------------------------------- #\n",
    "    def _subset(df, ids, keep_pred, keep_reason):\n",
    "        cols = [patient_id_col, true_col, \"canon_truth\", keep_pred, \"is_error\"]\n",
    "        if keep_reason is not None and keep_reason in df.columns:\n",
    "            cols.insert(3, keep_reason)\n",
    "        return df.loc[df[patient_id_col].isin(ids), cols].copy()\n",
    "\n",
    "    common_df = _subset(base_f, common_ids, base_pred_col, base_reason_col).merge(\n",
    "        _subset(meth_f, common_ids, meth_pred_col, meth_reason_col),\n",
    "        on=[patient_id_col],\n",
    "        suffixes=(\"_base\", \"_meth\"),\n",
    "        how=\"left\",\n",
    "    )\n",
    "    base_only_df = _subset(base_f, base_only_ids, base_pred_col, base_reason_col)\n",
    "    meth_only_df = _subset(meth_f, meth_only_ids, meth_pred_col, meth_reason_col)\n",
    "\n",
    "    # -------- write to disk ------------------------------------------ #\n",
    "    Path(out_common_csv).parent.mkdir(parents=True, exist_ok=True)\n",
    "    common_df.to_csv(out_common_csv, index=False)\n",
    "    base_only_df.to_csv(out_base_only_csv, index=False)\n",
    "    meth_only_df.to_csv(out_meth_only_csv, index=False)\n",
    "\n",
    "    print(f\"[saved] {out_common_csv}   ({len(common_df)} rows)\")\n",
    "    print(f\"[saved] {out_base_only_csv} ({len(base_only_df)} rows)\")\n",
    "    print(f\"[saved] {out_meth_only_csv} ({len(meth_only_df)} rows)\")\n",
    "\n",
    "    return common_df, base_only_df, meth_only_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e8b5cad9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Comparing ZSCOT vs. KEwLTM (T‑stage) ---\n",
      "[saved] t_common_errors_zscot_kewltm.csv   (53 rows)\n",
      "[saved] t_zscot_only_errors_vs_kewltm.csv (46 rows)\n",
      "[saved] t_kewltm_only_errors_vs_zscot.csv (26 rows)\n",
      "\n",
      "--- Comparing RAG vs. KEwRAG (T‑stage) ---\n",
      "[saved] t_common_errors_rag_kewrag.csv   (67 rows)\n",
      "[saved] t_rag_only_errors_vs_kewrag.csv (81 rows)\n",
      "[saved] t_kewrag_only_errors_vs_rag.csv (55 rows)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(                                     patient_filename  t_base  \\\n",
       " 0   TCGA-5L-AAT0.F9B6971F-23C0-465F-BFEC-778BF228A1AE       1   \n",
       " 1   TCGA-5L-AAT1.B5CA42BB-9514-42C6-9FB0-C8889C1DC51A       1   \n",
       " 2   TCGA-A2-A0CK.B065FC65-CD33-4878-AE2C-7E8C04F5ECAB       2   \n",
       " 3   TCGA-A2-A0ET.E9D3FFF1-5FB2-4F17-9C1D-D9775E3CC5AC       1   \n",
       " 4   TCGA-A2-A0SV.161E2817-7DB2-46F8-BFEB-256DBBEFE633       1   \n",
       " ..                                                ...     ...   \n",
       " 62  TCGA-PL-A8LV.D35DBECD-5241-4562-85CC-2822BB338279       3   \n",
       " 63  TCGA-PL-A8LX.E6DD0840-4D71-4EEC-B559-F6BFC9E7E68B       3   \n",
       " 64  TCGA-PL-A8LY.8C97B391-96B4-468D-AAA3-24E196DE03CA       2   \n",
       " 65  TCGA-PL-A8LZ.436F3280-98C7-4FA9-BD6F-1B02CAF1D262       3   \n",
       " 66  TCGA-S3-AA15.DD2B9E47-8C67-4599-B0B6-0D30DE727B55       1   \n",
       " \n",
       "    canon_truth_base                                rag_raw_t_reasoning  \\\n",
       " 0                T2  According to the pathology report, the patient...   \n",
       " 1                T2  According to the pathology report, the patient...   \n",
       " 2                T3  The report states that 'TUMOR SIZE: 0.5 CM BY ...   \n",
       " 3                T2  To determine the pathologic T stage of the pat...   \n",
       " 4                T2  To determine the pathologic T stage of the pat...   \n",
       " ..              ...                                                ...   \n",
       " 62               T4  The pathologic report states that the gross as...   \n",
       " 63               T4  To determine the pathologic T stage of the pat...   \n",
       " 64               T3  To determine the pathologic T stage of the pat...   \n",
       " 65               T4  To determine the pathologic T stage of the pat...   \n",
       " 66               T2  The patient's pathology report mentions that t...   \n",
       " \n",
       "                 rag_raw_t_stage  is_error_base  t_meth canon_truth_meth  \\\n",
       " 0                           T1c           True       1               T2   \n",
       " 1                            T1           True       1               T2   \n",
       " 2                            T1           True       2               T3   \n",
       " 3                           T1c           True       1               T2   \n",
       " 4                           T4b           True       1               T2   \n",
       " ..                          ...            ...     ...              ...   \n",
       " 62                           T2           True       3               T4   \n",
       " 63  Unable to determine T stage           True       3               T4   \n",
       " 64                           T2           True       2               T3   \n",
       " 65                           T2           True       3               T4   \n",
       " 66                          T1c           True       1               T2   \n",
       " \n",
       "                                  ltm_rag1_t_reasoning ltm_rag1_t_stage  \\\n",
       " 0   The pathology report states that the gross des...               T1   \n",
       " 1   Based on the pathology report, the patient has...               T1   \n",
       " 2   The pathology report states that the 'TUMOR SI...               T1   \n",
       " 3   Based on the pathology report, the patient's b...              T1c   \n",
       " 4   The report states that the tumor size (greates...              T4d   \n",
       " ..                                                ...              ...   \n",
       " 62  The report provides the size of the tumor as 5...               T2   \n",
       " 63  The pathology report provides information abou...         T1 or T2   \n",
       " 64  Based on the information provided in the patho...               T2   \n",
       " 65  Based on the information provided in the patho...               T2   \n",
       " 66  The report states that the 'Greatest dimension...              T1c   \n",
       " \n",
       "     is_error_meth  \n",
       " 0            True  \n",
       " 1            True  \n",
       " 2            True  \n",
       " 3            True  \n",
       " 4            True  \n",
       " ..            ...  \n",
       " 62           True  \n",
       " 63           True  \n",
       " 64           True  \n",
       " 65           True  \n",
       " 66           True  \n",
       " \n",
       " [67 rows x 11 columns],\n",
       "                                       patient_filename  t canon_truth  \\\n",
       " 8    TCGA-A1-A0SG.89A67051-898A-4E2A-A874-E0165F274E63  1          T2   \n",
       " 23   TCGA-A2-A0D4.990F5885-8EDD-42A2-AEFE-9DEE3E95A0B1  1          T2   \n",
       " 44   TCGA-A2-A1G6.02B4C31E-38F0-4AF3-8DAD-527D5F27C11A  1          T2   \n",
       " 56   TCGA-A2-A3XZ.A7B6FCF6-1B81-40B5-AD92-54681CBC8FE0  0          T1   \n",
       " 61   TCGA-A2-A4S0.0118D9AE-B923-4EEF-BBBC-82D559763FF9  1          T2   \n",
       " ..                                                 ... ..         ...   \n",
       " 754  TCGA-LL-A7T0.5540DD9C-FA3F-48B6-8243-414616816A76  1          T2   \n",
       " 757  TCGA-MS-A51U.58D54D1A-3A8D-40DD-A643-846CAF242494  1          T2   \n",
       " 761  TCGA-OL-A5D8.FD496AFB-43EF-41D8-850F-E57DD53C185C  2          T3   \n",
       " 766  TCGA-OL-A5RX.9536A134-A44C-4698-A525-DEEE7FACD144  0          T1   \n",
       " 792  TCGA-S3-AA12.9F96B0B1-2B88-49AE-8D3C-9FED70F1D950  2          T3   \n",
       " \n",
       "                                    rag_raw_t_reasoning rag_raw_t_stage  \\\n",
       " 8    The patient's pathologic T stage can be determ...              T1   \n",
       " 23   To determine the pathologic T stage of the pat...              T1   \n",
       " 44   According to the AJCC's TNM Staging System gui...              T1   \n",
       " 56   To determine the pathologic T stage of the pat...              T2   \n",
       " 61   The report mentions that there are two invasiv...             T1c   \n",
       " ..                                                 ...             ...   \n",
       " 754  The report mentions 'LARGEST INVASIVE CARCINOM...              T1   \n",
       " 757  To determine the pathologic T stage of the pat...              T3   \n",
       " 761  To determine the pathologic T stage of the pat...              T2   \n",
       " 766  To determine the pathologic T stage of the pat...              T2   \n",
       " 792  To determine the pathologic T stage of the pat...              T2   \n",
       " \n",
       "      is_error  \n",
       " 8        True  \n",
       " 23       True  \n",
       " 44       True  \n",
       " 56       True  \n",
       " 61       True  \n",
       " ..        ...  \n",
       " 754      True  \n",
       " 757      True  \n",
       " 761      True  \n",
       " 766      True  \n",
       " 792      True  \n",
       " \n",
       " [81 rows x 6 columns],\n",
       "                                       patient_filename  t canon_truth  \\\n",
       " 0    TCGA-3C-AALI.84E6A935-1A49-4BC1-9669-3DEA161CF6FC  1          T2   \n",
       " 17   TCGA-A2-A0CO.8C440009-E05E-4046-96AB-BAB211CB7D02  2          T3   \n",
       " 36   TCGA-A2-A0YH.0C344CD7-6FAB-462B-9A8E-75B720F2D626  1          T2   \n",
       " 39   TCGA-A2-A0YT.1C420F6B-8720-4CB0-9FB3-86A0EC05FBBD  3          T4   \n",
       " 43   TCGA-A2-A1G4.5AD3B697-F097-496C-978B-F73BDA5394A7  2          T3   \n",
       " 46   TCGA-A2-A25C.1D4CA1BA-B534-4CEA-ABA5-EF9B31694204  1          T2   \n",
       " 48   TCGA-A2-A25E.B81555D4-78D9-4235-AABE-F852CA6F15B5  1          T2   \n",
       " 60   TCGA-A2-A4RY.2D6815B5-7704-4D67-93D1-78EDD924F131  2          T3   \n",
       " 85   TCGA-A7-A4SE.C0C49D41-AA66-435B-9312-FAD8E2DC3305  1          T2   \n",
       " 133  TCGA-A8-A08H.33188769-D86E-41AD-A74D-31A7C2DD08B7  1          T2   \n",
       " 142  TCGA-A8-A08X.CA911D90-5DDA-49F1-993F-68FCA77A8558  3          T4   \n",
       " 190  TCGA-AC-A2QJ.E422C6E8-084F-42B8-84E7-4EF387F098C7  3          T4   \n",
       " 199  TCGA-AC-A4ZE.6931CAAE-6DFE-4A61-84D5-4EC804C2F2C6  2          T3   \n",
       " 200  TCGA-AC-A5XS.C4F259BA-7C00-4326-ACB2-924C4AE33E87  1          T2   \n",
       " 217  TCGA-AN-A0AJ.6E2365F3-0875-4949-8409-7E26BBCDE700  2          T3   \n",
       " 222  TCGA-AN-A0AS.0D1ACC76-910E-4243-A229-5F76B5A3299A  1          T2   \n",
       " 236  TCGA-AN-A0FZ.8CC70CDC-01EC-4E2B-84C1-F1C946602CCE  1          T2   \n",
       " 241  TCGA-AN-A0XP.80E9B7CE-0D5D-4C01-A8E3-E6345BE145EC  1          T2   \n",
       " 245  TCGA-AN-A0XU.8216B837-DCF3-4E7F-A72A-212655F394F0  1          T2   \n",
       " 246  TCGA-AN-A0XV.5F626AB6-EEB5-4005-97C5-9E48D9FA7730  1          T2   \n",
       " 247  TCGA-AN-A0XW.5CBC6417-4E3D-4E9C-AE93-A56B777EF2F4  1          T2   \n",
       " 258  TCGA-AO-A0JB.6C202D3E-5670-4B63-82A2-9EE7AC385089  2          T3   \n",
       " 266  TCGA-AO-A0JM.3023C34B-CF59-43EA-9241-33B738D19B05  1          T2   \n",
       " 275  TCGA-AQ-A0Y5.5BC01981-366E-44EC-A986-087DB7904E5C  1          T2   \n",
       " 346  TCGA-AR-A2LR.C310F4EC-D8B1-45C4-8D55-705A9EE35902  0          T1   \n",
       " 350  TCGA-AR-A5QQ.511282C5-02AC-4105-8B52-5BBEC8B45D22  2          T3   \n",
       " 351  TCGA-B6-A0I1.1E498731-FE74-493E-A330-477643800305  1          T2   \n",
       " 362  TCGA-B6-A0RP.889D6E80-A656-49FA-9DFA-1674EB3877F6  1          T2   \n",
       " 370  TCGA-B6-A0WZ.B5B7CCDD-1ED2-437A-BDF0-684AF474A8A6  1          T2   \n",
       " 371  TCGA-B6-A0X1.D792031E-2CCE-4341-B3B3-C7D1D84F8F6B  1          T2   \n",
       " 373  TCGA-B6-A0X5.29811BAC-AFD1-4951-BDF8-7B859941B634  1          T2   \n",
       " 374  TCGA-B6-A1KC.E1FDF8EB-9D75-4629-9C78-6019DAF8190E  1          T2   \n",
       " 409  TCGA-BH-A0BZ.1B56E5A6-35B8-4C60-8BC5-68716C946799  2          T3   \n",
       " 428  TCGA-BH-A0E2.29EDE507-90BA-4ABE-B3EF-559EBA4CCD56  1          T2   \n",
       " 436  TCGA-BH-A0H7.CC7D1177-1638-4A80-A3AA-266ADC260EC9  0          T1   \n",
       " 452  TCGA-BH-A18N.60E8A9CF-3E2D-4E36-947D-448BA27C6561  1          T2   \n",
       " 469  TCGA-BH-A1F6.7E9B0BE7-121E-423F-AA37-445BA922FC13  3          T4   \n",
       " 471  TCGA-BH-A1FB.C5D116FF-3193-48A0-B69C-D98831D9976A  1          T2   \n",
       " 484  TCGA-BH-A204.531B1E8E-9936-42E0-B49B-39C61DD4E943  1          T2   \n",
       " 486  TCGA-BH-A209.E28150CE-E9B6-4C3F-B867-F9A75E2648DC  0          T1   \n",
       " 513  TCGA-D8-A1JH.49C79E09-2F69-4089-BFB1-53197DA09A46  0          T1   \n",
       " 521  TCGA-D8-A1JS.0EA57ABF-E3DA-4862-BAB8-A6E36408AC42  0          T1   \n",
       " 526  TCGA-D8-A1X8.5F1C3B85-5E5C-428B-8779-3A5878DCE1F2  0          T1   \n",
       " 530  TCGA-D8-A1XC.E5D2E429-C41C-47E2-96CB-F3B6F412499B  3          T4   \n",
       " 533  TCGA-D8-A1XG.AA869507-1B46-4812-8EC5-A4E01C588E6E  3          T4   \n",
       " 542  TCGA-D8-A1XT.99050E48-8DFE-4405-B8D8-7649CE2C9CFF  0          T1   \n",
       " 545  TCGA-D8-A1XW.BD71F606-58BC-46B9-8A03-CB25D5BECA96  1          T2   \n",
       " 575  TCGA-E2-A10C.E8A61AAC-BFF5-4341-B051-EDD87515ECDC  1          T2   \n",
       " 636  TCGA-E2-A2P6.E6A21CE6-841D-4D9B-A563-80B9BAF6E682  1          T2   \n",
       " 655  TCGA-E9-A1NG.73953A70-DAC0-443A-AC04-28BDA8D5F673  1          T2   \n",
       " 685  TCGA-E9-A245.5F2A52E5-2844-4242-8CE4-A87DF9D9C97B  1          T2   \n",
       " 706  TCGA-EW-A1P1.AE0C51EF-017E-4F53-968C-804110F9A7E6  1          T2   \n",
       " 723  TCGA-EW-A424.EE644ADF-639E-450A-92E6-2A6F1BAD561F  2          T3   \n",
       " 750  TCGA-LL-A5YP.B6B4DCFE-F044-44D6-BB90-EF71E6981E82  1          T2   \n",
       " 794  TCGA-UL-AAZ6.1AD1E3C4-0D7C-447B-B330-0245F9159FCC  1          T2   \n",
       " \n",
       "                                   ltm_rag1_t_reasoning  \\\n",
       " 0    To determine the pathologic T stage of the pat...   \n",
       " 17   To determine the pathologic T stage of the pat...   \n",
       " 36   [Step 1: Identify the size of the tumor] The r...   \n",
       " 39   Based on the pathology report, the patient's b...   \n",
       " 43   [{\"step\":1,\"info\":\"From the pathology report, ...   \n",
       " 46   Based on the pathology report, the patient has...   \n",
       " 48   [{\"step\":1,\"description\":\"Examining the pathol...   \n",
       " 60   To determine the pathologic T stage of the pat...   \n",
       " 85   Based on the pathology report, the patient's b...   \n",
       " 133  The pathology report indicates that the patien...   \n",
       " 142  Based on the pathology report, the patient's b...   \n",
       " 190  The report mentions that the patient is female...   \n",
       " 199  To determine the pathologic T stage of the pat...   \n",
       " 200  [{\"step\":1,\"info\":\"The report mentions that th...   \n",
       " 217  The pathology report provides the tumor size a...   \n",
       " 222  The pathology report provides the T stage as '...   \n",
       " 236  The pathology report provides the tumor size a...   \n",
       " 241  The pathology report provides the tumor size a...   \n",
       " 245  The pathology report does not provide informat...   \n",
       " 246  The pathology report provides the tumor size a...   \n",
       " 247  The pathology report provides the tumor size a...   \n",
       " 258  [{\"step\":1,\"info\":\"The report states that the ...   \n",
       " 266  To determine the pathologic T stage of the pat...   \n",
       " 275  [{\"step\":1,\"description\":\"Examining the report...   \n",
       " 346  Step-by-step interpretation of the report acco...   \n",
       " 350  [Step 1: Identify the size of the tumor] The r...   \n",
       " 351  The report states that the size of the invasiv...   \n",
       " 362  To determine the pathologic T stage of the pat...   \n",
       " 370  [{\"step\":1,\"description\":\"Examining the report...   \n",
       " 371  The report states that the 'LEFT BREAST BIOPSY...   \n",
       " 373  [The largest focus of invasive carcinoma in th...   \n",
       " 374  To determine the pathologic T stage of the pat...   \n",
       " 409  To determine the pathologic T stage of the pat...   \n",
       " 428  [Step 1: Identify the size of the invasive tum...   \n",
       " 436  [{\"step\":1,\"info\":\"The report states that the ...   \n",
       " 452  The report mentions that the size of the invas...   \n",
       " 469  [{\"step\":1,\"description\":\"Reviewing the report...   \n",
       " 471  To determine the pathologic T stage of the pat...   \n",
       " 484  [{\"step\":1,\"description\":\"Examining the report...   \n",
       " 486  The pathology report indicates that the patien...   \n",
       " 513  The report states that the 'tumour sized 1,2 x...   \n",
       " 521  Based on the pathology report, the patient has...   \n",
       " 526  The report mentions the size of the tumor as 2...   \n",
       " 530  The report mentions the size of the tumor as 7...   \n",
       " 533  The report states that the size of the tumor i...   \n",
       " 542  Based on the pathology report, we can determin...   \n",
       " 545  Based on the pathology report, the patient has...   \n",
       " 575  The report mentions that the 'Gross Exam F Bre...   \n",
       " 636  [{\"step\":1,\"info\":\"The report states that the ...   \n",
       " 655  Based on the pathology report provided, we can...   \n",
       " 685  Based on the pathology report, the tumor size ...   \n",
       " 706  The pathology report states that the 'Size of ...   \n",
       " 723  To determine the pathologic T stage of the pat...   \n",
       " 750  The report states that the left breast carcino...   \n",
       " 794                                                 [{   \n",
       " \n",
       "                                       ltm_rag1_t_stage  is_error  \n",
       " 0                                                   T1      True  \n",
       " 17                                                  T2      True  \n",
       " 36                                                 T1c      True  \n",
       " 39                                                  T3      True  \n",
       " 43                                                  T1      True  \n",
       " 46                                                  T1      True  \n",
       " 48                                                  T4      True  \n",
       " 60                                                  T2      True  \n",
       " 85                                                 T1c      True  \n",
       " 133                                                T1c      True  \n",
       " 142                                                 T3      True  \n",
       " 190                                                 T3      True  \n",
       " 199                                                 T2      True  \n",
       " 200                                                T1c      True  \n",
       " 217                                                 T1      True  \n",
       " 222  To determine the T stage, we will follow the r...      True  \n",
       " 236                                                 T1      True  \n",
       " 241                                                 T1      True  \n",
       " 245                                            unknown      True  \n",
       " 246                                                 T1      True  \n",
       " 247                                                 T1      True  \n",
       " 258                                                 T2      True  \n",
       " 266                                                 T1      True  \n",
       " 275                                                T1c      True  \n",
       " 346                                                 T2      True  \n",
       " 350                                                 T4      True  \n",
       " 351                                                T1c      True  \n",
       " 362                                                 T1      True  \n",
       " 370                                                T1c      True  \n",
       " 371                                                T1c      True  \n",
       " 373                                                T1c      True  \n",
       " 374                                                T1b      True  \n",
       " 409                                                 T4      True  \n",
       " 428                                                T1b      True  \n",
       " 436                                                 T2      True  \n",
       " 452                                                 T3      True  \n",
       " 469                                                 T2      True  \n",
       " 471                                                T1c      True  \n",
       " 484                                                 T3      True  \n",
       " 486                                                 T2      True  \n",
       " 513  Unable to determine T stage based on provided ...      True  \n",
       " 521  Cannot determine T stage without tumor size in...      True  \n",
       " 526                                                 T2      True  \n",
       " 530                                                 T3      True  \n",
       " 533                                                 T3      True  \n",
       " 542                                                 T2      True  \n",
       " 545                                                T1c      True  \n",
       " 575  Unable to determine the T stage based on the p...      True  \n",
       " 636                                                 T3      True  \n",
       " 655                                                 T1      True  \n",
       " 685                                                 T1      True  \n",
       " 706                                                T1c      True  \n",
       " 723                                                 T2      True  \n",
       " 750                                                T1c      True  \n",
       " 794                                                 T1      True  )"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ------------------------------------------------------------------ #\n",
    "# 1)  ZSCOT  vs  KEwLTM                                              #\n",
    "# ------------------------------------------------------------------ #\n",
    "base_df  = pd.read_csv(\n",
    "    \"/home/yl3427/cylab/selfCorrectionAgent/mixtral_rag_result/0929_ltm_rag2.csv\"\n",
    ")\n",
    "ltm_df   = pd.read_csv(\n",
    "    \"/home/yl3427/cylab/selfCorrectionAgent/result/0718_t14_dynamic_test_0_outof_10runs.csv\"\n",
    ")\n",
    "\n",
    "print(\"\\n--- Comparing ZSCOT vs. KEwLTM (T‑stage) ---\")\n",
    "df_zscot_t = base_df[[\n",
    "    \"patient_filename\", \"t\", \"text\",\n",
    "    \"zscot_t_stage\", \"zscot_t_reasoning\"\n",
    "]]\n",
    "df_zscot_t = df_zscot_t[df_zscot_t[\"patient_filename\"].isin(ltm_df[\"patient_filename\"])]\n",
    "df_kewltm_t = ltm_df[[\n",
    "    \"patient_filename\", \"t\", \"text\",\n",
    "    \"cmem_t_40reports_ans_str\", \"cmem_t_40reasoning\"\n",
    "]]\n",
    "\n",
    "compare_error_cases(\n",
    "    df_baseline       = df_zscot_t,\n",
    "    df_method         = df_kewltm_t,\n",
    "    stage             = \"t\",\n",
    "    true_col          = \"t\",\n",
    "    base_pred_col     = \"zscot_t_stage\",\n",
    "    base_reason_col   = \"zscot_t_reasoning\",\n",
    "    meth_pred_col     = \"cmem_t_40reports_ans_str\",\n",
    "    meth_reason_col   = \"cmem_t_40reasoning\",\n",
    "    out_common_csv    = \"t_common_errors_zscot_kewltm.csv\",\n",
    "    out_base_only_csv = \"t_zscot_only_errors_vs_kewltm.csv\",\n",
    "    out_meth_only_csv = \"t_kewltm_only_errors_vs_zscot.csv\",\n",
    ")\n",
    "\n",
    "# ------------------------------------------------------------------ #\n",
    "# 2)  RAG  vs  KEwRAG                                                #\n",
    "# ------------------------------------------------------------------ #\n",
    "print(\"\\n--- Comparing RAG vs. KEwRAG (T‑stage) ---\")\n",
    "df_rag_t = base_df[[\n",
    "    \"patient_filename\", \"t\", \"text\",\n",
    "    \"rag_raw_t_stage\", \"rag_raw_t_reasoning\"\n",
    "]]\n",
    "\n",
    "df_kewrag_t = base_df[[\n",
    "    \"patient_filename\", \"t\", \"text\",\n",
    "    \"ltm_rag1_t_stage\", \"ltm_rag1_t_reasoning\"\n",
    "]]\n",
    "\n",
    "compare_error_cases(\n",
    "    df_baseline       = df_rag_t,\n",
    "    df_method         = df_kewrag_t,\n",
    "    stage             = \"t\",\n",
    "    true_col          = \"t\",\n",
    "    base_pred_col     = \"rag_raw_t_stage\",\n",
    "    base_reason_col   = \"rag_raw_t_reasoning\",\n",
    "    meth_pred_col     = \"ltm_rag1_t_stage\",\n",
    "    meth_reason_col   = \"ltm_rag1_t_reasoning\",\n",
    "    out_common_csv    = \"t_common_errors_rag_kewrag.csv\",\n",
    "    out_base_only_csv = \"t_rag_only_errors_vs_kewrag.csv\",\n",
    "    out_meth_only_csv = \"t_kewrag_only_errors_vs_rag.csv\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# from typing import List, Tuple\n",
    "\n",
    "# def _error_mask(\n",
    "#         df: pd.DataFrame,\n",
    "#         stage_prefix: str,\n",
    "#         true_col: str,\n",
    "#         pred_col: str,\n",
    "#         labels: List[int] | None,\n",
    "#     ) -> pd.Series:\n",
    "\n",
    "#     def canon(gt: int) -> str:\n",
    "#         # Correctly formats ground truth labels (e.g., T1, N0)\n",
    "#         return f\"{stage_prefix}{int(gt)+1}\" if stage_prefix == \"T\" else f\"{stage_prefix}{int(gt)}\"\n",
    "\n",
    "#     # Normalise predictions (e.g., \"NO\" to \"N0\")\n",
    "#     def norm_pred(p: str) -> str:\n",
    "#         p_str = str(p).upper() # Ensure p is string before upper()\n",
    "#         if stage_prefix == \"N\":\n",
    "#             p_str = p_str.replace(\"NO\", \"N0\").replace(\"NL\", \"N1\") # Common variations\n",
    "#         return p_str\n",
    "\n",
    "#     df_tmp = df.copy()\n",
    "#     # Ensure true labels are integers before applying canon\n",
    "#     try:\n",
    "#         df_tmp[\"__gt_int\"] = df_tmp[true_col].astype(int)\n",
    "#     except ValueError:\n",
    "#         # Handle cases where true_col might already be like \"T1\", \"N0\" if not careful\n",
    "#         # This function expects integer true_col that it will convert to T1/N0 format.\n",
    "#         # For robustness, one might add more sophisticated parsing if true_col format varies.\n",
    "#         raise ValueError(f\"Column '{true_col}' must contain integer representations of stages for _error_mask.\")\n",
    "\n",
    "#     df_tmp[\"__gt\"]   = df_tmp[\"__gt_int\"].apply(canon)\n",
    "#     df_tmp[\"__pred\"] = df_tmp[pred_col].astype(str).apply(norm_pred) # ensure pred_col is str\n",
    "\n",
    "#     # Determine the full set of possible canonical tags\n",
    "#     if labels is None:\n",
    "#         # Use the integer labels from __gt_int to form all_tags\n",
    "#         unique_int_labels = sorted(df_tmp[\"__gt_int\"].unique())\n",
    "#     else:\n",
    "#         unique_int_labels = sorted(list(set(labels))) # Use provided labels if available\n",
    "    \n",
    "#     all_tags = {canon(i) for i in unique_int_labels}\n",
    "\n",
    "\n",
    "#     # Row‑wise error test\n",
    "#     def row_is_error(row) -> bool:\n",
    "#         pred_text = row[\"__pred\"]\n",
    "#         gt_text   = row[\"__gt\"]\n",
    "\n",
    "#         # An error if:\n",
    "#         # 1. The ground truth tag is NOT in the prediction string (False Negative component)\n",
    "#         # OR\n",
    "#         # 2. Any OTHER tag (not the ground truth) IS in the prediction string (False Positive component)\n",
    "#         contains_gt    = gt_text in pred_text\n",
    "#         # Check if any tag from all_tags is in pred_text, AND that tag is not the gt_text\n",
    "#         contains_other = any(tag in pred_text and tag != gt_text for tag in all_tags)\n",
    "        \n",
    "#         is_error = (not contains_gt) or contains_other\n",
    "#         return is_error\n",
    "\n",
    "#     return df_tmp.apply(row_is_error, axis=1)\n",
    "\n",
    "\n",
    "# def compare_error_cases(\n",
    "#         df_baseline: pd.DataFrame,\n",
    "#         df_method:   pd.DataFrame,\n",
    "#         *,\n",
    "#         stage: str,                           # \"t\" or \"n\"\n",
    "#         id_col: str           = \"patient_filename\",\n",
    "#         true_col: str         = \"t\",          # or \"n\" (ensure this matches the stage and contains integer labels)\n",
    "#         base_pred_col: str    = \"prediction\",\n",
    "#         base_reason_col: str  = \"reasoning\",\n",
    "#         meth_pred_col: str    = \"prediction\",\n",
    "#         meth_reason_col: str  = \"reasoning\",\n",
    "#         text_col: str         = \"text\",\n",
    "#         labels: List[int] | None = None, # Integer labels for the stage\n",
    "#         out_common_csv: str = \"common_errors.csv\", \n",
    "#         out_base_only_csv: str = \"baseline_only_errors.csv\",\n",
    "#         out_meth_only_csv: str = \"method_only_errors.csv\",\n",
    "#     ) -> Tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame]: \n",
    "\n",
    "#     stage_prefix = \"T\" if stage.lower().startswith(\"t\") else \"N\"\n",
    "\n",
    "#     df_baseline[id_col] = df_baseline[id_col].astype(str)\n",
    "#     df_method[id_col] = df_method[id_col].astype(str)\n",
    "    \n",
    "#     common_ids = set(df_baseline[id_col]) & set(df_method[id_col])\n",
    "    \n",
    "#     df_b = df_baseline[df_baseline[id_col].isin(common_ids)].sort_values(by=id_col).reset_index(drop=True)\n",
    "#     df_m = df_method[df_method[id_col].isin(common_ids)].sort_values(by=id_col).reset_index(drop=True)\n",
    "\n",
    "#     mask_b = _error_mask(df_b, stage_prefix, true_col, base_pred_col, labels)\n",
    "#     mask_m = _error_mask(df_m, stage_prefix, true_col, meth_pred_col, labels)\n",
    "\n",
    "#     common_error_ids = set(df_b.loc[mask_b & mask_m, id_col])\n",
    "#     base_only_error_ids  = set(df_b.loc[mask_b & ~mask_m, id_col])\n",
    "#     meth_only_error_ids  = set(df_m.loc[mask_m & ~mask_b, id_col])\n",
    "\n",
    "#     output_cols_base = [id_col, true_col, base_pred_col, base_reason_col, text_col]\n",
    "#     output_cols_meth = [id_col, true_col, meth_pred_col, meth_reason_col, text_col]\n",
    "    \n",
    "#     common_errors_df_b = df_b[df_b[id_col].isin(common_error_ids)][output_cols_base].rename(\n",
    "#         columns={base_pred_col: \"baseline_pred\", base_reason_col: \"baseline_reason\", true_col: f\"{true_col}_true\"}\n",
    "#     )\n",
    "#     common_errors_df_m = df_m[df_m[id_col].isin(common_error_ids)][[id_col, meth_pred_col, meth_reason_col]].rename(\n",
    "#         columns={meth_pred_col: \"method_pred\", meth_reason_col: \"method_reason\"}\n",
    "#     )\n",
    "#     common_errors_df = pd.merge(common_errors_df_b, common_errors_df_m, on=id_col, how=\"left\")\n",
    "#     # Ensure original true_col name is preserved if it was renamed\n",
    "#     if f\"{true_col}_true\" in common_errors_df.columns:\n",
    "#          common_errors_df.rename(columns={f\"{true_col}_true\": true_col}, inplace=True)\n",
    "\n",
    "#     # Reorder columns for clarity\n",
    "#     common_errors_df = common_errors_df[[\n",
    "#         id_col, true_col, \n",
    "#         \"baseline_pred\", \"baseline_reason\", \n",
    "#         \"method_pred\", \"method_reason\", \n",
    "#         text_col\n",
    "#     ]]\n",
    "\n",
    "#     baseline_only_errors_df = (\n",
    "#         df_b[df_b[id_col].isin(base_only_error_ids)]\n",
    "#         [output_cols_base]\n",
    "#         .rename(columns={\n",
    "#             base_pred_col:  \"baseline_pred\",\n",
    "#             base_reason_col:\"baseline_reason\"})\n",
    "#         .reset_index(drop=True)\n",
    "#     )\n",
    "\n",
    "#     method_only_errors_df = (\n",
    "#         df_m[df_m[id_col].isin(meth_only_error_ids)]\n",
    "#         [output_cols_meth]\n",
    "#         .rename(columns={\n",
    "#             meth_pred_col:  \"method_pred\",\n",
    "#             meth_reason_col:\"method_reason\"})\n",
    "#         .reset_index(drop=True)\n",
    "#     )\n",
    "\n",
    "#     common_errors_df.to_csv(out_common_csv, index=False)\n",
    "#     baseline_only_errors_df.to_csv(out_base_only_csv, index=False)\n",
    "#     method_only_errors_df.to_csv(out_meth_only_csv, index=False)\n",
    "    \n",
    "#     print(f\"Saved {len(common_errors_df)} common errors between baseline and method                 → {out_common_csv}\")\n",
    "#     print(f\"Saved {len(baseline_only_errors_df)} errors unique to baseline ({base_pred_col})        → {out_base_only_csv}\")\n",
    "#     print(f\"Saved {len(method_only_errors_df)} errors unique to method ({meth_pred_col})          → {out_meth_only_csv}\")\n",
    "\n",
    "#     return common_errors_df, baseline_only_errors_df, method_only_errors_df\n",
    "\n",
    "# # --- Example Usage (Illustrative - replace with your actual DataFrames and filenames) ---\n",
    "# if __name__ == '__main__':\n",
    "#     base_df  = pd.read_csv('/home/yl3427/cylab/selfCorrectionAgent/mixtral_rag_result/0929_ltm_rag2.csv')\n",
    "#     ltm_df = pd.read_csv(f\"/home/yl3427/cylab/selfCorrectionAgent/result/0718_t14_dynamic_test_0_outof_10runs.csv\")\n",
    "\n",
    "#     print(\"\\n--- Comparing ZSCOT vs. KEwLTM (T-stage) ---\")\n",
    "#     df_zscot_t = base_df[[\n",
    "#         \"patient_filename\", \"t\", \"text\", \n",
    "#         \"zscot_t_stage\", \"zscot_t_reasoning\"\n",
    "#     ]].copy()\n",
    "#     df_kewltm_t = ltm_df[[\n",
    "#         \"patient_filename\", \"t\", \"text\",\n",
    "#         \"cmem_t_40reports_ans_str\", \"cmem_t_40reasoning\"\n",
    "#     ]].copy()\n",
    "\n",
    "#     common_z_k_t, z_only_t, k_only_t = compare_error_cases(\n",
    "#         df_baseline=df_zscot_t,\n",
    "#         df_method=df_kewltm_t,\n",
    "#         stage=\"t\",\n",
    "#         true_col=\"t\", \n",
    "#         base_pred_col=\"zscot_t_stage\",\n",
    "#         base_reason_col=\"zscot_t_reasoning\",\n",
    "#         meth_pred_col=\"cmem_t_40reports_ans_str\",\n",
    "#         meth_reason_col=\"cmem_t_40reasoning\",\n",
    "#         out_common_csv=\"t_common_errors_zscot_kewltm.csv\",\n",
    "#         out_base_only_csv=\"t_zscot_only_errors_vs_kewltm.csv\",\n",
    "#         out_meth_only_csv=\"t_kewltm_only_errors_vs_zscot.csv\"\n",
    "#     )\n",
    "    \n",
    "#     # --- Error Comparison: Pair 2: RAG vs KEwRAG (T-stage) ---\n",
    "#     print(\"\\n--- Comparing RAG vs. KEwRAG (T-stage) ---\")\n",
    "#     df_rag_t = base_df[[\n",
    "#         \"patient_filename\", \"t\", \"text\",\n",
    "#         \"rag_raw_t_stage\", \"rag_raw_t_reasoning\"\n",
    "#     ]].copy()\n",
    "#     df_kewrag_t = base_df[[\n",
    "#         \"patient_filename\", \"t\", \"text\",\n",
    "#         \"ltm_rag1_t_stage\", \"ltm_rag1_t_reasoning\"\n",
    "#     ]].copy()\n",
    "    \n",
    "#     common_r_kr_t, r_only_t, kr_only_t = compare_error_cases(\n",
    "#         df_baseline=df_rag_t,\n",
    "#         df_method=df_kewrag_t,\n",
    "#         stage=\"t\",\n",
    "#         true_col=\"t\",\n",
    "#         base_pred_col=\"rag_raw_t_stage\",\n",
    "#         base_reason_col=\"rag_raw_t_reasoning\",\n",
    "#         meth_pred_col=\"ltm_rag1_t_stage\",\n",
    "#         meth_reason_col=\"ltm_rag1_t_reasoning\",\n",
    "#         labels=t_stage_int_labels, # Pass integer labels\n",
    "#         out_common_csv=\"t_common_errors_rag_kewrag.csv\",\n",
    "#         out_base_only_csv=\"t_rag_only_errors_vs_kewrag.csv\",\n",
    "#         out_meth_only_csv=\"t_kewrag_only_errors_vs_rag.csv\"\n",
    "#     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8735061c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd2f4e6f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbe0ff31",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d5fc7f1e",
   "metadata": {},
   "source": [
    "# Plot (sensitive)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e767109",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "\n",
    "x_axis = np.array(range(1, 11)) * 10\n",
    "\n",
    "memory_precision_cumulative = []\n",
    "memory_recall_cumulative = []\n",
    "memory_f1_cumulative = []\n",
    "\n",
    "\n",
    "for run in [0, 1, 2, 3, 4, 5, 6, 8]:\n",
    "    test_df = pd.read_csv(f\"/home/yl3427/cylab/selfCorrectionAgent/result/0718_t14_dynamic_test_{run}_outof_10runs.csv\")\n",
    "\n",
    "    for i in np.array(range(1, 11)): # memory (10, 20, 30, 40, 50, 60, 70, 80, 90, 100)\n",
    "        result = t14_calculate_metrics(test_df['t'], test_df[f'cmem_t_{i*10}reports_ans_str'])['overall']\n",
    "        if run == 0:\n",
    "            memory_precision_cumulative.append(result['macro_precision'])\n",
    "            memory_recall_cumulative.append(result['macro_recall'])\n",
    "            memory_f1_cumulative.append(result['macro_f1'])\n",
    "        else:\n",
    "            memory_precision_cumulative[i-1] += result['macro_precision']\n",
    "            memory_recall_cumulative[i-1] += result['macro_recall']\n",
    "            memory_f1_cumulative[i-1] += result['macro_f1']\n",
    "\n",
    "\n",
    "\n",
    "# average\n",
    "precision_avg = [p / 8 for p in memory_precision_cumulative]\n",
    "recall_avg = [r / 8 for r in memory_recall_cumulative]\n",
    "f1_avg = [f / 8  for f in memory_f1_cumulative]\n",
    "\n",
    "\n",
    "plt.figure(figsize=(15, 10))\n",
    "\n",
    "plt.plot(x_axis, precision_avg, label='Average KEwLTM Precision', color='blue', marker='o')\n",
    "plt.plot(x_axis, recall_avg, label='Average KEwLTM Recall', color='green', marker='o')\n",
    "plt.plot(x_axis, f1_avg, label='Average KEwLTM F1 Score', color='red', marker='o')\n",
    "\n",
    "plt.xlabel('Number of Training Reports')\n",
    "plt.ylabel('Scores')\n",
    "# plt.title(f'The Average of 10 Results on 700 Test Reports (t14)')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ad42fa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "\n",
    "x_axis = np.array(range(1, 11)) * 10\n",
    "\n",
    "memory_precision_cumulative = []\n",
    "memory_recall_cumulative = []\n",
    "memory_f1_cumulative = []\n",
    "\n",
    "\n",
    "for run in [0, 1, 3, 4, 5, 6, 7, 9]:\n",
    "    test_df = pd.read_csv(f\"/home/yl3427/cylab/selfCorrectionAgent/result/0718_n03_dynamic_test_{run}_outof_10runs.csv\")\n",
    "\n",
    "    for i in np.array(range(1, 11)): # memory (10, 20, 30, 40, 50, 60, 70, 80, 90, 100)\n",
    "        result = n03_calculate_metrics(test_df['n'], test_df[f'cmem_n_{i*10}reports_ans_str'])['overall']\n",
    "        if run == 0:\n",
    "            memory_precision_cumulative.append(result['macro_precision'])\n",
    "            memory_recall_cumulative.append(result['macro_recall'])\n",
    "            memory_f1_cumulative.append(result['macro_f1'])\n",
    "        else:\n",
    "            memory_precision_cumulative[i-1] += result['macro_precision']\n",
    "            memory_recall_cumulative[i-1] += result['macro_recall']\n",
    "            memory_f1_cumulative[i-1] += result['macro_f1']\n",
    "\n",
    "\n",
    "\n",
    "# average\n",
    "precision_avg = [p / 8 for p in memory_precision_cumulative]\n",
    "recall_avg = [r / 8 for r in memory_recall_cumulative]\n",
    "f1_avg = [f / 8  for f in memory_f1_cumulative]\n",
    "\n",
    "\n",
    "plt.figure(figsize=(15, 10))\n",
    "\n",
    "plt.plot(x_axis, precision_avg, label='Average KEwLTM Precision', color='blue', marker='o')\n",
    "plt.plot(x_axis, recall_avg, label='Average KEwLTM Recall', color='green', marker='o')\n",
    "plt.plot(x_axis, f1_avg, label='Average KEwLTM F1 Score', color='red', marker='o')\n",
    "\n",
    "plt.xlabel('Number of Training Reports')\n",
    "plt.ylabel('Scores')\n",
    "# plt.title(f'The Average of 10 Results on 700 Test Reports (t14)')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
