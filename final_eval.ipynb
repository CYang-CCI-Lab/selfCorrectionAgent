{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "13d8bd98",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0b28967b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def t14_calculate_metrics(true_labels: pd.Series, predictions: pd.Series) -> dict:\n",
    "    \"\"\"\n",
    "    Calculates precision, recall, F1-score, and support for T-stage predictions.\n",
    "    Includes both per-label, micro-average, and macro-average scores.\n",
    "\n",
    "    Args:\n",
    "        true_labels: A pandas Series of true labels (e.g., integers from 0 to N-1).\n",
    "        predictions: A pandas Series of predicted labels (strings).\n",
    "\n",
    "    Returns:\n",
    "        A dictionary containing the metrics for each label and overall scores.\n",
    "    \"\"\"\n",
    "    # Check for valid inputs\n",
    "    if len(true_labels) != len(predictions):\n",
    "        raise ValueError(\"The length of true_labels and predictions must be the same.\")\n",
    "\n",
    "    if not isinstance(true_labels, pd.Series) or not isinstance(predictions, pd.Series):\n",
    "        raise TypeError(\"true_labels and predictions must be pandas Series.\")\n",
    "\n",
    "    if any(pd.isna(pred) or not isinstance(pred, str) for pred in predictions):\n",
    "        raise ValueError(\"All predictions must be non-null strings.\")\n",
    "\n",
    "    # Standardize true labels to \"T{x+1}\" format\n",
    "    true_labels = true_labels.apply(lambda x: f\"T{int(x)+1}\")\n",
    "\n",
    "    metrics = {}\n",
    "    label_counts = {}\n",
    "    unique_true_labels = sorted(list(set(true_labels))) # Ensure consistent order\n",
    "\n",
    "    for label in unique_true_labels:\n",
    "        metrics[label] = {\"tp\": 0, \"fp\": 0, \"fn\": 0}\n",
    "        label_counts[label] = 0\n",
    "\n",
    "    for true_label, prediction in zip(true_labels, predictions):\n",
    "        # Ensure prediction is a string and convert to uppercase\n",
    "        prediction_str = str(prediction).upper()\n",
    "        \n",
    "        label_counts[true_label] += 1\n",
    "        if true_label in prediction_str:\n",
    "            metrics[true_label][\"tp\"] += 1\n",
    "        else:\n",
    "            metrics[true_label][\"fn\"] += 1\n",
    "\n",
    "        # Calculate false positives\n",
    "        # A prediction is a false positive for a label if:\n",
    "        # 1. The label is present in the prediction string.\n",
    "        # 2. The label is NOT the true_label.\n",
    "        for label_to_check_fp in unique_true_labels:\n",
    "            if label_to_check_fp in prediction_str and label_to_check_fp != true_label:\n",
    "                metrics[label_to_check_fp][\"fp\"] += 1\n",
    "    \n",
    "    results = {}\n",
    "    # Variables for micro-averaging\n",
    "    total_tp_micro = 0\n",
    "    total_fp_micro = 0\n",
    "    total_fn_micro = 0\n",
    "    \n",
    "    # Variables for macro-averaging\n",
    "    macro_precision_sum = 0.0\n",
    "    macro_recall_sum = 0.0\n",
    "    macro_f1_sum = 0.0\n",
    "    \n",
    "    total_instances = len(true_labels)\n",
    "\n",
    "    for label in unique_true_labels: # Iterate in a defined order\n",
    "        counts = metrics[label]\n",
    "        tp = counts[\"tp\"]\n",
    "        fp = counts[\"fp\"]\n",
    "        fn = counts[\"fn\"]\n",
    "\n",
    "        # Precision: TP / (TP + FP)\n",
    "        precision = tp / (tp + fp) if (tp + fp) > 0 else 0.0\n",
    "        # Recall: TP / (TP + FN)\n",
    "        recall = tp / (tp + fn) if (tp + fn) > 0 else 0.0\n",
    "        # F1-Score: 2 * (Precision * Recall) / (Precision + Recall)\n",
    "        f1 = (\n",
    "            2 * precision * recall / (precision + recall)\n",
    "            if (precision + recall) > 0\n",
    "            else 0.0\n",
    "        )\n",
    "        support = label_counts[label]\n",
    "\n",
    "        results[label] = {\n",
    "            \"precision\": round(precision, 3),\n",
    "            \"recall\": round(recall, 3),\n",
    "            \"f1\": round(f1, 3),\n",
    "            \"support\": support,\n",
    "            \"tp\": tp,\n",
    "            \"fp\": fp,\n",
    "            \"fn\": fn,\n",
    "            \"num_errors\": fp + fn, # Sum of false positives and false negatives for this class\n",
    "        }\n",
    "\n",
    "        # Accumulate for micro-averages\n",
    "        total_tp_micro += tp\n",
    "        total_fp_micro += fp\n",
    "        total_fn_micro += fn\n",
    "        \n",
    "        # Accumulate for macro-averages\n",
    "        macro_precision_sum += precision\n",
    "        macro_recall_sum += recall\n",
    "        macro_f1_sum += f1\n",
    "\n",
    "    # Calculate macro-averaged metrics\n",
    "    num_labels = len(unique_true_labels)\n",
    "    macro_precision = macro_precision_sum / num_labels if num_labels > 0 else 0.0\n",
    "    macro_recall = macro_recall_sum / num_labels if num_labels > 0 else 0.0\n",
    "    macro_f1 = macro_f1_sum / num_labels if num_labels > 0 else 0.0 # Often calculated as harmonic mean of macro_precision and macro_recall\n",
    "\n",
    "    # Calculate micro-averaged (overall) precision, recall, and F1 score\n",
    "    # Micro-Precision: Sum of all TPs / (Sum of all TPs + Sum of all FPs)\n",
    "    micro_precision = (\n",
    "        total_tp_micro / (total_tp_micro + total_fp_micro) if (total_tp_micro + total_fp_micro) > 0 else 0.0\n",
    "    )\n",
    "    # Micro-Recall: Sum of all TPs / (Sum of all TPs + Sum of all FNs)\n",
    "    micro_recall = (\n",
    "        total_tp_micro / (total_tp_micro + total_fn_micro) if (total_tp_micro + total_fn_micro) > 0 else 0.0\n",
    "    )\n",
    "    # Micro-F1: 2 * (Micro-Precision * Micro-Recall) / (Micro-Precision + Micro-Recall)\n",
    "    micro_f1 = (\n",
    "        2 * micro_precision * micro_recall / (micro_precision + micro_recall)\n",
    "        if (micro_precision + micro_recall) > 0\n",
    "        else 0.0\n",
    "    )\n",
    "\n",
    "    # Calculate weighted F1 score\n",
    "    weighted_f1_sum = 0.0\n",
    "    for label in unique_true_labels:\n",
    "        weighted_f1_sum += results[label][\"f1\"] * label_counts[label]\n",
    "    weighted_f1 = weighted_f1_sum / total_instances if total_instances > 0 else 0.0\n",
    "\n",
    "    results[\"overall\"] = {\n",
    "        \"micro_precision\": round(micro_precision, 3),\n",
    "        \"micro_recall\": round(micro_recall, 3),\n",
    "        \"micro_f1\": round(micro_f1, 3),\n",
    "        \"macro_precision\": round(macro_precision, 3),\n",
    "        \"macro_recall\": round(macro_recall, 3),\n",
    "        \"macro_f1\": round(macro_f1, 3),\n",
    "        \"weighted_f1\": round(weighted_f1, 3),\n",
    "        \"support\": total_instances,\n",
    "        \"total_tp\": total_tp_micro,\n",
    "        \"total_fp\": total_fp_micro,\n",
    "        \"total_fn\": total_fn_micro,\n",
    "        \"num_errors\": total_fp_micro + total_fn_micro, # Sum of all false positives and false negatives\n",
    "    }\n",
    "    return results\n",
    "\n",
    "\n",
    "def n03_calculate_metrics(true_labels: pd.Series, predictions: pd.Series) -> dict:\n",
    "    \"\"\"\n",
    "    Calculates precision, recall, F1-score, and support for N-stage predictions.\n",
    "    Includes both per-label, micro-average, and macro-average scores.\n",
    "    Handles specific label replacements: \"NO\" to \"N0\", \"NL\" to \"N1\".\n",
    "\n",
    "    Args:\n",
    "        true_labels: A pandas Series of true labels (e.g., integers from 0 to N-1).\n",
    "        predictions: A pandas Series of predicted labels (strings).\n",
    "\n",
    "    Returns:\n",
    "        A dictionary containing the metrics for each label and overall scores.\n",
    "    \"\"\"\n",
    "    # Check for valid inputs\n",
    "    if len(true_labels) != len(predictions):\n",
    "        raise ValueError(\"The length of true_labels and predictions must be the same.\")\n",
    "\n",
    "    if not isinstance(true_labels, pd.Series) or not isinstance(predictions, pd.Series):\n",
    "        raise TypeError(\"true_labels and predictions must be pandas Series.\")\n",
    "\n",
    "    if any(pd.isna(pred) or not isinstance(pred, str) for pred in predictions):\n",
    "        raise ValueError(\"All predictions must be non-null strings.\")\n",
    "\n",
    "    # Standardize true labels to \"N{x}\" format\n",
    "    true_labels = true_labels.apply(lambda x: f\"N{int(x)}\")\n",
    "\n",
    "    metrics = {}\n",
    "    label_counts = {}\n",
    "    unique_true_labels = sorted(list(set(true_labels))) # Ensure consistent order\n",
    "\n",
    "    for label in unique_true_labels:\n",
    "        metrics[label] = {\"tp\": 0, \"fp\": 0, \"fn\": 0}\n",
    "        label_counts[label] = 0\n",
    "\n",
    "    for true_label, prediction in zip(true_labels, predictions):\n",
    "        # Ensure prediction is a string, convert to uppercase, and apply replacements\n",
    "        prediction_str = str(prediction).upper()\n",
    "        prediction_str = prediction_str.replace(\"NO\", \"N0\").replace(\"NL\", \"N1\")\n",
    "        \n",
    "        label_counts[true_label] += 1\n",
    "        if true_label in prediction_str:\n",
    "            metrics[true_label][\"tp\"] += 1\n",
    "        else:\n",
    "            metrics[true_label][\"fn\"] += 1\n",
    "\n",
    "        # Calculate false positives\n",
    "        for label_to_check_fp in unique_true_labels:\n",
    "            if label_to_check_fp in prediction_str and label_to_check_fp != true_label:\n",
    "                metrics[label_to_check_fp][\"fp\"] += 1\n",
    "\n",
    "    results = {}\n",
    "    # Variables for micro-averaging\n",
    "    total_tp_micro = 0\n",
    "    total_fp_micro = 0\n",
    "    total_fn_micro = 0\n",
    "    \n",
    "    # Variables for macro-averaging\n",
    "    macro_precision_sum = 0.0\n",
    "    macro_recall_sum = 0.0\n",
    "    macro_f1_sum = 0.0\n",
    "\n",
    "    total_instances = len(true_labels)\n",
    "\n",
    "    for label in unique_true_labels: # Iterate in a defined order\n",
    "        counts = metrics[label]\n",
    "        tp = counts[\"tp\"]\n",
    "        fp = counts[\"fp\"]\n",
    "        fn = counts[\"fn\"]\n",
    "\n",
    "        precision = tp / (tp + fp) if (tp + fp) > 0 else 0.0\n",
    "        recall = tp / (tp + fn) if (tp + fn) > 0 else 0.0\n",
    "        f1 = (\n",
    "            2 * precision * recall / (precision + recall)\n",
    "            if (precision + recall) > 0\n",
    "            else 0.0\n",
    "        )\n",
    "        support = label_counts[label]\n",
    "\n",
    "        results[label] = {\n",
    "            \"precision\": round(precision, 3),\n",
    "            \"recall\": round(recall, 3),\n",
    "            \"f1\": round(f1, 3),\n",
    "            \"support\": support,\n",
    "            \"tp\": tp,\n",
    "            \"fp\": fp,\n",
    "            \"fn\": fn,\n",
    "            \"num_errors\": fp + fn,\n",
    "        }\n",
    "\n",
    "        total_tp_micro += tp\n",
    "        total_fp_micro += fp\n",
    "        total_fn_micro += fn\n",
    "\n",
    "        macro_precision_sum += precision\n",
    "        macro_recall_sum += recall\n",
    "        macro_f1_sum += f1\n",
    "\n",
    "    # Calculate macro-averaged metrics\n",
    "    num_labels = len(unique_true_labels)\n",
    "    macro_precision = macro_precision_sum / num_labels if num_labels > 0 else 0.0\n",
    "    macro_recall = macro_recall_sum / num_labels if num_labels > 0 else 0.0\n",
    "    macro_f1 = macro_f1_sum / num_labels if num_labels > 0 else 0.0\n",
    "\n",
    "    # Calculate micro-averaged (overall) precision, recall, and F1 score\n",
    "    micro_precision = (\n",
    "        total_tp_micro / (total_tp_micro + total_fp_micro) if (total_tp_micro + total_fp_micro) > 0 else 0.0\n",
    "    )\n",
    "    micro_recall = (\n",
    "        total_tp_micro / (total_tp_micro + total_fn_micro) if (total_tp_micro + total_fn_micro) > 0 else 0.0\n",
    "    )\n",
    "    micro_f1 = (\n",
    "        2 * micro_precision * micro_recall / (micro_precision + micro_recall)\n",
    "        if (micro_precision + micro_recall) > 0\n",
    "        else 0.0\n",
    "    )\n",
    "\n",
    "    # Calculate weighted F1 score\n",
    "    weighted_f1_sum = 0.0\n",
    "    for label in unique_true_labels:\n",
    "        weighted_f1_sum += results[label][\"f1\"] * label_counts[label]\n",
    "    weighted_f1 = weighted_f1_sum / total_instances if total_instances > 0 else 0.0\n",
    "    \n",
    "    results[\"overall\"] = {\n",
    "        \"micro_precision\": round(micro_precision, 3),\n",
    "        \"micro_recall\": round(micro_recall, 3),\n",
    "        \"micro_f1\": round(micro_f1, 3),\n",
    "        \"macro_precision\": round(macro_precision, 3),\n",
    "        \"macro_recall\": round(macro_recall, 3),\n",
    "        \"macro_f1\": round(macro_f1, 3),\n",
    "        \"weighted_f1\": round(weighted_f1, 3),\n",
    "        \"support\": total_instances,\n",
    "        \"total_tp\": total_tp_micro,\n",
    "        \"total_fp\": total_fp_micro,\n",
    "        \"total_fn\": total_fn_micro,\n",
    "        \"num_errors\": total_fp_micro + total_fn_micro,\n",
    "    }\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6c3871d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_mean_std(\n",
    "        results: list[dict],\n",
    "        cat: str | None,\n",
    "        level: str = \"label\"     # \"label\", \"micro\", or \"macro\"\n",
    "    ) -> dict:\n",
    "    \"\"\"\n",
    "    Compute mean ± std for a single class (\"label\" level) or for the\n",
    "    overall micro / macro aggregates produced by *t14_calculate_metrics*.\n",
    "\n",
    "    Args\n",
    "    ----\n",
    "    results : list of metrics dicts (output of t14_calculate_metrics)\n",
    "    cat     : class label (e.g. \"T1\") – ignored for micro/macro levels\n",
    "    level   : \"label\" | \"micro\" | \"macro\"\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    dict with keys:\n",
    "        mean_precision, mean_recall, mean_f1,\n",
    "        std_precision,  std_recall,  std_f1,\n",
    "        (plus sums and raw means used elsewhere)\n",
    "    \"\"\"\n",
    "    # ------------------------------------------------------------------ #\n",
    "    # Gather the three score lists                                       #\n",
    "    # ------------------------------------------------------------------ #\n",
    "    precision_list, recall_list, f1_list = [], [], []\n",
    "    support_list, num_errors_list = [], []\n",
    "\n",
    "    for res in results:\n",
    "        if level == \"label\":\n",
    "            src = res[cat]                                 # per‑class block\n",
    "        elif level == \"micro\":\n",
    "            src = {                                        # NEW ↓\n",
    "                \"precision\": res[\"overall\"][\"micro_precision\"],\n",
    "                \"recall\":    res[\"overall\"][\"micro_recall\"],\n",
    "                \"f1\":        res[\"overall\"][\"micro_f1\"],\n",
    "                \"support\":   res[\"overall\"][\"support\"],\n",
    "                \"num_errors\": res[\"overall\"][\"num_errors\"],\n",
    "            }\n",
    "        elif level == \"macro\":\n",
    "            src = {                                        # NEW ↓\n",
    "                \"precision\": res[\"overall\"][\"macro_precision\"],\n",
    "                \"recall\":    res[\"overall\"][\"macro_recall\"],\n",
    "                \"f1\":        res[\"overall\"][\"macro_f1\"],\n",
    "                \"support\":   res[\"overall\"][\"support\"],\n",
    "                \"num_errors\": res[\"overall\"][\"num_errors\"],\n",
    "            }\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown level: {level}\")\n",
    "\n",
    "        precision_list.append(src[\"precision\"])\n",
    "        recall_list.append(src[\"recall\"])\n",
    "        f1_list.append(src[\"f1\"])\n",
    "        support_list.append(src[\"support\"])\n",
    "        num_errors_list.append(src[\"num_errors\"])\n",
    "\n",
    "    # ------------------------------------------------------------------ #\n",
    "    # Mean / std                                                         #\n",
    "    # ------------------------------------------------------------------ #\n",
    "    mean_p = sum(precision_list) / len(precision_list)\n",
    "    mean_r = sum(recall_list)    / len(recall_list)\n",
    "    mean_f = sum(f1_list)        / len(f1_list)\n",
    "\n",
    "    std_p = (sum((x - mean_p) ** 2 for x in precision_list) / len(precision_list)) ** 0.5\n",
    "    std_r = (sum((x - mean_r) ** 2 for x in recall_list)    / len(recall_list))    ** 0.5\n",
    "    std_f = (sum((x - mean_f) ** 2 for x in f1_list)        / len(f1_list))        ** 0.5\n",
    "\n",
    "    return {\n",
    "        \"mean_precision\": round(mean_p, 3),\n",
    "        \"mean_recall\":    round(mean_r, 3),\n",
    "        \"mean_f1\":        round(mean_f, 3),\n",
    "        \"std_precision\":  round(std_p, 3),\n",
    "        \"std_recall\":     round(std_r, 3),\n",
    "        \"std_f1\":         round(std_f, 3),\n",
    "        \"sum_support\":    sum(support_list),\n",
    "        \"sum_num_errors\": sum(num_errors_list),\n",
    "        \"raw_mean_precision\": mean_p,   # keep raw for higher‑level macro\n",
    "        \"raw_mean_recall\":    mean_r,\n",
    "        \"raw_mean_f1\":        mean_f,\n",
    "    }\n",
    "\n",
    "\n",
    "###############################################################################\n",
    "# Updated helper: output_tabular_performance                                  #\n",
    "###############################################################################\n",
    "def output_tabular_performance(\n",
    "        results: list[dict],\n",
    "        categories: list[str] = (\"T1\", \"T2\", \"T3\", \"T4\"),\n",
    "        show_overall: bool = True         # NEW flag\n",
    "    ) -> None:\n",
    "    \"\"\"\n",
    "    Print mean ± std precision/recall/F1 for each class, followed by:\n",
    "      • category‑macro average (same as before)\n",
    "      • micro‑average (overall)\n",
    "      • macro‑average (overall)\n",
    "    \"\"\"\n",
    "    # ------------------------------------------------------------------ #\n",
    "    # Per‑class lines                                                    #\n",
    "    # ------------------------------------------------------------------ #\n",
    "    label_means_p, label_means_r, label_means_f = [], [], []\n",
    "\n",
    "    for cat in categories:\n",
    "        stats = calculate_mean_std(results, cat, level=\"label\")\n",
    "        print(f\"{cat:8s} \"\n",
    "              f\"{stats['mean_precision']:.3f}({stats['std_precision']:.3f}) \"\n",
    "              f\"{stats['mean_recall']:.3f}({stats['std_recall']:.3f}) \"\n",
    "              f\"{stats['mean_f1']:.3f}({stats['std_f1']:.3f})\")\n",
    "\n",
    "        label_means_p.append(stats[\"raw_mean_precision\"])\n",
    "        label_means_r.append(stats[\"raw_mean_recall\"])\n",
    "        label_means_f.append(stats[\"raw_mean_f1\"])\n",
    "\n",
    "    # ------------------------------------------------------------------ #\n",
    "    # Category‑macro (average of label means)                            #\n",
    "    # ------------------------------------------------------------------ #\n",
    "    print(f\"{'Cat‑Macro':8s} \"\n",
    "          f\"{sum(label_means_p)/len(label_means_p):.3f} \"\n",
    "          f\"{sum(label_means_r)/len(label_means_r):.3f} \"\n",
    "          f\"{sum(label_means_f)/len(label_means_f):.3f}\")\n",
    "\n",
    "    # ------------------------------------------------------------------ #\n",
    "    # Overall micro / macro                                              #\n",
    "    # ------------------------------------------------------------------ #\n",
    "    if show_overall:\n",
    "        micro = calculate_mean_std(results, None, level=\"micro\")\n",
    "        macro = calculate_mean_std(results, None, level=\"macro\")\n",
    "\n",
    "        print(f\"{'MicroAvg.':8s} \"\n",
    "              f\"{micro['mean_precision']:.3f}({micro['std_precision']:.3f}) \"\n",
    "              f\"{micro['mean_recall']:.3f}({micro['std_recall']:.3f}) \"\n",
    "              f\"{micro['mean_f1']:.3f}({micro['std_f1']:.3f})\")\n",
    "\n",
    "        print(f\"{'MacroAvg.':8s} \"\n",
    "              f\"{macro['mean_precision']:.3f}({macro['std_precision']:.3f}) \"\n",
    "              f\"{macro['mean_recall']:.3f}({macro['std_recall']:.3f}) \"\n",
    "              f\"{macro['mean_f1']:.3f}({macro['std_f1']:.3f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e1f60051",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mixtral T-stage metrics:\n",
      "ZSCOT T-stage:\n",
      "{'micro_precision': 0.85, 'micro_recall': 0.863, 'micro_f1': 0.856, 'macro_precision': 0.831, 'macro_recall': 0.765, 'macro_f1': 0.792, 'weighted_f1': 0.854, 'support': 800, 'total_tp': 690, 'total_fp': 122, 'total_fn': 110, 'num_errors': 232}\n",
      "\n",
      "RAG T-stage:\n",
      "{'micro_precision': 0.81, 'micro_recall': 0.815, 'micro_f1': 0.812, 'macro_precision': 0.771, 'macro_recall': 0.73, 'macro_f1': 0.743, 'weighted_f1': 0.812, 'support': 800, 'total_tp': 652, 'total_fp': 153, 'total_fn': 148, 'num_errors': 301}\n",
      "\n",
      "KEwLTM T-stage:\n",
      "T1       0.904(0.017) 0.812(0.040) 0.855(0.018)\n",
      "T2       0.882(0.022) 0.938(0.018) 0.909(0.005)\n",
      "T3       0.834(0.054) 0.810(0.058) 0.818(0.018)\n",
      "T4       0.807(0.082) 0.634(0.038) 0.707(0.029)\n",
      "Cat‑Macro 0.857 0.799 0.822\n",
      "MicroAvg. 0.876(0.006) 0.878(0.007) 0.877(0.007)\n",
      "MacroAvg. 0.857(0.022) 0.799(0.020) 0.822(0.010)\n",
      "\n",
      "KEwRAG T-stage:\n",
      "{'micro_precision': 0.853, 'micro_recall': 0.848, 'micro_f1': 0.85, 'macro_precision': 0.792, 'macro_recall': 0.728, 'macro_f1': 0.746, 'weighted_f1': 0.846, 'support': 800, 'total_tp': 678, 'total_fp': 117, 'total_fn': 122, 'num_errors': 239}\n"
     ]
    }
   ],
   "source": [
    "mixtral_df = pd.read_csv('/home/yl3427/cylab/selfCorrectionAgent/mixtral_rag_result/0929_ltm_rag2.csv') # 이거야. 여기에 mixtral결과 다있어\n",
    "\n",
    "print(\"Mixtral T-stage metrics:\")\n",
    "\n",
    "print(\"ZSCOT T-stage:\")\n",
    "print(t14_calculate_metrics(mixtral_df['t'], mixtral_df['zscot_t_stage'])['overall'])\n",
    "print()\n",
    "print(\"RAG T-stage:\")\n",
    "print(t14_calculate_metrics(mixtral_df['t'], mixtral_df['rag_raw_t_stage'])['overall'])\n",
    "print()\n",
    "kewltm_t_results = []\n",
    "t_label = 't'\n",
    "kewltm_t_stage = \"cmem_t_40reports_ans_str\"\n",
    "run_lst = [0, 1, 2, 3, 4, 5, 6, 8]\n",
    "\n",
    "for run in run_lst:\n",
    "    t_test_df = pd.read_csv(\n",
    "        f\"/home/yl3427/cylab/selfCorrectionAgent/result/0718_t14_dynamic_test_{run}_outof_10runs.csv\"\n",
    "    ).sort_values(by=\"patient_filename\")\n",
    "\n",
    "    kewltm_t_results.append(\n",
    "        t14_calculate_metrics(t_test_df[t_label], t_test_df[kewltm_t_stage])\n",
    "    )\n",
    "print(\"KEwLTM T-stage:\")\n",
    "output_tabular_performance(kewltm_t_results)\n",
    "\n",
    "print()\n",
    "print(\"KEwRAG T-stage:\")\n",
    "print(t14_calculate_metrics(mixtral_df['t'], mixtral_df['ltm_rag1_t_stage'])['overall'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "15173e88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mixtral N-stage metrics:\n",
      "ZSCOT N-stage:\n",
      "{'micro_precision': 0.874, 'micro_recall': 0.873, 'micro_f1': 0.873, 'macro_precision': 0.843, 'macro_recall': 0.822, 'macro_f1': 0.832, 'weighted_f1': 0.872, 'support': 800, 'total_tp': 698, 'total_fp': 101, 'total_fn': 102, 'num_errors': 203}\n",
      "\n",
      "RAG N-stage:\n",
      "{'micro_precision': 0.841, 'micro_recall': 0.835, 'micro_f1': 0.838, 'macro_precision': 0.803, 'macro_recall': 0.799, 'macro_f1': 0.797, 'weighted_f1': 0.84, 'support': 800, 'total_tp': 668, 'total_fp': 126, 'total_fn': 132, 'num_errors': 258}\n",
      "\n",
      "KEwLTM N-stage:\n",
      "N0       0.944(0.008) 0.952(0.018) 0.948(0.011)\n",
      "N1       0.885(0.020) 0.883(0.026) 0.884(0.010)\n",
      "N2       0.713(0.031) 0.745(0.054) 0.727(0.022)\n",
      "N3       0.886(0.058) 0.784(0.042) 0.830(0.017)\n",
      "Cat‑Macro 0.857 0.841 0.847\n",
      "MicroAvg. 0.883(0.007) 0.883(0.007) 0.883(0.007)\n",
      "MacroAvg. 0.857(0.011) 0.841(0.011) 0.847(0.008)\n",
      "\n",
      "KEwRAG N-stage:\n",
      "{'micro_precision': 0.853, 'micro_recall': 0.859, 'micro_f1': 0.856, 'macro_precision': 0.807, 'macro_recall': 0.814, 'macro_f1': 0.81, 'weighted_f1': 0.856, 'support': 800, 'total_tp': 687, 'total_fp': 118, 'total_fn': 113, 'num_errors': 231}\n"
     ]
    }
   ],
   "source": [
    "print(\"Mixtral N-stage metrics:\")\n",
    "\n",
    "print(\"ZSCOT N-stage:\")\n",
    "print(n03_calculate_metrics(mixtral_df['n'], mixtral_df['zscot_n_stage'])['overall'])\n",
    "print()\n",
    "print(\"RAG N-stage:\")\n",
    "print(n03_calculate_metrics(mixtral_df['n'], mixtral_df['rag_raw_n_stage'])['overall'])\n",
    "print()\n",
    "kewltm_n_results = []\n",
    "n_label = 'n'\n",
    "kewltm_n_stage = \"cmem_n_40reports_ans_str\"\n",
    "run_lst = [0, 1, 3, 4, 5, 6, 7, 9]\n",
    "for run in run_lst:\n",
    "    n_test_df = pd.read_csv(\n",
    "        f\"/home/yl3427/cylab/selfCorrectionAgent/result/0718_n03_dynamic_test_{run}_outof_10runs.csv\"\n",
    "    ).sort_values(by=\"patient_filename\")\n",
    "\n",
    "    kewltm_n_results.append(\n",
    "        n03_calculate_metrics(n_test_df[n_label], n_test_df[kewltm_n_stage])\n",
    "    )\n",
    "print(\"KEwLTM N-stage:\")\n",
    "output_tabular_performance(kewltm_n_results, categories=[\"N0\", \"N1\", \"N2\", \"N3\"])\n",
    "print()\n",
    "print(\"KEwRAG N-stage:\")\n",
    "print(n03_calculate_metrics(mixtral_df['n'], mixtral_df['ltm_rag1_n_stage'])['overall'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69268edf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5f7b164a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Med42 T-stage metrics:\n",
      "ZSCOT T-stage:\n",
      "{'micro_precision': 0.769, 'micro_recall': 0.769, 'micro_f1': 0.769, 'macro_precision': 0.746, 'macro_recall': 0.678, 'macro_f1': 0.703, 'weighted_f1': 0.77, 'support': 800, 'total_tp': 615, 'total_fp': 185, 'total_fn': 185, 'num_errors': 370}\n",
      "\n",
      "RAG T-stage:\n",
      "{'micro_precision': 0.836, 'micro_recall': 0.836, 'micro_f1': 0.836, 'macro_precision': 0.786, 'macro_recall': 0.748, 'macro_f1': 0.764, 'weighted_f1': 0.834, 'support': 800, 'total_tp': 669, 'total_fp': 131, 'total_fn': 131, 'num_errors': 262}\n",
      "\n",
      "KEwLTM T-stage:\n",
      "T1       0.813(0.073) 0.759(0.076) 0.783(0.064)\n",
      "T2       0.855(0.031) 0.913(0.023) 0.882(0.016)\n",
      "T3       0.869(0.063) 0.703(0.099) 0.770(0.065)\n",
      "T4       0.630(0.046) 0.615(0.057) 0.621(0.042)\n",
      "Cat‑Macro 0.792 0.747 0.764\n",
      "MicroAvg. 0.835(0.025) 0.835(0.025) 0.835(0.025)\n",
      "MacroAvg. 0.792(0.032) 0.747(0.043) 0.764(0.034)\n",
      "\n",
      "KEwRAG T-stage:\n",
      "{'micro_precision': 0.879, 'micro_recall': 0.879, 'micro_f1': 0.879, 'macro_precision': 0.838, 'macro_recall': 0.793, 'macro_f1': 0.812, 'weighted_f1': 0.876, 'support': 800, 'total_tp': 703, 'total_fp': 97, 'total_fn': 97, 'num_errors': 194}\n"
     ]
    }
   ],
   "source": [
    "kewltm_t_stage = 'kepa_t_ans_str'\n",
    "zscot_t_df = pd.read_csv('/home/yl3427/cylab/selfCorrectionAgent/result/1118_t14_med42_v2_test_800.csv').sort_values(by=\"patient_filename\")[[\"patient_filename\", 't', 'zscot_t_ans_str']]\n",
    "rag_t_df = pd.read_csv('/home/yl3427/cylab/selfCorrectionAgent/result/1120_t14_rag_raw_med42_v2_800.csv').sort_values(by=\"patient_filename\")[[\"patient_filename\", 't', 't14_rag_raw_t_pred']]\n",
    "ltm_rag_t_df = pd.read_csv('/home/yl3427/cylab/selfCorrectionAgent/result/1128_t14_ltm_rag1_med42_v2_800.csv').sort_values(by=\"patient_filename\")[[\"patient_filename\", 't', 't14_ltm_rag1_t_pred']]\n",
    "\n",
    "print(\"Med42 T-stage metrics:\")\n",
    "\n",
    "print(\"ZSCOT T-stage:\")\n",
    "print(t14_calculate_metrics(zscot_t_df['t'], zscot_t_df['zscot_t_ans_str'])['overall'])\n",
    "print()\n",
    "print(\"RAG T-stage:\")\n",
    "print(t14_calculate_metrics(rag_t_df['t'], rag_t_df['t14_rag_raw_t_pred'])['overall'])\n",
    "print()\n",
    "kewltm_t_results = []\n",
    "t_label = 't'\n",
    "run_lst = [0, 1, 2, 3, 4, 5, 6, 8]\n",
    "\n",
    "for run in run_lst:\n",
    "    t_test_df = pd.read_csv(\n",
    "        f\"/home/yl3427/cylab/selfCorrectionAgent/result/1114_t14_med42_v2_test_{run}_outof_10runs.csv\"\n",
    "    ).sort_values(by=\"patient_filename\")\n",
    "\n",
    "    kewltm_t_results.append(\n",
    "        t14_calculate_metrics(t_test_df[t_label], t_test_df[kewltm_t_stage])\n",
    "    )\n",
    "print(\"KEwLTM T-stage:\")\n",
    "output_tabular_performance(kewltm_t_results)\n",
    "\n",
    "print()\n",
    "print(\"KEwRAG T-stage:\")\n",
    "print(t14_calculate_metrics(ltm_rag_t_df['t'], ltm_rag_t_df['t14_ltm_rag1_t_pred'])['overall'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8635d140",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Med42 N-stage metrics:\n",
      "ZSCOT N-stage:\n",
      "{'micro_precision': 0.738, 'micro_recall': 0.738, 'micro_f1': 0.738, 'macro_precision': 0.748, 'macro_recall': 0.723, 'macro_f1': 0.724, 'weighted_f1': 0.742, 'support': 800, 'total_tp': 590, 'total_fp': 210, 'total_fn': 210, 'num_errors': 420}\n",
      "\n",
      "RAG N-stage:\n",
      "{'micro_precision': 0.79, 'micro_recall': 0.79, 'micro_f1': 0.79, 'macro_precision': 0.76, 'macro_recall': 0.799, 'macro_f1': 0.759, 'weighted_f1': 0.796, 'support': 800, 'total_tp': 632, 'total_fp': 168, 'total_fn': 168, 'num_errors': 336}\n",
      "\n",
      "KEwLTM N-stage:\n",
      "N0       0.950(0.011) 0.821(0.059) 0.879(0.032)\n",
      "N1       0.775(0.056) 0.821(0.032) 0.795(0.022)\n",
      "N2       0.657(0.067) 0.711(0.076) 0.675(0.018)\n",
      "N3       0.759(0.103) 0.858(0.029) 0.800(0.062)\n",
      "Cat‑Macro 0.785 0.803 0.787\n",
      "MicroAvg. 0.809(0.024) 0.809(0.024) 0.809(0.024)\n",
      "MacroAvg. 0.785(0.021) 0.803(0.025) 0.788(0.026)\n",
      "\n",
      "KEwRAG N-stage:\n",
      "{'micro_precision': 0.88, 'micro_recall': 0.88, 'micro_f1': 0.88, 'macro_precision': 0.845, 'macro_recall': 0.849, 'macro_f1': 0.846, 'weighted_f1': 0.881, 'support': 800, 'total_tp': 704, 'total_fp': 96, 'total_fn': 96, 'num_errors': 192}\n"
     ]
    }
   ],
   "source": [
    "kewltm_n_stage = 'kepa_n_ans_str'\n",
    "zscot_n_df = pd.read_csv('/home/yl3427/cylab/selfCorrectionAgent/result/1118_n03_med42_v2_test_800.csv').sort_values(by=\"patient_filename\")[[\"patient_filename\", 'n', 'zscot_n_ans_str']]\n",
    "rag_n_df = pd.read_csv('/home/yl3427/cylab/selfCorrectionAgent/result/1120_n03_rag_raw_med42_v2_800.csv').sort_values(by=\"patient_filename\")[[\"patient_filename\", 'n', 'n03_rag_raw_n_pred']]\n",
    "ltm_rag_n_df = pd.read_csv('/home/yl3427/cylab/selfCorrectionAgent/result/1128_n03_ltm_rag1_med42_v2_800.csv').sort_values(by=\"patient_filename\")[[\"patient_filename\", 'n', 'n03_ltm_rag1_n_pred']]\n",
    "\n",
    "print(\"Med42 N-stage metrics:\")\n",
    "\n",
    "print(\"ZSCOT N-stage:\")\n",
    "print(n03_calculate_metrics(zscot_n_df['n'], zscot_n_df['zscot_n_ans_str'])['overall'])\n",
    "print()\n",
    "print(\"RAG N-stage:\")\n",
    "print(n03_calculate_metrics(rag_n_df['n'], rag_n_df['n03_rag_raw_n_pred'])['overall'])\n",
    "print()\n",
    "kewltm_n_results = []\n",
    "n_label = 'n'\n",
    "run_lst = [0, 1, 3, 4, 5, 6, 7, 9]\n",
    "\n",
    "for run in run_lst:\n",
    "    n_test_df = pd.read_csv(\n",
    "        f\"/home/yl3427/cylab/selfCorrectionAgent/result/1114_n03_med42_v2_test_{run}_outof_10runs.csv\"\n",
    "    ).sort_values(by=\"patient_filename\")\n",
    "\n",
    "    kewltm_n_results.append(\n",
    "        n03_calculate_metrics(n_test_df[n_label], n_test_df[kewltm_n_stage])\n",
    "    )\n",
    "print(\"KEwLTM N-stage:\")\n",
    "output_tabular_performance(kewltm_n_results, categories=[\"N0\", \"N1\", \"N2\", \"N3\"])\n",
    "\n",
    "print()\n",
    "print(\"KEwRAG N-stage:\")\n",
    "print(n03_calculate_metrics(ltm_rag_n_df['n'], ltm_rag_n_df['n03_ltm_rag1_n_pred'])['overall'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
