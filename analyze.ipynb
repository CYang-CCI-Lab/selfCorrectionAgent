{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_wrong_indices(file_path):\n",
    "    with open(file_path, 'r') as file:\n",
    "        log_content = file.read()\n",
    "    matches = re.findall(r'index: (\\d+)[\\s|\\n|\\|]+\\|\\sWrong!', log_content, re.DOTALL)\n",
    "    wrong_indices = [int(match) for match in matches]\n",
    "    return wrong_indices\n",
    "\n",
    "file_path = '/home/yl3427/cylab/selfCorrectionAgent/result_mixtral.log'\n",
    "# '/home/yl3427/cylab/selfCorrectionAgent/result_mixtral.log'\n",
    "# '/home/yl3427/cylab/selfCorrectionAgent/result_llama3.log'\n",
    "\n",
    "wrong_indices = find_wrong_indices(file_path)\n",
    "\n",
    "print(\"Indices with 'Wrong!':\", wrong_indices)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(wrong_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = []\n",
    "for i in range(len(wrong_indices)-1):\n",
    "    if wrong_indices[i] > wrong_indices[i+1]:\n",
    "        print(i)\n",
    "        print(wrong_indices[i: i+2])\n",
    "        indices.append(i)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx1 = indices[0]+1\n",
    "idx2 = indices[1]+1\n",
    "base = set(wrong_indices[:idx1])\n",
    "always = set(wrong_indices[idx1:idx2])\n",
    "condition = set(wrong_indices[idx2:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(base)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(base & always), base & always # 49"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(base - always), base - always "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(always - base),always - base # 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "brca_report = pd.read_csv(\"/secure/shared_data/rag_tnm_results/summary/5_folds_summary/brca_df.csv\")\n",
    "sample_reports = brca_report.iloc[:500, :]\n",
    "\n",
    "sample_reports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in sorted(always - base):\n",
    "    print(f\"T{sample_reports.iloc[i].t+1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_similarity(file_path):\n",
    "    with open(file_path, 'r') as file:\n",
    "        log_content = file.read()\n",
    "    matches = re.findall(r'similarity: (\\d+)', log_content, re.DOTALL)\n",
    "    scores = [int(match) for match in matches]\n",
    "    return scores\n",
    "\n",
    "file_path = '/home/yl3427/cylab/selfCorrectionAgent/result_llama3.log'\n",
    "\n",
    "# '/home/yl3427/cylab/selfCorrectionAgent/result_mixtral.log'\n",
    "# '/home/yl3427/cylab/selfCorrectionAgent/result_llama3.log'\n",
    "\n",
    "similarity_scores = find_similarity(file_path)\n",
    "\n",
    "print(similarity_scores)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(similarity_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "index = range(0, 500)\n",
    "\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(index, similarity_scores)\n",
    "plt.xlabel('index of report')\n",
    "plt.ylabel('similiarity between created rules and the existing rules (memory)')\n",
    "plt.xlim(0, 500)\n",
    "plt.xticks(np.arange(0, 501, 25))  # Adjust the interval as needed\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "index = range(0, 80)\n",
    "\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(index, similarity_scores[:80])\n",
    "plt.xlabel('index of report')\n",
    "plt.ylabel('similiarity between created rules and the existing rules (memory)')\n",
    "plt.xlim(0, 80)\n",
    "plt.xticks(np.arange(0, 80, 5))  # Adjust the interval as needed\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_memory(file_path):\n",
    "    with open(file_path, 'r') as file:\n",
    "        log_content = file.read()\n",
    "    matches = re.findall(r\" memory: (\\[.*?\\])\", log_content, re.DOTALL)\n",
    "    wrong_indices = [ast.literal_eval(match) for match in matches]\n",
    "    return wrong_indices\n",
    "\n",
    "file_path = '/home/yl3427/cylab/selfCorrectionAgent/memory.txt'\n",
    "\n",
    "\n",
    "wrong_indices = find_memory(file_path)\n",
    "\n",
    "print(len(wrong_indices))\n",
    "print(wrong_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from src.metrics import t14_performance_report, n03_performance_report, relax_t14_performance_report, relax_n03_performance_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zshot\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          T1       0.92      0.71      0.80       188\n",
      "          T2       0.83      0.96      0.89       468\n",
      "          T3       0.91      0.74      0.82       108\n",
      "          T4       0.74      0.64      0.69        36\n",
      "\n",
      "    accuracy                           0.85       800\n",
      "   macro avg       0.85      0.76      0.80       800\n",
      "weighted avg       0.86      0.85      0.85       800\n",
      "\n",
      "\n",
      "conditional memory, test size: 10\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          N0       0.94      0.95      0.95       315\n",
      "          N1       0.86      0.90      0.88       295\n",
      "          N2       0.68      0.74      0.71       108\n",
      "          N3       1.00      0.67      0.80        72\n",
      "\n",
      "    accuracy                           0.88       790\n",
      "   macro avg       0.87      0.81      0.83       790\n",
      "weighted avg       0.88      0.88      0.88       790\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yl3427/miniconda3/envs/selfCorrection/lib/python3.9/site-packages/sklearn/metrics/_classification.py:2628: UserWarning: labels size, 4, does not match size of target_names, 5\n",
      "  warnings.warn(\n",
      "/home/yl3427/miniconda3/envs/selfCorrection/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1517: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/yl3427/miniconda3/envs/selfCorrection/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1517: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/yl3427/miniconda3/envs/selfCorrection/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1517: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "          N0       0.94      0.95      0.95       315\n",
      "          N1       0.86      0.90      0.88       295\n",
      "          N2       0.68      0.74      0.71       108\n",
      "          N3       1.00      0.67      0.80        72\n",
      "\n",
      "    accuracy                           0.88       790\n",
      "   macro avg       0.87      0.81      0.83       790\n",
      "weighted avg       0.88      0.88      0.88       790\n",
      "\n",
      "\n",
      "conditional memory, test size: 15\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          N0       0.94      0.97      0.95       306\n",
      "          N1       0.89      0.84      0.87       290\n",
      "          N2       0.67      0.81      0.73       108\n",
      "          N3       0.96      0.75      0.84        71\n",
      "\n",
      "    accuracy                           0.88       775\n",
      "   macro avg       0.87      0.84      0.85       775\n",
      "weighted avg       0.89      0.88      0.88       775\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          N0       0.94      0.95      0.94       312\n",
      "          N1       0.89      0.84      0.87       291\n",
      "          N2       0.67      0.79      0.72       110\n",
      "          N3       0.96      0.74      0.83        72\n",
      "         Unk       0.00      0.00      0.00         0\n",
      "\n",
      "    accuracy                           0.87       785\n",
      "   macro avg       0.69      0.66      0.67       785\n",
      "weighted avg       0.89      0.87      0.87       785\n",
      "\n",
      "\n",
      "conditional memory, test size: 20\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          N0       0.95      0.97      0.96       309\n",
      "          N1       0.90      0.84      0.87       292\n",
      "          N2       0.65      0.77      0.70       105\n",
      "          N3       0.90      0.77      0.83        71\n",
      "\n",
      "    accuracy                           0.88       777\n",
      "   macro avg       0.85      0.84      0.84       777\n",
      "weighted avg       0.89      0.88      0.88       777\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yl3427/miniconda3/envs/selfCorrection/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1517: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/yl3427/miniconda3/envs/selfCorrection/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1517: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/yl3427/miniconda3/envs/selfCorrection/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1517: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/yl3427/miniconda3/envs/selfCorrection/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1517: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/yl3427/miniconda3/envs/selfCorrection/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1517: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/yl3427/miniconda3/envs/selfCorrection/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1517: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "          N0       0.95      0.97      0.96       310\n",
      "          N1       0.90      0.84      0.87       292\n",
      "          N2       0.65      0.76      0.70       106\n",
      "          N3       0.90      0.76      0.83        72\n",
      "         Unk       0.00      0.00      0.00         0\n",
      "\n",
      "    accuracy                           0.88       780\n",
      "   macro avg       0.68      0.67      0.67       780\n",
      "weighted avg       0.88      0.88      0.88       780\n",
      "\n",
      "\n",
      "conditional memory, test size: 25\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          N0       0.94      0.98      0.96       308\n",
      "          N1       0.89      0.88      0.89       286\n",
      "          N2       0.72      0.70      0.71       107\n",
      "          N3       0.88      0.79      0.83        72\n",
      "\n",
      "    accuracy                           0.89       773\n",
      "   macro avg       0.86      0.84      0.85       773\n",
      "weighted avg       0.89      0.89      0.89       773\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          N0       0.94      0.98      0.96       308\n",
      "          N1       0.89      0.88      0.89       286\n",
      "          N2       0.72      0.69      0.71       108\n",
      "          N3       0.88      0.78      0.83        73\n",
      "         Unk       0.00      0.00      0.00         0\n",
      "\n",
      "    accuracy                           0.89       775\n",
      "   macro avg       0.69      0.67      0.68       775\n",
      "weighted avg       0.89      0.89      0.88       775\n",
      "\n",
      "\n",
      "conditional memory, test size: 30\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          N0       0.94      0.90      0.92       240\n",
      "          N1       0.82      0.94      0.88       265\n",
      "          N2       0.79      0.67      0.72       100\n",
      "          N3       0.91      0.74      0.82        66\n",
      "\n",
      "    accuracy                           0.86       671\n",
      "   macro avg       0.86      0.81      0.83       671\n",
      "weighted avg       0.87      0.86      0.86       671\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yl3427/miniconda3/envs/selfCorrection/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1517: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/yl3427/miniconda3/envs/selfCorrection/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1517: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/yl3427/miniconda3/envs/selfCorrection/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1517: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/yl3427/miniconda3/envs/selfCorrection/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1517: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/yl3427/miniconda3/envs/selfCorrection/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1517: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/yl3427/miniconda3/envs/selfCorrection/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1517: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "          N0       0.94      0.71      0.81       302\n",
      "          N1       0.82      0.86      0.84       289\n",
      "          N2       0.79      0.62      0.69       108\n",
      "          N3       0.91      0.69      0.78        71\n",
      "         Unk       0.00      0.00      0.00         0\n",
      "\n",
      "    accuracy                           0.75       770\n",
      "   macro avg       0.69      0.58      0.63       770\n",
      "weighted avg       0.87      0.75      0.80       770\n",
      "\n",
      "\n",
      "conditional memory, test size: 35\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          N0       0.94      0.93      0.93       300\n",
      "          N1       0.84      0.89      0.87       285\n",
      "          N2       0.70      0.69      0.69       107\n",
      "          N3       1.00      0.77      0.87        70\n",
      "\n",
      "    accuracy                           0.87       762\n",
      "   macro avg       0.87      0.82      0.84       762\n",
      "weighted avg       0.87      0.87      0.87       762\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          N0       0.94      0.92      0.93       303\n",
      "          N1       0.84      0.89      0.87       285\n",
      "          N2       0.70      0.69      0.69       107\n",
      "          N3       1.00      0.77      0.87        70\n",
      "         Unk       0.00      0.00      0.00         0\n",
      "\n",
      "    accuracy                           0.87       765\n",
      "   macro avg       0.69      0.66      0.67       765\n",
      "weighted avg       0.87      0.87      0.87       765\n",
      "\n",
      "\n",
      "conditional memory, test size: 40\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          N0       0.94      0.98      0.96       299\n",
      "          N1       0.90      0.85      0.87       281\n",
      "          N2       0.68      0.78      0.73       102\n",
      "          N3       0.90      0.80      0.85        69\n",
      "\n",
      "    accuracy                           0.89       751\n",
      "   macro avg       0.86      0.85      0.85       751\n",
      "weighted avg       0.89      0.89      0.89       751\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yl3427/miniconda3/envs/selfCorrection/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1517: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/yl3427/miniconda3/envs/selfCorrection/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1517: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/yl3427/miniconda3/envs/selfCorrection/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1517: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/yl3427/miniconda3/envs/selfCorrection/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1517: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/yl3427/miniconda3/envs/selfCorrection/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1517: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/yl3427/miniconda3/envs/selfCorrection/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1517: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "          N0       0.94      0.96      0.95       304\n",
      "          N1       0.90      0.84      0.87       282\n",
      "          N2       0.68      0.77      0.72       104\n",
      "          N3       0.90      0.79      0.84        70\n",
      "         Unk       0.00      0.00      0.00         0\n",
      "\n",
      "    accuracy                           0.88       760\n",
      "   macro avg       0.69      0.67      0.68       760\n",
      "weighted avg       0.89      0.88      0.88       760\n",
      "\n",
      "\n",
      "conditional memory, test size: 45\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          N0       0.93      0.98      0.95       299\n",
      "          N1       0.89      0.88      0.88       284\n",
      "          N2       0.71      0.70      0.71       100\n",
      "          N3       0.90      0.80      0.85        70\n",
      "\n",
      "    accuracy                           0.89       753\n",
      "   macro avg       0.86      0.84      0.85       753\n",
      "weighted avg       0.88      0.89      0.88       753\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          N0       0.93      0.97      0.95       300\n",
      "          N1       0.89      0.88      0.88       284\n",
      "          N2       0.71      0.70      0.71       100\n",
      "          N3       0.90      0.79      0.84        71\n",
      "         Unk       0.00      0.00      0.00         0\n",
      "\n",
      "    accuracy                           0.88       755\n",
      "   macro avg       0.69      0.67      0.68       755\n",
      "weighted avg       0.88      0.88      0.88       755\n",
      "\n",
      "\n",
      "conditional memory, test size: 50\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          N0       0.96      0.93      0.95       291\n",
      "          N1       0.86      0.89      0.88       275\n",
      "          N2       0.71      0.76      0.74       101\n",
      "          N3       0.89      0.77      0.82        70\n",
      "\n",
      "    accuracy                           0.88       737\n",
      "   macro avg       0.85      0.84      0.85       737\n",
      "weighted avg       0.88      0.88      0.88       737\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yl3427/miniconda3/envs/selfCorrection/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1517: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/yl3427/miniconda3/envs/selfCorrection/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1517: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/yl3427/miniconda3/envs/selfCorrection/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1517: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/yl3427/miniconda3/envs/selfCorrection/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1517: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/yl3427/miniconda3/envs/selfCorrection/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1517: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/yl3427/miniconda3/envs/selfCorrection/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1517: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "          N0       0.96      0.90      0.93       301\n",
      "          N1       0.86      0.89      0.88       275\n",
      "          N2       0.71      0.75      0.73       103\n",
      "          N3       0.89      0.76      0.82        71\n",
      "         Unk       0.00      0.00      0.00         0\n",
      "\n",
      "    accuracy                           0.86       750\n",
      "   macro avg       0.68      0.66      0.67       750\n",
      "weighted avg       0.88      0.86      0.87       750\n",
      "\n",
      "\n",
      "conditional memory, test size: 55\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          N0       0.94      0.98      0.96       282\n",
      "          N1       0.90      0.87      0.88       278\n",
      "          N2       0.73      0.77      0.75       103\n",
      "          N3       0.92      0.82      0.86        66\n",
      "\n",
      "    accuracy                           0.89       729\n",
      "   macro avg       0.87      0.86      0.86       729\n",
      "weighted avg       0.89      0.89      0.89       729\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          N0       0.94      0.93      0.94       295\n",
      "          N1       0.90      0.87      0.88       279\n",
      "          N2       0.73      0.76      0.75       104\n",
      "          N3       0.92      0.81      0.86        67\n",
      "         Unk       0.00      0.00      0.00         0\n",
      "\n",
      "    accuracy                           0.87       745\n",
      "   macro avg       0.70      0.67      0.68       745\n",
      "weighted avg       0.89      0.87      0.88       745\n",
      "\n",
      "\n",
      "conditional memory, test size: 60\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          N0       0.94      0.98      0.96       291\n",
      "          N1       0.91      0.87      0.89       277\n",
      "          N2       0.68      0.77      0.72        97\n",
      "          N3       0.98      0.76      0.85        66\n",
      "\n",
      "    accuracy                           0.89       731\n",
      "   macro avg       0.88      0.85      0.86       731\n",
      "weighted avg       0.90      0.89      0.89       731\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yl3427/miniconda3/envs/selfCorrection/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1517: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/yl3427/miniconda3/envs/selfCorrection/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1517: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/yl3427/miniconda3/envs/selfCorrection/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1517: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/yl3427/miniconda3/envs/selfCorrection/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1517: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/yl3427/miniconda3/envs/selfCorrection/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1517: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/yl3427/miniconda3/envs/selfCorrection/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1517: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "          N0       0.94      0.95      0.95       299\n",
      "          N1       0.91      0.87      0.89       277\n",
      "          N2       0.68      0.77      0.72        97\n",
      "          N3       0.98      0.75      0.85        67\n",
      "         Unk       0.00      0.00      0.00         0\n",
      "\n",
      "    accuracy                           0.88       740\n",
      "   macro avg       0.70      0.67      0.68       740\n",
      "weighted avg       0.90      0.88      0.89       740\n",
      "\n",
      "\n",
      "conditional memory, test size: 65\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          N0       0.93      0.98      0.96       286\n",
      "          N1       0.88      0.88      0.88       277\n",
      "          N2       0.73      0.72      0.72       106\n",
      "          N3       0.96      0.78      0.86        65\n",
      "\n",
      "    accuracy                           0.89       734\n",
      "   macro avg       0.88      0.84      0.86       734\n",
      "weighted avg       0.89      0.89      0.89       734\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          N0       0.93      0.98      0.96       286\n",
      "          N1       0.88      0.88      0.88       277\n",
      "          N2       0.73      0.72      0.72       106\n",
      "          N3       0.96      0.77      0.86        66\n",
      "         Unk       0.00      0.00      0.00         0\n",
      "\n",
      "    accuracy                           0.89       735\n",
      "   macro avg       0.70      0.67      0.68       735\n",
      "weighted avg       0.89      0.89      0.89       735\n",
      "\n",
      "\n",
      "conditional memory, test size: 70\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          N0       0.93      0.98      0.96       288\n",
      "          N1       0.89      0.87      0.88       271\n",
      "          N2       0.76      0.69      0.72       103\n",
      "          N3       0.84      0.84      0.84        64\n",
      "\n",
      "    accuracy                           0.89       726\n",
      "   macro avg       0.86      0.85      0.85       726\n",
      "weighted avg       0.88      0.89      0.88       726\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yl3427/miniconda3/envs/selfCorrection/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1517: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/yl3427/miniconda3/envs/selfCorrection/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1517: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/yl3427/miniconda3/envs/selfCorrection/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1517: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/yl3427/miniconda3/envs/selfCorrection/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1517: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/yl3427/miniconda3/envs/selfCorrection/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1517: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/yl3427/miniconda3/envs/selfCorrection/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1517: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "          N0       0.93      0.97      0.95       292\n",
      "          N1       0.89      0.87      0.88       271\n",
      "          N2       0.76      0.69      0.72       103\n",
      "          N3       0.84      0.84      0.84        64\n",
      "         Unk       0.00      0.00      0.00         0\n",
      "\n",
      "    accuracy                           0.88       730\n",
      "   macro avg       0.68      0.67      0.68       730\n",
      "weighted avg       0.88      0.88      0.88       730\n",
      "\n",
      "\n",
      "conditional memory, test size: 75\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          N0       0.93      0.98      0.95       286\n",
      "          N1       0.87      0.89      0.88       271\n",
      "          N2       0.74      0.68      0.71       100\n",
      "          N3       0.94      0.76      0.84        66\n",
      "\n",
      "    accuracy                           0.88       723\n",
      "   macro avg       0.87      0.83      0.85       723\n",
      "weighted avg       0.88      0.88      0.88       723\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          N0       0.93      0.98      0.95       286\n",
      "          N1       0.87      0.89      0.88       272\n",
      "          N2       0.74      0.68      0.71       100\n",
      "          N3       0.94      0.75      0.83        67\n",
      "         Unk       0.00      0.00      0.00         0\n",
      "\n",
      "    accuracy                           0.88       725\n",
      "   macro avg       0.70      0.66      0.67       725\n",
      "weighted avg       0.88      0.88      0.88       725\n",
      "\n",
      "\n",
      "conditional memory, test size: 80\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          N0       0.94      0.97      0.96       279\n",
      "          N1       0.89      0.90      0.90       273\n",
      "          N2       0.73      0.75      0.74        97\n",
      "          N3       0.96      0.75      0.84        67\n",
      "\n",
      "    accuracy                           0.89       716\n",
      "   macro avg       0.88      0.84      0.86       716\n",
      "weighted avg       0.90      0.89      0.89       716\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yl3427/miniconda3/envs/selfCorrection/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1517: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/yl3427/miniconda3/envs/selfCorrection/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1517: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/yl3427/miniconda3/envs/selfCorrection/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1517: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/yl3427/miniconda3/envs/selfCorrection/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1517: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/yl3427/miniconda3/envs/selfCorrection/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1517: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/yl3427/miniconda3/envs/selfCorrection/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1517: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "          N0       0.94      0.96      0.95       282\n",
      "          N1       0.89      0.90      0.90       273\n",
      "          N2       0.73      0.75      0.74        97\n",
      "          N3       0.96      0.74      0.83        68\n",
      "         Unk       0.00      0.00      0.00         0\n",
      "\n",
      "    accuracy                           0.89       720\n",
      "   macro avg       0.70      0.67      0.68       720\n",
      "weighted avg       0.90      0.89      0.89       720\n",
      "\n",
      "\n",
      "conditional memory, test size: 85\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          N0       0.93      0.98      0.96       277\n",
      "          N1       0.89      0.84      0.87       268\n",
      "          N2       0.65      0.73      0.69       100\n",
      "          N3       0.95      0.77      0.85        69\n",
      "\n",
      "    accuracy                           0.87       714\n",
      "   macro avg       0.85      0.83      0.84       714\n",
      "weighted avg       0.88      0.87      0.87       714\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          N0       0.93      0.98      0.96       277\n",
      "          N1       0.89      0.84      0.87       268\n",
      "          N2       0.65      0.73      0.69       100\n",
      "          N3       0.95      0.76      0.84        70\n",
      "         Unk       0.00      0.00      0.00         0\n",
      "\n",
      "    accuracy                           0.87       715\n",
      "   macro avg       0.68      0.66      0.67       715\n",
      "weighted avg       0.88      0.87      0.87       715\n",
      "\n",
      "\n",
      "conditional memory, test size: 90\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          N0       0.93      0.98      0.96       274\n",
      "          N1       0.87      0.91      0.89       266\n",
      "          N2       0.77      0.70      0.73       105\n",
      "          N3       0.94      0.71      0.81        65\n",
      "\n",
      "    accuracy                           0.89       710\n",
      "   macro avg       0.88      0.82      0.85       710\n",
      "weighted avg       0.88      0.89      0.88       710\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yl3427/miniconda3/envs/selfCorrection/lib/python3.9/site-packages/sklearn/metrics/_classification.py:2628: UserWarning: labels size, 4, does not match size of target_names, 5\n",
      "  warnings.warn(\n",
      "/home/yl3427/miniconda3/envs/selfCorrection/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1517: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/yl3427/miniconda3/envs/selfCorrection/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1517: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/yl3427/miniconda3/envs/selfCorrection/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1517: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "          N0       0.93      0.98      0.96       274\n",
      "          N1       0.87      0.91      0.89       266\n",
      "          N2       0.77      0.70      0.73       105\n",
      "          N3       0.94      0.71      0.81        65\n",
      "\n",
      "    accuracy                           0.89       710\n",
      "   macro avg       0.88      0.82      0.85       710\n",
      "weighted avg       0.88      0.89      0.88       710\n",
      "\n",
      "\n",
      "conditional memory, test size: 95\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          N0       0.95      0.98      0.96       266\n",
      "          N1       0.92      0.78      0.84       264\n",
      "          N2       0.54      0.83      0.65        96\n",
      "          N3       1.00      0.69      0.81        64\n",
      "\n",
      "    accuracy                           0.86       690\n",
      "   macro avg       0.85      0.82      0.82       690\n",
      "weighted avg       0.89      0.86      0.86       690\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          N0       0.95      0.95      0.95       275\n",
      "          N1       0.92      0.76      0.84       268\n",
      "          N2       0.54      0.82      0.65        97\n",
      "          N3       1.00      0.68      0.81        65\n",
      "         Unk       0.00      0.00      0.00         0\n",
      "\n",
      "    accuracy                           0.84       705\n",
      "   macro avg       0.68      0.64      0.65       705\n",
      "weighted avg       0.89      0.84      0.85       705\n",
      "\n",
      "\n",
      "conditional memory, test size: 100\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          N0       0.93      0.98      0.96       273\n",
      "          N1       0.92      0.83      0.87       266\n",
      "          N2       0.63      0.78      0.70        94\n",
      "          N3       0.89      0.75      0.81        64\n",
      "\n",
      "    accuracy                           0.88       697\n",
      "   macro avg       0.84      0.83      0.83       697\n",
      "weighted avg       0.88      0.88      0.88       697\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          N0       0.93      0.98      0.96       274\n",
      "          N1       0.92      0.83      0.87       267\n",
      "          N2       0.63      0.78      0.70        94\n",
      "          N3       0.89      0.74      0.81        65\n",
      "         Unk       0.00      0.00      0.00         0\n",
      "\n",
      "    accuracy                           0.87       700\n",
      "   macro avg       0.67      0.66      0.67       700\n",
      "weighted avg       0.88      0.87      0.87       700\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yl3427/miniconda3/envs/selfCorrection/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1517: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/yl3427/miniconda3/envs/selfCorrection/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1517: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/yl3427/miniconda3/envs/selfCorrection/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1517: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "print(\"zshot\")\n",
    "df = pd.read_csv(\"result/t14/brca_df_zs_t14.csv\")\n",
    "t14_performance_report(df, \"zs_t_ans_str\")\n",
    "print()\n",
    "\n",
    "for size in range(10, 101, 5):\n",
    "    print(f\"conditional memory, test size: {size}\")\n",
    "    df = pd.read_csv(f\"/home/yl3427/cylab/selfCorrectionAgent/result/n03/saturation_test_result_{size}.csv\")\n",
    "    n03_performance_report(df, \"cmem_n_ans_str\")\n",
    "    relax_n03_performance_report(df, \"cmem_n_ans_str\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train size: 80\n",
      "\n",
      " 0 fold\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          N0       0.93      0.96      0.95       253\n",
      "          N1       0.88      0.84      0.86       244\n",
      "          N2       0.70      0.76      0.73        86\n",
      "          N3       0.89      0.80      0.85        51\n",
      "\n",
      "    accuracy                           0.87       634\n",
      "   macro avg       0.85      0.84      0.84       634\n",
      "weighted avg       0.88      0.87      0.87       634\n",
      "\n",
      "\n",
      " 1 fold\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          N0       0.94      0.97      0.95       240\n",
      "          N1       0.92      0.77      0.84       243\n",
      "          N2       0.53      0.88      0.67        78\n",
      "          N3       1.00      0.69      0.82        61\n",
      "\n",
      "    accuracy                           0.85       622\n",
      "   macro avg       0.85      0.83      0.82       622\n",
      "weighted avg       0.89      0.85      0.86       622\n",
      "\n",
      "\n",
      " 2 fold\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          N0       0.96      0.99      0.97       266\n",
      "          N1       0.85      0.95      0.90       217\n",
      "          N2       0.81      0.61      0.69        89\n",
      "          N3       1.00      0.80      0.89        65\n",
      "\n",
      "    accuracy                           0.90       637\n",
      "   macro avg       0.90      0.84      0.86       637\n",
      "weighted avg       0.90      0.90      0.90       637\n",
      "\n",
      "\n",
      " 3 fold\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          N0       0.92      0.98      0.95       236\n",
      "          N1       0.91      0.84      0.87       251\n",
      "          N2       0.61      0.79      0.69        89\n",
      "          N3       1.00      0.65      0.79        60\n",
      "\n",
      "    accuracy                           0.87       636\n",
      "   macro avg       0.86      0.81      0.82       636\n",
      "weighted avg       0.88      0.87      0.87       636\n",
      "\n",
      "\n",
      " 4 fold\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          N0       0.93      0.95      0.94       233\n",
      "          N1       0.88      0.75      0.81       244\n",
      "          N2       0.61      0.77      0.68        94\n",
      "          N3       0.75      0.82      0.78        55\n",
      "\n",
      "    accuracy                           0.83       626\n",
      "   macro avg       0.79      0.82      0.80       626\n",
      "weighted avg       0.84      0.83      0.84       626\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"train size: 80\")\n",
    "print()\n",
    "for fold in range(5):\n",
    "    print(f\" {fold} fold\")\n",
    "    df = pd.read_csv(f\"/home/yl3427/cylab/selfCorrectionAgent/result/n03/{fold}_fold_saturation_test_result_80.csv\")\n",
    "    n03_performance_report(df, \"cmem_n_ans_str\")\n",
    "    # relax_n03_performance_report(df, \"cmem_n_ans_str\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "          T1       0.90      0.86      0.88       177\n",
      "          T2       0.89      0.95      0.92       418\n",
      "          T3       0.88      0.84      0.86        91\n",
      "          T4       0.82      0.53      0.64        34\n",
      "\n",
      "    accuracy                           0.89       720\n",
      "   macro avg       0.87      0.79      0.83       720\n",
      "weighted avg       0.89      0.89      0.89       720\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# /home/yl3427/cylab/selfCorrectionAgent/result/n03/saturation_test_result_80.csv\n",
    "df = pd.read_csv(\"/home/yl3427/cylab/selfCorrectionAgent/result/t14/memory_test_result_80.csv\")\n",
    "t14_performance_report(df, \"mem_t_ans_str\")\n",
    "# relax_n03_performance_report(df, \"mem_n_ans_str\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "          N0       0.95      0.98      0.97       288\n",
      "          N1       0.91      0.89      0.90       263\n",
      "          N2       0.77      0.78      0.77        92\n",
      "          N3       0.93      0.84      0.89        64\n",
      "\n",
      "    accuracy                           0.91       707\n",
      "   macro avg       0.89      0.87      0.88       707\n",
      "weighted avg       0.91      0.91      0.91       707\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"/home/yl3427/cylab/selfCorrectionAgent/result/n03/memory_test_result_80.csv\")\n",
    "n03_performance_report(df, \"mem_n_ans_str\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "/home/yl3427/cylab/selfCorrectionAgent/result/t14/memory_test_result_80.csv"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "selfCorrection",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
