{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from metrics import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_mean_std(results, cat):\n",
    "    precision_list = [result[cat][\"precision\"] for result in results]\n",
    "    recall_list = [result[cat][\"recall\"] for result in results]\n",
    "    f1_list = [result[cat][\"f1\"] for result in results]\n",
    "    support_list = [result[cat][\"support\"] for result in results]\n",
    "    num_errors_list = [result[cat][\"num_errors\"] for result in results]\n",
    "\n",
    "    mean_precision = sum(precision_list) / len(precision_list)\n",
    "    mean_recall = sum(recall_list) / len(recall_list)\n",
    "    mean_f1 = sum(f1_list) / len(f1_list)\n",
    "\n",
    "    std_precision = (\n",
    "        sum([(x - mean_precision) ** 2 for x in precision_list]) / len(precision_list)\n",
    "    ) ** 0.5\n",
    "    std_recall = (\n",
    "        sum([(x - mean_recall) ** 2 for x in recall_list]) / len(recall_list)\n",
    "    ) ** 0.5\n",
    "    std_f1 = (sum([(x - mean_f1) ** 2 for x in f1_list]) / len(f1_list)) ** 0.5\n",
    "\n",
    "    return {\n",
    "        \"mean_precision\": round(mean_precision, 3),\n",
    "        \"mean_recall\": round(mean_recall, 3),\n",
    "        \"mean_f1\": round(mean_f1, 3),\n",
    "        \"std_precision\": round(std_precision, 3),\n",
    "        \"std_recall\": round(std_recall, 3),\n",
    "        \"std_f1\": round(std_f1, 3),\n",
    "        \"sum_support\": sum(support_list),\n",
    "        \"sum_num_errors\": sum(num_errors_list),\n",
    "        \"raw_mean_precision\": mean_precision,\n",
    "        \"raw_mean_recall\": mean_recall,\n",
    "        \"raw_mean_f1\": mean_f1,\n",
    "    }\n",
    "\n",
    "\n",
    "def output_tabular_performance(results, categories=[\"T1\", \"T2\", \"T3\", \"T4\"]):\n",
    "    precisions = []\n",
    "    recalls = []\n",
    "    f1s = []\n",
    "\n",
    "    for category in categories:\n",
    "        eval = calculate_mean_std(results, category)\n",
    "        print(\n",
    "            \"{} {:.3f}({:.3f}) {:.3f}({:.3f}) {:.3f}({:.3f})\".format(\n",
    "                category,\n",
    "                eval[\"mean_precision\"],\n",
    "                eval[\"std_precision\"],\n",
    "                eval[\"mean_recall\"],\n",
    "                eval[\"std_recall\"],\n",
    "                eval[\"mean_f1\"],\n",
    "                eval[\"std_f1\"],\n",
    "            )\n",
    "        )\n",
    "\n",
    "        # for calculating macro average\n",
    "        precisions.append(eval[\"raw_mean_precision\"])\n",
    "        recalls.append(eval[\"raw_mean_recall\"])\n",
    "        f1s.append(eval[\"raw_mean_f1\"])\n",
    "\n",
    "    print(\n",
    "        \"MacroAvg. {:.3f} {:.3f} {:.3f}\".format(\n",
    "            round(sum(precisions) / len(precisions), 3),\n",
    "            round(sum(recalls) / len(recalls), 3),\n",
    "            round(sum(f1s) / len(f1s), 3),\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# kepa (reported in the draft)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "kepa_t_results = []\n",
    "zs_t_results = []\n",
    "zscot_t_results = []\n",
    "\n",
    "\n",
    "kepa_run_lst = [0, 1, 2, 3, 4, 5, 6, 8]\n",
    "\n",
    "for run in kepa_run_lst:\n",
    "    # print(f\"Run {run}, memory 40\")\n",
    "    pred_column = \"cmem_t_40reports_ans_str\"\n",
    "\n",
    "    t_test_df = pd.read_csv(\n",
    "        f\"/home/yl3427/cylab/selfCorrectionAgent/result/0718_t14_dynamic_test_{run}_outof_10runs.csv\"\n",
    "    ).sort_values(by=\"patient_filename\")\n",
    "    t_zs_df = pd.read_csv(\n",
    "        \"/home/yl3427/cylab/selfCorrectionAgent/result/0716_t14_zs_test_800.csv\"\n",
    "    ).sort_values(by=\"patient_filename\")\n",
    "    t_zscot_df = pd.read_csv(\n",
    "        \"/home/yl3427/cylab/selfCorrectionAgent/result/0716_t14_zscot_test_800.csv\"\n",
    "    ).sort_values(by=\"patient_filename\")\n",
    "\n",
    "    split_ids = t_test_df.patient_filename\n",
    "\n",
    "    label_column = t_test_df[\"t\"]\n",
    "    t_test_pred_df = t_test_df[t_test_df.patient_filename.isin(split_ids)][pred_column]\n",
    "    kepa_t_results.append(\n",
    "        t14_calculate_metrics(true_labels=label_column, predictions=t_test_pred_df)\n",
    "    )\n",
    "\n",
    "    t_zs_pred_df = t_zs_df[t_zs_df.patient_filename.isin(split_ids)][\"zs_t_ans_str\"]\n",
    "    zs_t_results.append(\n",
    "        t14_calculate_metrics(true_labels=label_column, predictions=t_zs_pred_df)\n",
    "    )\n",
    "\n",
    "    t_zscot_pred_df = t_zscot_df[t_zscot_df.patient_filename.isin(split_ids)][\n",
    "        \"zs_t_ans_str\"\n",
    "    ]\n",
    "    zscot_t_results.append(\n",
    "        t14_calculate_metrics(true_labels=label_column, predictions=t_zscot_pred_df)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "T1 0.904(0.017) 0.812(0.040) 0.855(0.018)\n",
      "T2 0.882(0.022) 0.938(0.018) 0.909(0.005)\n",
      "T3 0.834(0.054) 0.810(0.058) 0.818(0.018)\n",
      "T4 0.807(0.082) 0.634(0.038) 0.707(0.029)\n",
      "MacroAvg. 0.857 0.799 0.822\n"
     ]
    }
   ],
   "source": [
    "output_tabular_performance(kepa_t_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "kepa_n_results = []\n",
    "zs_n_results = []\n",
    "zscot_n_results = []\n",
    "\n",
    "kepa_run_lst = [0, 1, 3, 4, 5, 6, 7, 9]\n",
    "\n",
    "for run in kepa_run_lst:\n",
    "    # print(f\"Run {run}, memory 40\")\n",
    "    pred_column = \"cmem_n_40reports_ans_str\"\n",
    "\n",
    "    n_test_df = pd.read_csv(\n",
    "        f\"/home/yl3427/cylab/selfCorrectionAgent/result/0718_n03_dynamic_test_{run}_outof_10runs.csv\"\n",
    "    ).sort_values(by=\"patient_filename\")\n",
    "    n_zs_df = pd.read_csv(\n",
    "        \"/home/yl3427/cylab/selfCorrectionAgent/result/0716_n03_zs_test_800.csv\"\n",
    "    ).sort_values(by=\"patient_filename\")\n",
    "    n_zscot_df = pd.read_csv(\n",
    "        \"/home/yl3427/cylab/selfCorrectionAgent/result/0716_n03_zscot_test_800.csv\"\n",
    "    ).sort_values(by=\"patient_filename\")\n",
    "\n",
    "    split_ids = n_test_df.patient_filename\n",
    "\n",
    "    label_column = n_test_df[\"n\"]\n",
    "    n_test_pred_df = n_test_df[n_test_df.patient_filename.isin(split_ids)][pred_column]\n",
    "    kepa_n_results.append(\n",
    "        n03_calculate_metrics(true_labels=label_column, predictions=n_test_pred_df)\n",
    "    )\n",
    "\n",
    "    n_zs_pred_df = n_zs_df[n_zs_df.patient_filename.isin(split_ids)][\"zs_n_ans_str\"]\n",
    "    zs_n_results.append(\n",
    "        n03_calculate_metrics(true_labels=label_column, predictions=n_zs_pred_df)\n",
    "    )\n",
    "\n",
    "    n_zscot_pred_df = n_zscot_df[n_zscot_df.patient_filename.isin(split_ids)][\n",
    "        \"zs_n_ans_str\"\n",
    "    ]\n",
    "    zscot_n_results.append(\n",
    "        n03_calculate_metrics(true_labels=label_column, predictions=n_zscot_pred_df)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N0 0.944(0.008) 0.952(0.018) 0.948(0.011)\n",
      "N1 0.885(0.020) 0.883(0.026) 0.884(0.010)\n",
      "N2 0.713(0.031) 0.745(0.054) 0.727(0.022)\n",
      "N3 0.886(0.058) 0.784(0.042) 0.830(0.017)\n",
      "MacroAvg. 0.857 0.841 0.847\n"
     ]
    }
   ],
   "source": [
    "output_tabular_performance(kepa_n_results, categories=[\"N0\", \"N1\", \"N2\", \"N3\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "kepa_t_results = []\n",
    "\n",
    "kepa_run_lst = [0, 1, 2, 3, 4, 5, 6, 8]\n",
    "\n",
    "for run in kepa_run_lst:\n",
    "    # print(f\"Run {run}, memory 40\")\n",
    "    pred_column = \"gpt4o_t_stage\"\n",
    "\n",
    "    t_test_df = pd.read_csv(\n",
    "        f\"/home/yl3427/cylab/selfCorrectionAgent/result/1112_t14_gpt_test_{run}_outof_8runs.csv\"\n",
    "    ).sort_values(by=\"patient_filename\")\n",
    "\n",
    "    label_column = t_test_df[\"t\"]\n",
    "    t_test_pred_df = t_test_df[pred_column]\n",
    "    kepa_t_results.append(\n",
    "        t14_calculate_metrics(true_labels=label_column, predictions=t_test_pred_df)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "T1 0.902(0.009) 0.903(0.025) 0.902(0.013)\n",
      "T2 0.935(0.023) 0.939(0.015) 0.936(0.007)\n",
      "T3 0.905(0.048) 0.813(0.080) 0.852(0.039)\n",
      "T4 0.622(0.136) 0.728(0.052) 0.659(0.052)\n",
      "MacroAvg. 0.841 0.846 0.837\n"
     ]
    }
   ],
   "source": [
    "output_tabular_performance(kepa_t_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "kepa_n_results = []\n",
    "\n",
    "kepa_run_lst = [0, 1, 3, 4, 5, 6, 7, 9]\n",
    "\n",
    "for run in kepa_run_lst:\n",
    "    # print(f\"Run {run}, memory 40\")\n",
    "    pred_column = \"gpt4o_n_stage\"\n",
    "\n",
    "    n_test_df = pd.read_csv(\n",
    "        f\"/home/yl3427/cylab/selfCorrectionAgent/result/1112_n03_gpt_test_{run}_outof_8runs.csv\"\n",
    "    ).sort_values(by=\"patient_filename\")\n",
    "\n",
    "    label_column = n_test_df[\"n\"]\n",
    "    n_test_pred_df = n_test_df[pred_column]\n",
    "    kepa_n_results.append(\n",
    "        n03_calculate_metrics(true_labels=label_column, predictions=n_test_pred_df)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N0 0.928(0.006) 0.962(0.051) 0.944(0.026)\n",
      "N1 0.921(0.006) 0.875(0.013) 0.897(0.008)\n",
      "N2 0.777(0.110) 0.786(0.034) 0.778(0.077)\n",
      "N3 0.855(0.047) 0.850(0.016) 0.852(0.031)\n",
      "MacroAvg. 0.870 0.868 0.868\n"
     ]
    }
   ],
   "source": [
    "output_tabular_performance(kepa_n_results, categories=[\"N0\", \"N1\", \"N2\", \"N3\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# llama\n",
    "kepa_t_results = []\n",
    "\n",
    "kepa_run_lst = [0, 1, 2, 3, 4, 5, 6, 8]\n",
    "\n",
    "for run in kepa_run_lst:\n",
    "    # print(f\"Run {run}, memory 40\")\n",
    "    pred_column = \"t14_kewltm_t_pred\"\n",
    "\n",
    "    t_test_df = pd.read_csv(\n",
    "        f\"/home/yl3427/cylab/selfCorrectionAgent/result/1208_t14_llama3_kewltm_{run}_outof_10runs.csv\"\n",
    "    ).sort_values(by=\"patient_filename\")\n",
    "\n",
    "    label_column = t_test_df[\"t\"]\n",
    "    t_test_pred_df = t_test_df[pred_column]\n",
    "    kepa_t_results.append(\n",
    "        t14_calculate_metrics(true_labels=label_column, predictions=t_test_pred_df)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "T1 0.878(0.017) 0.771(0.043) 0.820(0.026)\n",
      "T2 0.853(0.032) 0.921(0.010) 0.885(0.021)\n",
      "T3 0.932(0.024) 0.629(0.127) 0.742(0.103)\n",
      "T4 0.505(0.042) 0.784(0.020) 0.613(0.028)\n",
      "MacroAvg. 0.792 0.776 0.765\n"
     ]
    }
   ],
   "source": [
    "output_tabular_performance(kepa_t_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "kepa_n_results = []\n",
    "\n",
    "kepa_run_lst = [0, 1, 3, 4, 5, 6, 7, 9]\n",
    "\n",
    "for run in kepa_run_lst:\n",
    "    # print(f\"Run {run}, memory 40\")\n",
    "    pred_column = \"n03_kewltm_n_pred\"\n",
    "\n",
    "    n_test_df = pd.read_csv(\n",
    "        f\"/home/yl3427/cylab/selfCorrectionAgent/result/1208_n03_llama3_kewltm_{run}_outof_10runs.csv\"\n",
    "    ).sort_values(by=\"patient_filename\")\n",
    "\n",
    "    label_column = n_test_df[\"n\"]\n",
    "    n_test_pred_df = n_test_df[pred_column]\n",
    "    kepa_n_results.append(\n",
    "        n03_calculate_metrics(true_labels=label_column, predictions=n_test_pred_df)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N0 0.936(0.004) 0.976(0.013) 0.955(0.006)\n",
      "N1 0.905(0.031) 0.900(0.015) 0.902(0.012)\n",
      "N2 0.797(0.035) 0.732(0.100) 0.761(0.074)\n",
      "N3 0.879(0.016) 0.838(0.026) 0.858(0.013)\n",
      "MacroAvg. 0.879 0.861 0.869\n"
     ]
    }
   ],
   "source": [
    "output_tabular_performance(kepa_n_results, categories=[\"N0\", \"N1\", \"N2\", \"N3\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# other models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "kepa_t_results = []\n",
    "\n",
    "kepa_run_lst = [0, 1, 2, 3, 4, 5, 6, 8]\n",
    "\n",
    "for run in kepa_run_lst:\n",
    "    # print(f\"Run {run}, memory 40\")\n",
    "    pred_column = \"kepa_t_ans_str\"\n",
    "\n",
    "    t_test_df = pd.read_csv(\n",
    "        f\"/home/yl3427/cylab/selfCorrectionAgent/result/1114_t14_med42_v2_test_{run}_outof_10runs.csv\"\n",
    "    ).sort_values(by=\"patient_filename\")\n",
    "\n",
    "    label_column = t_test_df[\"t\"]\n",
    "    t_test_pred_df = t_test_df[pred_column]\n",
    "    kepa_t_results.append(\n",
    "        t14_calculate_metrics(true_labels=label_column, predictions=t_test_pred_df)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "T1 0.813(0.073) 0.759(0.076) 0.783(0.064)\n",
      "T2 0.855(0.031) 0.913(0.023) 0.882(0.016)\n",
      "T3 0.869(0.063) 0.703(0.099) 0.770(0.065)\n",
      "T4 0.630(0.046) 0.615(0.057) 0.621(0.042)\n",
      "MacroAvg. 0.792 0.747 0.764\n"
     ]
    }
   ],
   "source": [
    "output_tabular_performance(kepa_t_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "kepa_n_results = []\n",
    "\n",
    "kepa_run_lst = [0, 1, 3, 4, 5, 6, 7, 9]\n",
    "\n",
    "for run in kepa_run_lst:\n",
    "    # print(f\"Run {run}, memory 40\")\n",
    "    pred_column = \"kepa_n_ans_str\"\n",
    "\n",
    "    n_test_df = pd.read_csv(\n",
    "        f\"/home/yl3427/cylab/selfCorrectionAgent/result/1114_n03_med42_v2_test_{run}_outof_10runs.csv\"\n",
    "    ).sort_values(by=\"patient_filename\")\n",
    "\n",
    "    label_column = n_test_df[\"n\"]\n",
    "    n_test_pred_df = n_test_df[pred_column]\n",
    "    kepa_n_results.append(\n",
    "        n03_calculate_metrics(true_labels=label_column, predictions=n_test_pred_df)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N0 0.950(0.011) 0.821(0.059) 0.879(0.032)\n",
      "N1 0.775(0.056) 0.821(0.032) 0.795(0.022)\n",
      "N2 0.657(0.067) 0.711(0.076) 0.675(0.018)\n",
      "N3 0.759(0.103) 0.858(0.029) 0.800(0.062)\n",
      "MacroAvg. 0.785 0.803 0.787\n"
     ]
    }
   ],
   "source": [
    "output_tabular_performance(kepa_n_results, categories=[\"N0\", \"N1\", \"N2\", \"N3\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### zscot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "zscot_t_df = pd.read_csv(\n",
    "    f\"/home/yl3427/cylab/selfCorrectionAgent/result/1118_t14_med42_v2_test_800.csv\"\n",
    ")\n",
    "zscot_n_df = pd.read_csv(\n",
    "    f\"/home/yl3427/cylab/selfCorrectionAgent/result/1118_n03_med42_v2_test_800.csv\"\n",
    ")\n",
    "\n",
    "t_label_column = zscot_t_df[\"t\"]\n",
    "t_pred_column = zscot_t_df[\"zscot_t_ans_str\"]\n",
    "\n",
    "n_label_column = zscot_n_df[\"n\"]\n",
    "n_pred_column = zscot_n_df[\"zscot_n_ans_str\"]\n",
    "\n",
    "t_results = t14_calculate_metrics(true_labels=t_label_column, predictions=t_pred_column)\n",
    "n_results = n03_calculate_metrics(true_labels=n_label_column, predictions=n_pred_column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'macro_precision': 0.746,\n",
       " 'macro_recall': 0.678,\n",
       " 'macro_f1': 0.703,\n",
       " 'support': 800,\n",
       " 'num_errors': 370}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t_results[\"overall\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'macro_precision': 0.748,\n",
       " 'macro_recall': 0.723,\n",
       " 'macro_f1': 0.724,\n",
       " 'support': 800,\n",
       " 'num_errors': 420}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_results[\"overall\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'macro_precision': 0.786,\n",
       " 'macro_recall': 0.748,\n",
       " 'macro_f1': 0.764,\n",
       " 'support': 800,\n",
       " 'num_errors': 262}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# t14\n",
    "test_df = pd.read_csv(\n",
    "    f\"/home/yl3427/cylab/selfCorrectionAgent/result/1120_t14_rag_raw_med42_v2_800.csv\"\n",
    ")\n",
    "test_df.columns\n",
    "\n",
    "t_label_column = test_df[\"t\"]\n",
    "t_pred_column = test_df[\"t14_rag_raw_t_pred\"]\n",
    "\n",
    "t14_calculate_metrics(true_labels=t_label_column, predictions=t_pred_column)[\"overall\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'macro_precision': 0.76,\n",
       " 'macro_recall': 0.799,\n",
       " 'macro_f1': 0.759,\n",
       " 'support': 800,\n",
       " 'num_errors': 336}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# n03\n",
    "test_df = pd.read_csv(\n",
    "    f\"/home/yl3427/cylab/selfCorrectionAgent/result/1120_n03_rag_raw_med42_v2_800.csv\"\n",
    ")\n",
    "test_df.columns\n",
    "\n",
    "n_label_column = test_df[\"n\"]\n",
    "n_pred_column = test_df[\"n03_rag_raw_n_pred\"]\n",
    "\n",
    "n03_calculate_metrics(true_labels=n_label_column, predictions=n_pred_column)[\"overall\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'macro_precision': 0.837,\n",
       " 'macro_recall': 0.799,\n",
       " 'macro_f1': 0.816,\n",
       " 'support': 800,\n",
       " 'num_errors': 206}"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# t14\n",
    "test_df = pd.read_csv(\n",
    "    f\"/home/yl3427/cylab/selfCorrectionAgent/result/1128_t14_ltm_zs_med42_v2_800.csv\"\n",
    ")\n",
    "\n",
    "t_label_column = test_df[\"t\"]\n",
    "t_pred_column = test_df[\"t14_ltm_zs_t_pred\"]\n",
    "\n",
    "t14_calculate_metrics(true_labels=t_label_column, predictions=t_pred_column)[\"overall\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'macro_precision': 0.801,\n",
       " 'macro_recall': 0.826,\n",
       " 'macro_f1': 0.807,\n",
       " 'support': 800,\n",
       " 'num_errors': 256}"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# n03\n",
    "test_df = pd.read_csv(\n",
    "    f\"/home/yl3427/cylab/selfCorrectionAgent/result/1128_n03_ltm_zs_med42_v2_800.csv\"\n",
    ")\n",
    "test_df.columns\n",
    "\n",
    "n_label_column = test_df[\"n\"]\n",
    "n_pred_column = test_df[\"n03_ltm_zs_n_pred\"]\n",
    "\n",
    "n03_calculate_metrics(true_labels=n_label_column, predictions=n_pred_column)[\"overall\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'macro_precision': 0.838,\n",
       " 'macro_recall': 0.793,\n",
       " 'macro_f1': 0.812,\n",
       " 'support': 800,\n",
       " 'num_errors': 194}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# t14\n",
    "test_df = pd.read_csv(\n",
    "    f\"/home/yl3427/cylab/selfCorrectionAgent/result/1128_t14_ltm_rag1_med42_v2_800.csv\"\n",
    ")\n",
    "test_df.columns\n",
    "\n",
    "t_label_column = test_df[\"t\"]\n",
    "t_pred_column = test_df[\"t14_ltm_rag1_t_pred\"]\n",
    "\n",
    "t14_calculate_metrics(true_labels=t_label_column, predictions=t_pred_column)[\"overall\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'macro_precision': 0.845,\n",
       " 'macro_recall': 0.849,\n",
       " 'macro_f1': 0.846,\n",
       " 'support': 800,\n",
       " 'num_errors': 192}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# n03\n",
    "test_df = pd.read_csv(\n",
    "    f\"/home/yl3427/cylab/selfCorrectionAgent/result/1128_n03_ltm_rag1_med42_v2_800.csv\"\n",
    ")\n",
    "test_df.columns\n",
    "\n",
    "n_label_column = test_df[\"n\"]\n",
    "n_pred_column = test_df[\"n03_ltm_rag1_n_pred\"]\n",
    "\n",
    "n03_calculate_metrics(true_labels=n_label_column, predictions=n_pred_column)[\"overall\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'macro_precision': 0.835,\n",
       " 'macro_recall': 0.772,\n",
       " 'macro_f1': 0.799,\n",
       " 'support': 800,\n",
       " 'num_errors': 220}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# t14\n",
    "test_df = pd.read_csv(\n",
    "    f\"/home/yl3427/cylab/selfCorrectionAgent/result/1128_t14_ltm_rag2_med42_v2_800.csv\"\n",
    ")\n",
    "test_df.columns\n",
    "\n",
    "t_label_column = test_df[\"t\"]\n",
    "t_pred_column = test_df[\"t14_ltm_rag2_t_pred\"]\n",
    "\n",
    "t14_calculate_metrics(true_labels=t_label_column, predictions=t_pred_column)[\"overall\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'macro_precision': 0.766,\n",
       " 'macro_recall': 0.763,\n",
       " 'macro_f1': 0.761,\n",
       " 'support': 800,\n",
       " 'num_errors': 294}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# n03\n",
    "test_df = pd.read_csv(\n",
    "    f\"/home/yl3427/cylab/selfCorrectionAgent/result/1128_n03_ltm_rag2_med42_v2_800.csv\"\n",
    ")\n",
    "test_df.columns\n",
    "\n",
    "n_label_column = test_df[\"n\"]\n",
    "n_pred_column = test_df[\"n03_ltm_rag2_n_pred\"]\n",
    "\n",
    "n03_calculate_metrics(true_labels=n_label_column, predictions=n_pred_column)[\"overall\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### llama3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_df = pd.read_csv(\n",
    "    f\"/home/yl3427/cylab/selfCorrectionAgent/result/1211_t14_llama3_ltm_rag1.csv\"\n",
    ")\n",
    "n_df = pd.read_csv(\n",
    "    f\"/home/yl3427/cylab/selfCorrectionAgent/result/1210_n03_llama3_ltm_rag1.csv\"\n",
    ")\n",
    "\n",
    "t_label_column = t_df[\"t\"]\n",
    "t_pred_column = t_df[\"t14_ltm_rag1_t_pred\"]\n",
    "\n",
    "n_label_column = n_df[\"n\"]\n",
    "n_pred_column = n_df[\"n03_ltm_rag1_n_pred\"]\n",
    "\n",
    "t_results = t14_calculate_metrics(true_labels=t_label_column, predictions=t_pred_column)\n",
    "n_results = n03_calculate_metrics(true_labels=n_label_column, predictions=n_pred_column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'macro_precision': 0.821,\n",
       " 'macro_recall': 0.819,\n",
       " 'macro_f1': 0.816,\n",
       " 'support': 800,\n",
       " 'num_errors': 202}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t_results[\"overall\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'macro_precision': 0.831,\n",
       " 'macro_recall': 0.851,\n",
       " 'macro_f1': 0.833,\n",
       " 'support': 728,\n",
       " 'num_errors': 140}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_with_rules_df = pd.read_csv(\"/home/yl3427/cylab/selfCorrectionAgent/result/1213_test_data_with_refined_ltm.csv\")\n",
    "t14_calculate_metrics(true_labels=test_with_rules_df.t, predictions=test_with_rules_df.t14_test_t_pred)[\"overall\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'macro_precision': 0.849,\n",
       " 'macro_recall': 0.823,\n",
       " 'macro_f1': 0.835,\n",
       " 'support': 728,\n",
       " 'num_errors': 160}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df = pd.read_csv(\"/home/yl3427/cylab/selfCorrectionAgent/result/1213_test_data_base.csv\")\n",
    "t14_calculate_metrics(true_labels=test_df.t, predictions=test_df.t14_test_t_pred)[\"overall\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The divergence in reasoning occurred when determining the size of the tumor. The report states that the tumor measures 5.2 cm by gross measurement, which exceeds the 5 cm threshold for T2 classification. This should have led to the classification of T3, as the tumor size is greater than 5 cm.\n",
      "The divergence in reasoning occurred when determining the T stage based on the tumor size. The report mentions the tumor size as 2.0 cm, which led to the incorrect prediction of T2. However, the correct T stage is T1, indicating that the tumor size is indeed 2.0 cm or less, but the reasoning failed to accurately apply the AJCC criteria.\n",
      "The divergence in reasoning occurred because the initial analysis focused solely on the size of the invasive tumor without fully considering the extent of the tumor and its relationship to the surrounding tissues, as described in the AJCC criteria.\n",
      "The divergence in reasoning occurred because the initial assessment did not accurately account for the multicentric nature of the tumor and the sizes of the individual foci of invasive carcinoma. The report mentions multiple lesions, but the key factor for determining the T stage is the size of the largest invasive carcinoma focus.\n",
      "The divergence in reasoning occurred because the initial analysis focused solely on the size of the primary tumor, which measured 5.8 cm. However, it overlooked the presence of skin ulcerations and the fact that the tumor extended into the skin, which are critical factors in determining the T stage according to the AJCC TNM classification system.\n",
      "The divergence in reasoning occurred when determining the T stage based on the tumor's size and its relationship with nearby structures. The report mentions the tumor is 3.0 cm in greatest dimension and involves the deep nipple tissue and the deep dermis within the skin adjacent to the nipple, but it does not indicate invasion of the chest wall or extensive skin involvement that would classify it as T4.\n",
      "The divergence in reasoning occurred when considering the tumor size and its relation to the T stage classification. The report mentions a maximum microscopic tumor diameter of 1.5 cm, which directly corresponds to the T1c classification according to the AJCC TNM system, as T1c tumors are those more than 1 cm but not more than 2 cm in greatest dimension.\n",
      "The divergence in reasoning occurred when determining the T stage based on the tumor's characteristics. The report mentions the tumor diameter is 3 cm and there's extensive infiltration of the skin but no ulceration. The incorrect prediction of T4 was likely due to misinterpreting the skin infiltration as a criterion for T4, which typically involves ulceration of the skin or fixation to the chest wall.\n",
      "The divergence in reasoning occurred because the initial assessment focused solely on the tumor size (6.5 cm) without fully considering other factors that contribute to the T stage, such as the involvement of the nipple-areolar complex by invasive carcinoma and the presence of extensive vascular invasion. However, according to the AJCC TNM classification system, a tumor is classified as T4 if it involves the chest wall or skin, but involvement of the nipple-areolar complex alone does not automatically classify it as T4. The correct classification of T3 is based on the tumor size being more than 5 cm but not more than 10 cm in its greatest dimension, without evidence of chest wall or skin involvement.\n",
      "The divergence in reasoning occurred because the initial analysis focused solely on the size of the tumor (6 cm) without considering other factors that could affect the T stage, such as the tumor's relationship to the chest wall or skin. However, the key factor missed was the correct interpretation of the tumor size in relation to the AJCC TNM criteria.\n",
      "The reasoning diverged from the correct reasoning because it considered the size of the tumor masses individually, but did not account for the fact that the report mentions the invasive carcinomas range in size from 0.3 cm up to 1.8 cm. The largest dimension of the invasive tumor is actually 1.8 cm, which is less than 2 cm but still within the T1 or T2 range. However, since the report specifically mentions that the largest dimension is up to 1.8 cm and there are multiple foci, the correct approach is to consider the size of the largest single invasive tumor focus, not the cumulative size of multiple foci.\n",
      "The divergence in reasoning occurred when determining the T stage based solely on the size of the tumor. The correct T stage, T3, indicates that the tumor is greater than 5 cm but not greater than 10 cm, however the report already stated the tumor size as [AJCC pT2], which implies the pathologist considered other factors as well.\n",
      "The divergence in reasoning occurred when determining the size of the tumor. The report mentions two masses with sizes 1.1 x 1.0 x 1.0 cm and 1.6 x 1.5 x 1.5 cm, but it also states [AJCC pT1c], which directly refers to the T category based on the tumor size.\n",
      "The divergence in reasoning occurred when considering the size and extent of the tumor for T staging. The initial focus was on the main tumor's size (5.2 cm) without fully considering the implications of the additional tumor nodules and the overall tumor extent as described in the pathology report. The correct T stage, T3, suggests that the tumor's extent or size exceeded what is typically classified as T2, which includes tumors more than 5 cm but not more than 10 cm in greatest dimension. The key factor missed was the involvement of the tumor with the margins and the presence of additional nodules, which, when considered together, may indicate a more extensive tumor than initially assessed.\n",
      "The divergence in reasoning occurred when I relied heavily on the clinical history of 'inflammatory breast ca' to classify the tumor as T4d without considering the lack of specific details in the pathology report that confirm skin involvement characteristic of inflammatory breast cancer, such as edema, erythema, or direct invasion of the skin. The report does mention a fragment of skin attached to the specimen, but it does not explicitly state that the tumor has invaded the skin or caused the characteristic changes of inflammatory breast cancer.\n",
      "The divergence in reasoning occurred when determining the T stage based solely on the size of the tumor. The correct reasoning should have also considered the tumor's proximity to the margin and its potential impact on the T stage classification.\n",
      "The divergence in reasoning occurred when determining the T stage based on the tumor size. The correct T stage is T2, which corresponds to a tumor size greater than 2 cm but not greater than 5 cm. The initial reasoning incorrectly concluded that the tumor size of 3 cm corresponded to a T1 stage, which is defined as a tumor size of 2 cm or less.\n",
      "The divergence in reasoning occurred because the initial analysis focused solely on the size of the tumor and its spread to the chest wall or skin, without fully considering the specific size criteria defined by the AJCC TNM system for each T stage. The report mentions the maximum dimension of the invasive component as 8.3 cm, which directly corresponds to the T3 classification according to the AJCC criteria, as T3 tumors are those more than 5 cm but not more than 10 cm in greatest dimension.\n",
      "The divergence in reasoning occurred because the initial assessment incorrectly applied the criteria for T4, which involves direct extension to the chest wall or skin, excluding the nipple and areola. The report mentions the tumor extends through the nipple into the upper papillary dermis of the skin but does not indicate ulceration or Paget's disease of the nipple, which are specific criteria for T4. The correct T stage, T3, is based on the size of the tumor being greater than 5 cm but not larger than 10 cm, as the report states the tumor measures 7.0 cm.\n",
      "The divergence in reasoning occurred when determining the size of the tumor for T staging. The report mentions the 'Maximum dimension invasive component: 2 cm' and also notes a 'TUMOR AGGREGATE SIZE: Sum of the sizes of multiple invasive tumors: 2.5 cm.' However, for T staging, the AJCC criteria focus on the size of the largest single invasive tumor, not the aggregate size of multiple tumors or the overall size including in situ components.\n",
      "The divergence in reasoning occurred when determining the size of the tumor for T staging. The report mentions two tumors with sizes 1.5 cm and 1.2 cm, and it also mentions the 'tumor aggregate size' as 2.7 cm. However, for multifocal tumors, the AJCC TNM system instructs to measure the size of the largest individual tumor focus, not the aggregate size of all foci, unless the foci are clearly contiguous, which was not indicated in this report.\n",
      "The divergence in reasoning occurred when determining the size of the tumor. The report mentions two tumor sizes: 1.8 cm and 1.2 cm. However, for multifocal tumors, the AJCC TNM system requires considering the largest dimension of the largest tumor focus for T staging.\n",
      "The reasoning diverged from the correct reasoning because it considered the size of the largest tumor (2.0 cm) as the sole determinant for the T stage, without considering the multifocal nature of the tumors and how their sizes should be interpreted according to the AJCC TNM classification system. The correct approach should have considered that when there are multiple foci of tumor, the T category is determined by the largest dimension of the largest focus, but if the foci are all less than or equal to 2 cm and are not more than 5 cm in aggregate, the tumor is classified as T1c if the aggregate size is more than 1 cm but not more than 2 cm, or as T1 if the largest focus is 1 cm or less and the aggregate size does not exceed 1 cm for T1a or is more than 1 cm but not more than 2 cm for T1c.\n",
      "The divergence in reasoning occurred because the initial analysis focused solely on the size of the tumor (6.0 cm) without fully considering the extent of the tumor and its involvement with surrounding tissues as described in the AJCC TNM classification system. Specifically, the report mentions 'PROMINENT VASCULAR SPACE INVOLVEMENT' and 'MICROSCOPIC FOCI OF TUMOR IN VASCULAR SPACES INVOLVE RANDOM SECTIONS OF UPPER OUTER, LOWER INNER AND UPPER INNER QUADRANTS,' which indicates a more extensive disease than initially accounted for.\n",
      "The divergence in reasoning occurred because the initial analysis overemphasized the extent of invasion into surrounding tissues, such as the involvement of skeletal muscle and the presence of angiolymphatic and perineural invasion, which are important for assessing the aggressiveness of the tumor but not directly for determining the T stage based on size. The correct focus should have been on the size of the tumor itself, as stated in the report (4.0 cm), which directly corresponds to the T2 classification according to the AJCC TNM system.\n",
      "The divergence in reasoning occurred because the initial assessment incorrectly prioritized the involvement of the skin (nipple dermis) as an indicator for a T4 classification without properly considering the size of the tumor and the specific criteria for T4 classification as defined by the AJCC.\n",
      "The divergence in reasoning occurred because the initial analysis did not accurately apply the AJCC TNM classification system's criteria for determining the T stage, specifically regarding the size of the tumor. The report mentions the tumor size as 8.0 x 6.5 x 4.2 cm, which directly corresponds to the T3 classification since the tumor is larger than 5 cm but not larger than 10 cm.\n",
      "Upon re-examining the pathology report, I noticed that my previous reasoning diverged from the correct reasoning due to overlooking the actual size of the tumor and its proximity to the skin. The report states that the tumor is sized 3.9 x 6.6 x 2.8 cm and is placed 0.0 cm from the skin, indicating skin involvement.\n",
      "The divergence in reasoning occurred when determining the size of the tumor. The report states the tumor is sized 2.6 x 2 x 1.8 cm, which is larger than the criteria for T1 but fits within the criteria for T2.\n",
      "Upon reviewing my previous response, I realize that I failed to consider the actual size of the tumor and its relationship to the skin, as described in the pathology report. The report mentions a tumor size of 4.3 x 6.2 x 2.2 cm and skin retraction of 3 cm in diameter, which suggests that the tumor is larger than what I initially considered.\n",
      "Upon re-examining the pathology report, I noticed that my initial reasoning diverged from the correct reasoning due to overlooking the actual size of the tumor and its relationship with the skin. The report states that the tumor is sized 6.0 x 6.0 x 3.5 cm and is located 0 cm from the skin, indicating that it is adherent to or invading the skin.\n",
      "The divergence in reasoning occurred because the initial analysis solely focused on the size of the tumor (5.5 x 4 x 4 cm) to determine the T stage, which would indeed suggest a T2 classification based on size alone (since T2 includes tumors more than 2 cm but not more than 5 cm in greatest dimension). However, the correct T stage is T3, indicating that other factors beyond just the tumor size were considered.\n",
      "Upon reviewing the provided pathology report and the actual ground truth T stage (T1), it appears that the divergence in reasoning occurred due to an incorrect interpretation of the tumor size. The report mentions the main tumor mass measuring 1.5 cm, but it also describes the tumor as multifocal with additional scattered foci of invasive lobular carcinoma. However, for the purpose of T staging, the AJCC guidelines specify that the size of the tumor should be based on the largest dimension of the invasive component. In multifocal tumors, if the foci are clearly separate, the largest dimension of the largest focus is used. The report does indicate that the tumor is multifocal but still provides a size measurement of the main tumor mass as 1.5 cm, which might have led to the incorrect assumption of a T2 classification (since T2 is defined as a tumor more than 2 cm but not more than 5 cm in greatest dimension).\n",
      "The divergence in reasoning occurred when determining the T stage based on the tumor size and its extent. The report mentions the tumor size as 2.7 x 0 x 3.2 cm and notes that the tumor extent involves the skin. However, the involvement of the skin alone does not necessarily classify the tumor as T4 without other specific criteria such as ulceration, fixation to the chest wall, or involvement of more than one quadrant with significant skin involvement (e.g., peau d'orange, skin nodules).\n",
      "The reasoning diverged from the correct reasoning because it incorrectly applied the size criteria for T1 and T2. The AJCC TNM classification system defines T1 as a tumor 2 cm or less in greatest dimension, and T2 as a tumor more than 2 cm but not more than 5 cm in greatest dimension. In this case, the tumor size is exactly 2.0 cm, which falls into the T1 category.\n",
      "The divergence in reasoning occurred when determining the size of the main tumor. The report mentions the largest focus of invasive carcinoma measuring 1.2 cm, which directly corresponds to the T1 category according to the AJCC TNM classification system. The incorrect prediction of T2 was likely due to not strictly adhering to the size criteria defined by the AJCC for T staging.\n",
      "The reasoning diverged from the correct reasoning when it considered the size of the largest tumor (2.0 cm) and incorrectly concluded that since it was greater than 2 cm, the T stage would be T2. The correct approach should have focused on the fact that all tumors were less than 2 cm (2.0 cm, 1.9 cm, and 1.1 cm), and since the largest dimension of the largest tumor is indeed 2.0 cm, which is less than or equal to 2 cm, the correct T stage should be T1.\n",
      "The reasoning diverged from the correct reasoning when considering the size of the invasive carcinoma. The report mentions that the largest focus of invasive carcinoma measures 1.3cm, and this measurement should be used to determine the T stage. The mention of multiple foci and the size of the main mass, which includes both invasive carcinoma and DCIS, led to unnecessary complexity in the reasoning. The correct approach should have focused solely on the largest dimension of the invasive carcinoma, which is 1.3cm.\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"/home/yl3427/cylab/selfCorrectionAgent/result/1213_training_data.csv\")\n",
    "for i in df[df['t14_train_t_feedback']==True]['t14_train_t_error_analysis']:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'macro_precision': 0.809,\n",
       " 'macro_recall': 0.713,\n",
       " 'macro_f1': 0.722,\n",
       " 'support': 725,\n",
       " 'num_errors': 206}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"/home/yl3427/cylab/selfCorrectionAgent/result/1214_test_data_with_refined_ltm_mixtral.csv\")\n",
    "df =  df[df[\"t14_test_t_is_parsed\"]]\n",
    "t14_calculate_metrics(true_labels=df.t, predictions=df.t14_test_t_pred)[\"overall\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'macro_precision': 0.839,\n",
       " 'macro_recall': 0.78,\n",
       " 'macro_f1': 0.804,\n",
       " 'support': 727,\n",
       " 'num_errors': 184}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"/home/yl3427/cylab/selfCorrectionAgent/result/1214_test_data_base_mixtral.csv\")\n",
    "df =  df[df[\"t14_test_t_is_parsed\"]]\n",
    "t14_calculate_metrics(true_labels=df.t, predictions=df.t14_test_t_pred)[\"overall\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Re-Run Parsing Error Cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from agent import *\n",
    "from prompt import *\n",
    "from openai import OpenAI\n",
    "import pandas as pd\n",
    "from pydantic import BaseModel, Field\n",
    "from typing import Literal\n",
    "import logging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KEPA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Response_T(BaseModel):\n",
    "    reasoning: str = Field(\n",
    "        description=\"Step-by-step explanation of how you interpreted the report to determine the T stage.\"\n",
    "    )\n",
    "    stage: Literal[\"T1\", \"T2\", \"T3\", \"T4\"] = Field(\n",
    "        description=\"The T stage determined from the report. Stage must be one of 'T1', 'T2', 'T3' or 'T4.'\"\n",
    "    )\n",
    "\n",
    "\n",
    "class Response_N(BaseModel):\n",
    "    reasoning: str = Field(\n",
    "        description=\"Step-by-step explanation of how you interpreted the report to determine the N stage.\"\n",
    "    )\n",
    "    stage: Literal[\"N0\", \"N1\", \"N2\", \"N3\"] = Field(\n",
    "        description=\"The N stage determined from the report. Stage must be one of 'N0', 'N1', 'N2' or 'N3.'\"\n",
    "    )\n",
    "\n",
    "\n",
    "testing_schema_t14 = Response_T.model_json_schema()\n",
    "testing_schema_n03 = Response_N.model_json_schema()\n",
    "\n",
    "client = OpenAI(api_key=\"empty\", base_url=\"http://localhost:8000/v1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_individual_report(\n",
    "    dataset: pd.DataFrame,\n",
    "    patient_filename: str,\n",
    "    memory: str,\n",
    "    label: str,\n",
    "    testing_schema: dict,\n",
    "    model: str = \"m42-health/Llama3-Med42-70B\",\n",
    "):\n",
    "\n",
    "    report = dataset[dataset.patient_filename == patient_filename][\"text\"].values[0]\n",
    "\n",
    "    if label.lower()[0] == \"n\":\n",
    "        prompt = testing_predict_prompt_n03.format(memory=memory, report=report)\n",
    "    else:\n",
    "        prompt = testing_predict_prompt_t14.format(memory=memory, report=report)\n",
    "\n",
    "    filled_prompt = system_instruction + \"\\n\" + prompt\n",
    "    messages = [{\"role\": \"user\", \"content\": filled_prompt}]\n",
    "\n",
    "    response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "        extra_body={\"guided_json\": testing_schema},\n",
    "        temperature=0.1,  # 0.3, 0.5, 0.7, 0.9\n",
    "    )\n",
    "\n",
    "    try:\n",
    "        response = json.loads(response.choices[0].message.content)\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "        return None\n",
    "\n",
    "    dataset.loc[\n",
    "        dataset[\"patient_filename\"] == patient_filename, f\"kepa_{label}_is_parsed\"\n",
    "    ] = True\n",
    "    dataset.loc[\n",
    "        dataset[\"patient_filename\"] == patient_filename, f\"kepa_{label}_ans_str\"\n",
    "    ] = response[\"stage\"]\n",
    "    dataset.loc[\n",
    "        dataset[\"patient_filename\"] == patient_filename, f\"kepa_{label}_reasoning\"\n",
    "    ] = response[\"reasoning\"]\n",
    "\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# T14  [0, 1, 2, 3, 4, 5, 6, 8]\n",
    "for run in [0, 1, 2, 3, 4, 5, 6, 8]:\n",
    "    print(f\"{run}th split\")\n",
    "\n",
    "    # Extract memory for t14\n",
    "    t_train_file_path = (\n",
    "        f\"/home/yl3427/cylab/selfCorrectionAgent/result/t14_memory_dataset{run}.csv\"\n",
    "    )\n",
    "    t_train_data = pd.read_csv(t_train_file_path)\n",
    "\n",
    "    t_memory_dict = {}\n",
    "    for idx, row in t_train_data.iterrows():\n",
    "        t_memory_dict[idx + 1] = row[\"cmem_t_memory_str\"]\n",
    "    t_memory = t_memory_dict.get(40, \"\")  # Use .get to avoid KeyError\n",
    "\n",
    "    test_df = pd.read_csv(\n",
    "        f\"/home/yl3427/cylab/selfCorrectionAgent/result/1114_t14_med42_v2_test_{run}_outof_10runs.csv\"\n",
    "    )\n",
    "    unparsed_df = test_df[\n",
    "        ~test_df[\"kepa_t_is_parsed\"].astype(bool) | test_df[\"kepa_t_ans_str\"].isna()\n",
    "    ]\n",
    "\n",
    "    for idx, row in unparsed_df.iterrows():\n",
    "        # if run == 8 and idx <= 149:\n",
    "        #     continue\n",
    "        patient_filename = row[\"patient_filename\"]\n",
    "        print(f\"Processing patient: {patient_filename} (Index: {idx})\")\n",
    "\n",
    "        print(f\"Before: {row['kepa_t_ans_str']}\")\n",
    "\n",
    "        updated_df = test_individual_report(\n",
    "            dataset=test_df,\n",
    "            patient_filename=patient_filename,\n",
    "            memory=t_memory,\n",
    "            label=\"t\",\n",
    "            testing_schema=testing_schema_t14,\n",
    "        )\n",
    "\n",
    "        if updated_df is None:\n",
    "            print(f\"Failed to process patient: {patient_filename}. Skipping...\")\n",
    "            continue\n",
    "        else:\n",
    "            test_df = updated_df  # Only assign if not None\n",
    "\n",
    "        after_stage = test_df.loc[\n",
    "            test_df[\"patient_filename\"] == patient_filename, \"kepa_t_ans_str\"\n",
    "        ].values\n",
    "        after_reasoning = test_df.loc[\n",
    "            test_df[\"patient_filename\"] == patient_filename, \"kepa_t_reasoning\"\n",
    "        ].values\n",
    "        label_value = test_df.loc[\n",
    "            test_df[\"patient_filename\"] == patient_filename, \"t\"\n",
    "        ].values\n",
    "\n",
    "        print(f\"After Stage: {after_stage}\")\n",
    "        print(f\"After Reasoning: {after_reasoning}\")\n",
    "        print(f\"Label: {label_value}\")\n",
    "\n",
    "        rerun_df_path = f\"/home/yl3427/cylab/selfCorrectionAgent/result/1114_t14_med42_v2_test_{run}_outof_10runs.csv\"\n",
    "        test_df.to_csv(rerun_df_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# N03 [0, 1, 3, 4, 5, 6, 7, 9]\n",
    "for run in [0, 1, 3, 4, 5, 6, 7, 9]:\n",
    "    print(f\"{run}th split\")\n",
    "\n",
    "    # extract memory for t14\n",
    "    n_train_file_path = (\n",
    "        f\"/home/yl3427/cylab/selfCorrectionAgent/result/n03_memory_dataset{run}.csv\"\n",
    "    )\n",
    "    n_train_data = pd.read_csv(n_train_file_path)\n",
    "\n",
    "    n_memory_dict = {}\n",
    "    for idx, row in n_train_data.iterrows():\n",
    "        n_memory_dict[idx + 1] = row[\"cmem_n_memory_str\"]\n",
    "    n_memory = n_memory_dict[40]\n",
    "\n",
    "    test_df = pd.read_csv(\n",
    "        f\"/home/yl3427/cylab/selfCorrectionAgent/result/1114_n03_med42_v2_test_{run}_outof_10runs.csv\"\n",
    "    )\n",
    "    unparsed_df = test_df[\n",
    "        ~test_df[\"kepa_n_is_parsed\"].astype(bool) | test_df[\"kepa_n_ans_str\"].isna()\n",
    "    ]\n",
    "\n",
    "    for idx, row in unparsed_df.iterrows():\n",
    "        # if run == 8 and idx <= 149:\n",
    "        #     continue\n",
    "\n",
    "        patient_filename = row[\"patient_filename\"]\n",
    "        print(f\"Processing patient: {patient_filename} (Index: {idx})\")\n",
    "\n",
    "        print(f\"Before: {row['kepa_n_ans_str']}\")\n",
    "\n",
    "        updated_df = test_individual_report(\n",
    "            dataset=test_df,\n",
    "            patient_filename=patient_filename,\n",
    "            memory=n_memory,\n",
    "            label=\"n\",\n",
    "            testing_schema=testing_schema_n03,\n",
    "        )\n",
    "\n",
    "        if updated_df is None:\n",
    "            print(f\"Failed to process patient: {patient_filename}. Skipping...\")\n",
    "            continue\n",
    "        else:\n",
    "            test_df = updated_df\n",
    "\n",
    "        after_stage = test_df.loc[\n",
    "            test_df[\"patient_filename\"] == patient_filename, \"kepa_n_ans_str\"\n",
    "        ].values\n",
    "        after_reasoning = test_df.loc[\n",
    "            test_df[\"patient_filename\"] == patient_filename, \"kepa_n_reasoning\"\n",
    "        ].values\n",
    "        label_value = test_df.loc[\n",
    "            test_df[\"patient_filename\"] == patient_filename, \"n\"\n",
    "        ].values\n",
    "\n",
    "        print(f\"After Stage: {after_stage}\")\n",
    "        print(f\"After Reasoning: {after_reasoning}\")\n",
    "        print(f\"Label: {label_value}\")\n",
    "\n",
    "        rerun_df_path = f\"/home/yl3427/cylab/selfCorrectionAgent/result/1114_n03_med42_v2_test_{run}_outof_10runs.csv\"\n",
    "        test_df.to_csv(rerun_df_path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ZSCOT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Response_T(BaseModel):\n",
    "    reasoning: str = Field(\n",
    "        description=\"Step-by-step explanation of how you interpreted the report to determine the T stage.\"\n",
    "    )\n",
    "    stage: Literal[\"T1\", \"T2\", \"T3\", \"T4\"] = Field(\n",
    "        description=\"The T stage determined from the report. Stage must be one of 'T1', 'T2', 'T3' or 'T4.'\"\n",
    "    )\n",
    "\n",
    "\n",
    "class Response_N(BaseModel):\n",
    "    reasoning: str = Field(\n",
    "        description=\"Step-by-step explanation of how you interpreted the report to determine the N stage.\"\n",
    "    )\n",
    "    stage: Literal[\"N0\", \"N1\", \"N2\", \"N3\"] = Field(\n",
    "        description=\"The N stage determined from the report. Stage must be one of 'N0', 'N1', 'N2' or 'N3.'\"\n",
    "    )\n",
    "\n",
    "\n",
    "testing_schema_t14 = Response_T.model_json_schema()\n",
    "testing_schema_n03 = Response_N.model_json_schema()\n",
    "\n",
    "client = OpenAI(api_key=\"empty\", base_url=\"http://localhost:8000/v1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_individual_report(\n",
    "    dataset: pd.DataFrame,\n",
    "    patient_filename: str,\n",
    "    label: str,\n",
    "    testing_schema: dict,\n",
    "    model: str = \"m42-health/Llama3-Med42-70B\",\n",
    "):\n",
    "\n",
    "    report = dataset[dataset.patient_filename == patient_filename][\"text\"].values[0]\n",
    "\n",
    "    if label.lower()[0] == \"n\":\n",
    "        prompt = zscot_predict_prompt_n03.format(report=report)\n",
    "    else:\n",
    "        prompt = zscot_predict_prompt_t14.format(report=report)\n",
    "\n",
    "    filled_prompt = prompt_template.format(\n",
    "        system_instruction=system_instruction, prompt=prompt\n",
    "    )\n",
    "    messages = [{\"role\": \"user\", \"content\": filled_prompt}]\n",
    "\n",
    "    response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "        extra_body={\"guided_json\": testing_schema},\n",
    "        temperature=0.1,  # 0.3, 0.5, 0.7, 0.9,\n",
    "        # top_p = 1.0,\n",
    "    )\n",
    "\n",
    "    try:\n",
    "        response = json.loads(response.choices[0].message.content)\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "        return None\n",
    "\n",
    "    dataset.loc[\n",
    "        dataset[\"patient_filename\"] == patient_filename, f\"zscot_{label}_is_parsed\"\n",
    "    ] = True\n",
    "    dataset.loc[\n",
    "        dataset[\"patient_filename\"] == patient_filename, f\"zscot_{label}_ans_str\"\n",
    "    ] = response[\"stage\"]\n",
    "    dataset.loc[\n",
    "        dataset[\"patient_filename\"] == patient_filename, f\"zscot_{label}_reasoning\"\n",
    "    ] = response[\"reasoning\"]\n",
    "\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "### T14\n",
    "test_df = pd.read_csv(\n",
    "    f\"/home/yl3427/cylab/selfCorrectionAgent/result/1118_t14_med42_v2_test_800.csv\"\n",
    ")\n",
    "unparsed_df = test_df[\n",
    "    ~test_df[\"zscot_t_is_parsed\"].astype(bool) | test_df[\"zscot_t_ans_str\"].isna()\n",
    "]\n",
    "\n",
    "for idx, row in unparsed_df.iterrows():\n",
    "    # if idx <= 245:\n",
    "    #     continue\n",
    "\n",
    "    patient_filename = row[\"patient_filename\"]\n",
    "    print(f\"Processing patient: {patient_filename} (Index: {idx})\")\n",
    "\n",
    "    print(f\"Before: {row['zscot_t_ans_str']}\")\n",
    "\n",
    "    updated_df = test_individual_report(\n",
    "        dataset=test_df,\n",
    "        patient_filename=patient_filename,\n",
    "        label=\"t\",\n",
    "        testing_schema=testing_schema_t14,\n",
    "    )\n",
    "\n",
    "    if updated_df is None:\n",
    "        print(f\"Failed to process patient: {patient_filename}. Skipping...\")\n",
    "        continue\n",
    "    else:\n",
    "        test_df = updated_df  # Only assign if not None\n",
    "\n",
    "    after_stage = test_df.loc[\n",
    "        test_df[\"patient_filename\"] == patient_filename, \"zscot_t_ans_str\"\n",
    "    ].values\n",
    "    after_reasoning = test_df.loc[\n",
    "        test_df[\"patient_filename\"] == patient_filename, \"zscot_t_reasoning\"\n",
    "    ].values\n",
    "    label_value = test_df.loc[\n",
    "        test_df[\"patient_filename\"] == patient_filename, \"t\"\n",
    "    ].values\n",
    "\n",
    "    print(f\"After Stage: {after_stage}\")\n",
    "    print(f\"After Reasoning: {after_reasoning}\")\n",
    "    print(f\"Label: {label_value}\")\n",
    "\n",
    "    rerun_df_path = (\n",
    "        f\"/home/yl3427/cylab/selfCorrectionAgent/result/1118_t14_med42_v2_test_800.csv\"\n",
    "    )\n",
    "    test_df.to_csv(rerun_df_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "### N03\n",
    "test_df = pd.read_csv(\n",
    "    f\"/home/yl3427/cylab/selfCorrectionAgent/result/1118_n03_med42_v2_test_800.csv\"\n",
    ")\n",
    "unparsed_df = test_df[\n",
    "    ~test_df[\"zscot_n_is_parsed\"].astype(bool) | test_df[\"zscot_n_ans_str\"].isna()\n",
    "]\n",
    "\n",
    "for idx, row in unparsed_df.iterrows():\n",
    "\n",
    "    patient_filename = row[\"patient_filename\"]\n",
    "    print(f\"Processing patient: {patient_filename} (Index: {idx})\")\n",
    "\n",
    "    print(f\"Before: {row['zscot_n_ans_str']}\")\n",
    "\n",
    "    updated_df = test_individual_report(\n",
    "        dataset=test_df,\n",
    "        patient_filename=patient_filename,\n",
    "        label=\"n\",\n",
    "        testing_schema=testing_schema_n03,\n",
    "    )\n",
    "\n",
    "    if updated_df is None:\n",
    "        print(f\"Failed to process patient: {patient_filename}. Skipping...\")\n",
    "        continue\n",
    "    else:\n",
    "        test_df = updated_df  # Only assign if not None\n",
    "\n",
    "    after_stage = test_df.loc[\n",
    "        test_df[\"patient_filename\"] == patient_filename, \"zscot_n_ans_str\"\n",
    "    ].values\n",
    "    after_reasoning = test_df.loc[\n",
    "        test_df[\"patient_filename\"] == patient_filename, \"zscot_n_reasoning\"\n",
    "    ].values\n",
    "    label_value = test_df.loc[\n",
    "        test_df[\"patient_filename\"] == patient_filename, \"n\"\n",
    "    ].values\n",
    "\n",
    "    print(f\"After Stage: {after_stage}\")\n",
    "    print(f\"After Reasoning: {after_reasoning}\")\n",
    "    print(f\"Label: {label_value}\")\n",
    "\n",
    "    rerun_df_path = (\n",
    "        f\"/home/yl3427/cylab/selfCorrectionAgent/result/1118_n03_med42_v2_test_800.csv\"\n",
    "    )\n",
    "    test_df.to_csv(rerun_df_path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Response_T(BaseModel):\n",
    "    reasoning: str = Field(\n",
    "        description=\"Step-by-step explanation of how you interpreted the report to determine the T stage.\"\n",
    "    )\n",
    "    stage: Literal[\"T1\", \"T2\", \"T3\", \"T4\"] = Field(\n",
    "        description=\"The T stage determined from the report. Stage must be one of 'T1', 'T2', 'T3' or 'T4.'\"\n",
    "    )\n",
    "\n",
    "\n",
    "class Response_N(BaseModel):\n",
    "    reasoning: str = Field(\n",
    "        description=\"Step-by-step explanation of how you interpreted the report to determine the N stage.\"\n",
    "    )\n",
    "    stage: Literal[\"N0\", \"N1\", \"N2\", \"N3\"] = Field(\n",
    "        description=\"The N stage determined from the report. Stage must be one of 'N0', 'N1', 'N2' or 'N3.'\"\n",
    "    )\n",
    "\n",
    "\n",
    "testing_schema_t14 = Response_T.model_json_schema()\n",
    "testing_schema_n03 = Response_N.model_json_schema()\n",
    "\n",
    "client = OpenAI(api_key=\"empty\", base_url=\"http://localhost:8000/v1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_individual_report(\n",
    "    dataset: pd.DataFrame,\n",
    "    patient_filename: str,\n",
    "    label: str,\n",
    "    model: str = \"m42-health/Llama3-Med42-70B\",\n",
    "):\n",
    "\n",
    "    report = dataset[dataset.patient_filename == patient_filename][\"text\"].values[0]\n",
    "\n",
    "    if label.lower()[0] == \"n\":\n",
    "        prompt = rag_n03.format(report=report)\n",
    "        test_name = \"n03_rag_raw\"\n",
    "        testing_schema = testing_schema_n03\n",
    "    else:\n",
    "        prompt = rag_t14.format(report=report)\n",
    "        test_name = \"t14_rag_raw\"\n",
    "        testing_schema = testing_schema_t14\n",
    "\n",
    "    filled_prompt = prompt_template_med42.format(\n",
    "        system_instruction=system_instruction, prompt=prompt\n",
    "    )\n",
    "    messages = [{\"role\": \"user\", \"content\": filled_prompt}]\n",
    "\n",
    "    response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "        extra_body={\"guided_json\": testing_schema},\n",
    "        temperature=0.1,  # 0.3, 0.5, 0.7, 0.9,\n",
    "        # top_p = 1.0,\n",
    "    )\n",
    "\n",
    "    try:\n",
    "        response = json.loads(response.choices[0].message.content)\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "        return None\n",
    "\n",
    "    dataset.loc[\n",
    "        dataset[\"patient_filename\"] == patient_filename,\n",
    "        f\"{test_name}_{label}_is_parsed\",\n",
    "    ] = True\n",
    "    dataset.loc[\n",
    "        dataset[\"patient_filename\"] == patient_filename, f\"{test_name}_{label}_pred\"\n",
    "    ] = response[\"stage\"]\n",
    "    dataset.loc[\n",
    "        dataset[\"patient_filename\"] == patient_filename,\n",
    "        f\"{test_name}_{label}_reasoning\",\n",
    "    ] = response[\"reasoning\"]\n",
    "\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### T14\n",
    "test_df = pd.read_csv(\n",
    "    f\"/home/yl3427/cylab/selfCorrectionAgent/result/1120_t14_rag_raw_med42_v2_800.csv\"\n",
    ")\n",
    "unparsed_df = test_df[\n",
    "    ~test_df[\"t14_rag_raw_t_is_parsed\"].astype(bool)\n",
    "    | test_df[\"t14_rag_raw_t_pred\"].isna()\n",
    "]\n",
    "\n",
    "for idx, row in unparsed_df.iterrows():\n",
    "    # if idx <= 245:\n",
    "    #     continue\n",
    "\n",
    "    patient_filename = row[\"patient_filename\"]\n",
    "    print(f\"Processing patient: {patient_filename} (Index: {idx})\")\n",
    "\n",
    "    print(f\"Before: {row['t14_rag_raw_t_pred']}\")\n",
    "\n",
    "    updated_df = test_individual_report(\n",
    "        dataset=test_df,\n",
    "        patient_filename=patient_filename,\n",
    "        label=\"t\",\n",
    "    )\n",
    "\n",
    "    if updated_df is None:\n",
    "        print(f\"Failed to process patient: {patient_filename}. Skipping...\")\n",
    "        continue\n",
    "    else:\n",
    "        test_df = updated_df  # Only assign if not None\n",
    "\n",
    "    after_stage = test_df.loc[\n",
    "        test_df[\"patient_filename\"] == patient_filename, \"t14_rag_raw_t_pred\"\n",
    "    ].values\n",
    "    after_reasoning = test_df.loc[\n",
    "        test_df[\"patient_filename\"] == patient_filename, \"t14_rag_raw_t_reasoning\"\n",
    "    ].values\n",
    "    label_value = test_df.loc[\n",
    "        test_df[\"patient_filename\"] == patient_filename, \"t\"\n",
    "    ].values\n",
    "\n",
    "    print(f\"After Stage: {after_stage}\")\n",
    "    print(f\"After Reasoning: {after_reasoning}\")\n",
    "    print(f\"Label: {label_value}\")\n",
    "\n",
    "    rerun_df_path = f\"/home/yl3427/cylab/selfCorrectionAgent/result/1120_t14_rag_raw_med42_v2_800.csv\"\n",
    "    test_df.to_csv(rerun_df_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### N03\n",
    "test_df = pd.read_csv(\n",
    "    f\"/home/yl3427/cylab/selfCorrectionAgent/result/1120_n03_rag_raw_med42_v2_800.csv\"\n",
    ")\n",
    "unparsed_df = test_df[\n",
    "    ~test_df[\"n03_rag_raw_n_is_parsed\"].astype(bool)\n",
    "    | test_df[\"n03_rag_raw_n_pred\"].isna()\n",
    "]\n",
    "\n",
    "for idx, row in unparsed_df.iterrows():\n",
    "\n",
    "    patient_filename = row[\"patient_filename\"]\n",
    "    print(f\"Processing patient: {patient_filename} (Index: {idx})\")\n",
    "\n",
    "    print(f\"Before: {row['n03_rag_raw_n_pred']}\")\n",
    "\n",
    "    updated_df = test_individual_report(\n",
    "        dataset=test_df,\n",
    "        patient_filename=patient_filename,\n",
    "        label=\"n\",\n",
    "    )\n",
    "\n",
    "    if updated_df is None:\n",
    "        print(f\"Failed to process patient: {patient_filename}. Skipping...\")\n",
    "        continue\n",
    "    else:\n",
    "        test_df = updated_df  # Only assign if not None\n",
    "\n",
    "    after_stage = test_df.loc[\n",
    "        test_df[\"patient_filename\"] == patient_filename, \"n03_rag_raw_n_pred\"\n",
    "    ].values\n",
    "    after_reasoning = test_df.loc[\n",
    "        test_df[\"patient_filename\"] == patient_filename, \"n03_rag_raw_n_reasoning\"\n",
    "    ].values\n",
    "    label_value = test_df.loc[\n",
    "        test_df[\"patient_filename\"] == patient_filename, \"n\"\n",
    "    ].values\n",
    "\n",
    "    print(f\"After Stage: {after_stage}\")\n",
    "    print(f\"After Reasoning: {after_reasoning}\")\n",
    "    print(f\"Label: {label_value}\")\n",
    "\n",
    "    rerun_df_path = f\"/home/yl3427/cylab/selfCorrectionAgent/result/1120_n03_rag_raw_med42_v2_800.csv\"\n",
    "    test_df.to_csv(rerun_df_path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Response_T(BaseModel):\n",
    "    reasoning: str = Field(\n",
    "        description=\"Step-by-step explanation of how you interpreted the report to determine the T stage.\"\n",
    "    )\n",
    "    stage: Literal[\"T1\", \"T2\", \"T3\", \"T4\"] = Field(\n",
    "        description=\"The T stage determined from the report. Stage must be one of 'T1', 'T2', 'T3' or 'T4.'\"\n",
    "    )\n",
    "\n",
    "\n",
    "class Response_N(BaseModel):\n",
    "    reasoning: str = Field(\n",
    "        description=\"Step-by-step explanation of how you interpreted the report to determine the N stage.\"\n",
    "    )\n",
    "    stage: Literal[\"N0\", \"N1\", \"N2\", \"N3\"] = Field(\n",
    "        description=\"The N stage determined from the report. Stage must be one of 'N0', 'N1', 'N2' or 'N3.'\"\n",
    "    )\n",
    "\n",
    "\n",
    "testing_schema_t14 = Response_T.model_json_schema()\n",
    "testing_schema_n03 = Response_N.model_json_schema()\n",
    "\n",
    "client = OpenAI(api_key=\"empty\", base_url=\"http://localhost:8000/v1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../context.json\", \"r\") as f:\n",
    "    context = json.load(f)\n",
    "\n",
    "rag_raw_t14 = context[\"rag_raw_t14\"]\n",
    "rag_raw_n03 = context[\"rag_raw_n03\"]\n",
    "ltm_zs_t14 = context[\"ltm_zs_t14\"]\n",
    "ltm_zs_n03 = context[\"ltm_zs_n03\"]\n",
    "ltm_rag1_t14 = context[\"ltm_rag1_t14\"]\n",
    "ltm_rag1_n03 = context[\"ltm_rag1_n03\"]\n",
    "ltm_rag2_t14 = context[\"ltm_rag2_t14\"]\n",
    "ltm_rag2_n03 = context[\"ltm_rag2_n03\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_individual_report(\n",
    "    dataset: pd.DataFrame,\n",
    "    patient_filename: str,\n",
    "    label: str,\n",
    "    model: str = \"m42-health/Llama3-Med42-70B\",\n",
    "):\n",
    "\n",
    "    report = dataset[dataset.patient_filename == patient_filename][\"text\"].values[0]\n",
    "\n",
    "    if label.lower()[0] == \"n\":\n",
    "        prompt = ltm_n03.format(report=report, context=ltm_rag1_n03)\n",
    "        test_name = \"n03_ltm_rag1\"\n",
    "        testing_schema = testing_schema_n03\n",
    "    else:\n",
    "        prompt = ltm_t14.format(report=report, context=ltm_rag1_t14)\n",
    "        test_name = \"t14_ltm_rag1\"\n",
    "        testing_schema = testing_schema_t14\n",
    "\n",
    "    filled_prompt = prompt_template_med42.format(\n",
    "        system_instruction=system_instruction, prompt=prompt\n",
    "    )\n",
    "    messages = [{\"role\": \"user\", \"content\": filled_prompt}]\n",
    "\n",
    "    response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "        extra_body={\"guided_json\": testing_schema},\n",
    "        temperature=0.1,  # 0.3, 0.5, 0.7, 0.9,\n",
    "        # top_p = 1.0,\n",
    "    )\n",
    "\n",
    "    try:\n",
    "        response = json.loads(response.choices[0].message.content)\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "        return None\n",
    "\n",
    "    dataset.loc[\n",
    "        dataset[\"patient_filename\"] == patient_filename,\n",
    "        f\"{test_name}_{label}_is_parsed\",\n",
    "    ] = True\n",
    "    dataset.loc[\n",
    "        dataset[\"patient_filename\"] == patient_filename, f\"{test_name}_{label}_pred\"\n",
    "    ] = response[\"stage\"]\n",
    "    dataset.loc[\n",
    "        dataset[\"patient_filename\"] == patient_filename,\n",
    "        f\"{test_name}_{label}_reasoning\",\n",
    "    ] = response[\"reasoning\"]\n",
    "\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "### T14\n",
    "test_df = pd.read_csv(\n",
    "    f\"/home/yl3427/cylab/selfCorrectionAgent/result/1128_t14_ltm_rag1_med42_v2_800.csv\"\n",
    ")\n",
    "unparsed_df = test_df[\n",
    "    (~test_df[\"t14_ltm_rag1_t_is_parsed\"].astype(bool))\n",
    "    | (test_df[\"t14_ltm_rag1_t_pred\"].isna())\n",
    "]\n",
    "\n",
    "for idx, row in unparsed_df.iterrows():\n",
    "\n",
    "    patient_filename = row[\"patient_filename\"]\n",
    "    print(f\"Processing patient: {patient_filename} (Index: {idx})\")\n",
    "\n",
    "    print(f\"Before: {row['t14_ltm_rag1_t_pred']}\")\n",
    "\n",
    "    updated_df = test_individual_report(\n",
    "        dataset=test_df,\n",
    "        patient_filename=patient_filename,\n",
    "        label=\"t\",\n",
    "    )\n",
    "\n",
    "    if updated_df is None:\n",
    "        print(f\"Failed to process patient: {patient_filename}. Skipping...\")\n",
    "        continue\n",
    "    else:\n",
    "        test_df = updated_df  # Only assign if not None\n",
    "\n",
    "    after_stage = test_df.loc[\n",
    "        test_df[\"patient_filename\"] == patient_filename, \"t14_ltm_rag1_t_pred\"\n",
    "    ].values\n",
    "    after_reasoning = test_df.loc[\n",
    "        test_df[\"patient_filename\"] == patient_filename, \"t14_ltm_rag1_t_reasoning\"\n",
    "    ].values\n",
    "    label_value = test_df.loc[\n",
    "        test_df[\"patient_filename\"] == patient_filename, \"t\"\n",
    "    ].values\n",
    "\n",
    "    print(f\"After Stage: {after_stage}\")\n",
    "    print(f\"After Reasoning: {after_reasoning}\")\n",
    "    print(f\"Label: {label_value}\")\n",
    "\n",
    "    rerun_df_path = f\"/home/yl3427/cylab/selfCorrectionAgent/result/1128_t14_ltm_rag1_med42_v2_800.csv\"\n",
    "    test_df.to_csv(rerun_df_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "### N03\n",
    "test_df = pd.read_csv(\n",
    "    f\"/home/yl3427/cylab/selfCorrectionAgent/result/1128_n03_ltm_rag1_med42_v2_800.csv\"\n",
    ")\n",
    "unparsed_df = test_df[\n",
    "    (~test_df[\"n03_ltm_rag1_n_is_parsed\"].astype(bool))\n",
    "    | (test_df[\"n03_ltm_rag1_n_pred\"].isna())\n",
    "]\n",
    "\n",
    "for idx, row in unparsed_df.iterrows():\n",
    "\n",
    "    patient_filename = row[\"patient_filename\"]\n",
    "    print(f\"Processing patient: {patient_filename} (Index: {idx})\")\n",
    "\n",
    "    print(f\"Before: {row['n03_ltm_rag1_n_pred']}\")\n",
    "\n",
    "    updated_df = test_individual_report(\n",
    "        dataset=test_df,\n",
    "        patient_filename=patient_filename,\n",
    "        label=\"n\",\n",
    "    )\n",
    "\n",
    "    if updated_df is None:\n",
    "        print(f\"Failed to process patient: {patient_filename}. Skipping...\")\n",
    "        continue\n",
    "    else:\n",
    "        test_df = updated_df  # Only assign if not None\n",
    "\n",
    "    after_stage = test_df.loc[\n",
    "        test_df[\"patient_filename\"] == patient_filename, \"n03_ltm_rag1_n_pred\"\n",
    "    ].values\n",
    "    after_reasoning = test_df.loc[\n",
    "        test_df[\"patient_filename\"] == patient_filename, \"n03_ltm_rag1_n_reasoning\"\n",
    "    ].values\n",
    "    label_value = test_df.loc[\n",
    "        test_df[\"patient_filename\"] == patient_filename, \"n\"\n",
    "    ].values\n",
    "\n",
    "    print(f\"After Stage: {after_stage}\")\n",
    "    print(f\"After Reasoning: {after_reasoning}\")\n",
    "    print(f\"Label: {label_value}\")\n",
    "\n",
    "    rerun_df_path = f\"/home/yl3427/cylab/selfCorrectionAgent/result/1128_n03_ltm_rag1_med42_v2_800.csv\"\n",
    "    test_df.to_csv(rerun_df_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "for run in [0, 1, 2, 3, 4, 5, 6, 8]:\n",
    "    test_df = pd.read_csv(\n",
    "            f\"/home/yl3427/cylab/selfCorrectionAgent/result/1208_t14_llama3_kewltm_{run}_outof_10runs.csv\"\n",
    "        )\n",
    "    unparsed_df = test_df[\n",
    "        (~test_df[\"t14_kewltm_t_is_parsed\"].astype(bool))\n",
    "        | (test_df[\"t14_kewltm_t_pred\"].isna())\n",
    "    ]\n",
    "    print(len(unparsed_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>patient_filename</th>\n",
       "      <th>n</th>\n",
       "      <th>text</th>\n",
       "      <th>n03_kewltm_n_is_parsed</th>\n",
       "      <th>n03_kewltm_n_reasoning</th>\n",
       "      <th>n03_kewltm_n_pred</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [patient_filename, n, text, n03_kewltm_n_is_parsed, n03_kewltm_n_reasoning, n03_kewltm_n_pred]\n",
       "Index: []"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unparsed_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "test_df = pd.read_csv(\n",
    "    f\"/home/yl3427/cylab/selfCorrectionAgent/result/1120_t14_rag_raw_med42_v2_800.csv\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "800"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df[\"t14_rag_raw_t_is_parsed\"].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "from agent import *\n",
    "from prompt import *\n",
    "import pandas as pd\n",
    "from pydantic import BaseModel, Field\n",
    "from typing import Literal\n",
    "import logging\n",
    "\n",
    "client = OpenAI(api_key=\"empty\", base_url=\"http://localhost:8000/v1\")\n",
    "messages = [{\"role\": \"user\", \"content\": \"What is SOAP format?\"}]\n",
    "\n",
    "\n",
    "class Assesssment(BaseModel):\n",
    "    assesssment: str = Field(description=\"Explain the SOAP format.\")\n",
    "\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=\"mistralai/Mixtral-8x7B-Instruct-v0.1\",\n",
    "    messages=messages,\n",
    "    # extra_body={\"guided_json\": Assesssment.model_json_schema()},\n",
    "    temperature=0.1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatCompletionMessage(content=' SOAP (Simple Object Access Protocol) is a messaging protocol used for exchanging structured information in the implementation of web services in computer networks. It is an XML-based protocol that allows remote procedure calls to be made over the internet.\\n\\nA SOAP message consists of three parts:\\n\\n1. Envelope: This is the outermost element of a SOAP message, which identifies the message as a SOAP message and provides information about the message, such as its encoding style and version.\\n2. Header: This is an optional element that contains header information, such as authentication or transaction details.\\n3. Body: This is the main part of the message, which contains the actual data being exchanged. The data is typically encoded in XML format.\\n\\nSOAP messages can be transported using a variety of underlying protocols, such as HTTP, SMTP, or TCP. The use of SOAP enables interoperability between different systems and platforms, allowing them to communicate and exchange data in a standardized way.\\n\\nSOAP has largely been replaced by REST (Representational State Transfer) in modern web services development due to its simplicity and performance advantages. However, SOAP is still used in some legacy systems and specific use cases where its features and capabilities are required.', refusal=None, role='assistant', audio=None, function_call=None, tool_calls=[])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.choices[0].message"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "umls_env_py310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
