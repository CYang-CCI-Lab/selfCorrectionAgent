{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from metrics import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_mean_std(results, cat):\n",
    "    precision_list = [result[cat][\"precision\"] for result in results]\n",
    "    recall_list = [result[cat][\"recall\"] for result in results]\n",
    "    f1_list = [result[cat][\"f1\"] for result in results]\n",
    "    support_list = [result[cat][\"support\"] for result in results]\n",
    "    num_errors_list = [result[cat][\"num_errors\"] for result in results]\n",
    "\n",
    "    mean_precision = sum(precision_list) / len(precision_list)\n",
    "    mean_recall = sum(recall_list) / len(recall_list)\n",
    "    mean_f1 = sum(f1_list) / len(f1_list)\n",
    "\n",
    "    std_precision = (\n",
    "        sum([(x - mean_precision) ** 2 for x in precision_list]) / len(precision_list)\n",
    "    ) ** 0.5\n",
    "    std_recall = (\n",
    "        sum([(x - mean_recall) ** 2 for x in recall_list]) / len(recall_list)\n",
    "    ) ** 0.5\n",
    "    std_f1 = (sum([(x - mean_f1) ** 2 for x in f1_list]) / len(f1_list)) ** 0.5\n",
    "\n",
    "    return {\n",
    "        \"mean_precision\": round(mean_precision, 3),\n",
    "        \"mean_recall\": round(mean_recall, 3),\n",
    "        \"mean_f1\": round(mean_f1, 3),\n",
    "        \"std_precision\": round(std_precision, 3),\n",
    "        \"std_recall\": round(std_recall, 3),\n",
    "        \"std_f1\": round(std_f1, 3),\n",
    "        \"sum_support\": sum(support_list),\n",
    "        \"sum_num_errors\": sum(num_errors_list),\n",
    "        \"raw_mean_precision\": mean_precision,\n",
    "        \"raw_mean_recall\": mean_recall,\n",
    "        \"raw_mean_f1\": mean_f1,\n",
    "    }\n",
    "\n",
    "\n",
    "def output_tabular_performance(results, categories=[\"T1\", \"T2\", \"T3\", \"T4\"]):\n",
    "    precisions = []\n",
    "    recalls = []\n",
    "    f1s = []\n",
    "\n",
    "    for category in categories:\n",
    "        eval = calculate_mean_std(results, category)\n",
    "        print(\n",
    "            \"{} {:.3f}({:.3f}) {:.3f}({:.3f}) {:.3f}({:.3f})\".format(\n",
    "                category,\n",
    "                eval[\"mean_precision\"],\n",
    "                eval[\"std_precision\"],\n",
    "                eval[\"mean_recall\"],\n",
    "                eval[\"std_recall\"],\n",
    "                eval[\"mean_f1\"],\n",
    "                eval[\"std_f1\"],\n",
    "            )\n",
    "        )\n",
    "\n",
    "        # for calculating macro average\n",
    "        precisions.append(eval[\"raw_mean_precision\"])\n",
    "        recalls.append(eval[\"raw_mean_recall\"])\n",
    "        f1s.append(eval[\"raw_mean_f1\"])\n",
    "\n",
    "    print(\n",
    "        \"MacroAvg. {:.3f} {:.3f} {:.3f}\".format(\n",
    "            round(sum(precisions) / len(precisions), 3),\n",
    "            round(sum(recalls) / len(recalls), 3),\n",
    "            round(sum(f1s) / len(f1s), 3),\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# kepa (reported in the draft)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "kepa_t_results = []\n",
    "zs_t_results = []\n",
    "zscot_t_results = []\n",
    "\n",
    "\n",
    "kepa_run_lst = [0, 1, 2, 3, 4, 5, 6, 8]\n",
    "\n",
    "for run in kepa_run_lst:\n",
    "    # print(f\"Run {run}, memory 40\")\n",
    "    pred_column = \"cmem_t_40reports_ans_str\"\n",
    "\n",
    "    t_test_df = pd.read_csv(\n",
    "        f\"/home/yl3427/cylab/selfCorrectionAgent/result/0718_t14_dynamic_test_{run}_outof_10runs.csv\"\n",
    "    ).sort_values(by=\"patient_filename\")\n",
    "    t_zs_df = pd.read_csv(\n",
    "        \"/home/yl3427/cylab/selfCorrectionAgent/result/0716_t14_zs_test_800.csv\"\n",
    "    ).sort_values(by=\"patient_filename\")\n",
    "    t_zscot_df = pd.read_csv(\n",
    "        \"/home/yl3427/cylab/selfCorrectionAgent/result/0716_t14_zscot_test_800.csv\"\n",
    "    ).sort_values(by=\"patient_filename\")\n",
    "\n",
    "    split_ids = t_test_df.patient_filename\n",
    "\n",
    "    label_column = t_test_df[\"t\"]\n",
    "    t_test_pred_df = t_test_df[t_test_df.patient_filename.isin(split_ids)][pred_column]\n",
    "    kepa_t_results.append(\n",
    "        t14_calculate_metrics(true_labels=label_column, predictions=t_test_pred_df)\n",
    "    )\n",
    "\n",
    "    t_zs_pred_df = t_zs_df[t_zs_df.patient_filename.isin(split_ids)][\"zs_t_ans_str\"]\n",
    "    zs_t_results.append(\n",
    "        t14_calculate_metrics(true_labels=label_column, predictions=t_zs_pred_df)\n",
    "    )\n",
    "\n",
    "    t_zscot_pred_df = t_zscot_df[t_zscot_df.patient_filename.isin(split_ids)][\n",
    "        \"zs_t_ans_str\"\n",
    "    ]\n",
    "    zscot_t_results.append(\n",
    "        t14_calculate_metrics(true_labels=label_column, predictions=t_zscot_pred_df)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "T1 0.904(0.017) 0.812(0.040) 0.855(0.018)\n",
      "T2 0.882(0.022) 0.938(0.018) 0.909(0.005)\n",
      "T3 0.834(0.054) 0.810(0.058) 0.818(0.018)\n",
      "T4 0.807(0.082) 0.634(0.038) 0.707(0.029)\n",
      "MacroAvg. 0.857 0.799 0.822\n"
     ]
    }
   ],
   "source": [
    "output_tabular_performance(kepa_t_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "kepa_n_results = []\n",
    "zs_n_results = []\n",
    "zscot_n_results = []\n",
    "\n",
    "kepa_run_lst = [0, 1, 3, 4, 5, 6, 7, 9]\n",
    "\n",
    "for run in kepa_run_lst:\n",
    "    # print(f\"Run {run}, memory 40\")\n",
    "    pred_column = \"cmem_n_40reports_ans_str\"\n",
    "\n",
    "    n_test_df = pd.read_csv(\n",
    "        f\"/home/yl3427/cylab/selfCorrectionAgent/result/0718_n03_dynamic_test_{run}_outof_10runs.csv\"\n",
    "    ).sort_values(by=\"patient_filename\")\n",
    "    n_zs_df = pd.read_csv(\n",
    "        \"/home/yl3427/cylab/selfCorrectionAgent/result/0716_n03_zs_test_800.csv\"\n",
    "    ).sort_values(by=\"patient_filename\")\n",
    "    n_zscot_df = pd.read_csv(\n",
    "        \"/home/yl3427/cylab/selfCorrectionAgent/result/0716_n03_zscot_test_800.csv\"\n",
    "    ).sort_values(by=\"patient_filename\")\n",
    "\n",
    "    split_ids = n_test_df.patient_filename\n",
    "\n",
    "    label_column = n_test_df[\"n\"]\n",
    "    n_test_pred_df = n_test_df[n_test_df.patient_filename.isin(split_ids)][pred_column]\n",
    "    kepa_n_results.append(\n",
    "        n03_calculate_metrics(true_labels=label_column, predictions=n_test_pred_df)\n",
    "    )\n",
    "\n",
    "    n_zs_pred_df = n_zs_df[n_zs_df.patient_filename.isin(split_ids)][\"zs_n_ans_str\"]\n",
    "    zs_n_results.append(\n",
    "        n03_calculate_metrics(true_labels=label_column, predictions=n_zs_pred_df)\n",
    "    )\n",
    "\n",
    "    n_zscot_pred_df = n_zscot_df[n_zscot_df.patient_filename.isin(split_ids)][\n",
    "        \"zs_n_ans_str\"\n",
    "    ]\n",
    "    zscot_n_results.append(\n",
    "        n03_calculate_metrics(true_labels=label_column, predictions=n_zscot_pred_df)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N0 0.944(0.008) 0.952(0.018) 0.948(0.011)\n",
      "N1 0.885(0.020) 0.883(0.026) 0.884(0.010)\n",
      "N2 0.713(0.031) 0.745(0.054) 0.727(0.022)\n",
      "N3 0.886(0.058) 0.784(0.042) 0.830(0.017)\n",
      "MacroAvg. 0.857 0.841 0.847\n"
     ]
    }
   ],
   "source": [
    "output_tabular_performance(kepa_n_results, categories=[\"N0\", \"N1\", \"N2\", \"N3\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "kepa_t_results = []\n",
    "\n",
    "kepa_run_lst = [0, 1, 2, 3, 4, 5, 6, 8]\n",
    "\n",
    "for run in kepa_run_lst:\n",
    "    # print(f\"Run {run}, memory 40\")\n",
    "    pred_column = \"gpt4o_t_stage\"\n",
    "\n",
    "    t_test_df = pd.read_csv(\n",
    "        f\"/home/yl3427/cylab/selfCorrectionAgent/result/1112_t14_gpt_test_{run}_outof_8runs.csv\"\n",
    "    ).sort_values(by=\"patient_filename\")\n",
    "\n",
    "    label_column = t_test_df[\"t\"]\n",
    "    t_test_pred_df = t_test_df[pred_column]\n",
    "    kepa_t_results.append(\n",
    "        t14_calculate_metrics(true_labels=label_column, predictions=t_test_pred_df)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "T1 0.902(0.009) 0.903(0.025) 0.902(0.013)\n",
      "T2 0.935(0.023) 0.939(0.015) 0.936(0.007)\n",
      "T3 0.905(0.048) 0.813(0.080) 0.852(0.039)\n",
      "T4 0.622(0.136) 0.728(0.052) 0.659(0.052)\n",
      "MacroAvg. 0.841 0.846 0.837\n"
     ]
    }
   ],
   "source": [
    "output_tabular_performance(kepa_t_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['patient_filename', 'text', 'n', 'cmem_n_10reports_is_parsed',\n",
       "       'cmem_n_10reasoning', 'cmem_n_10reports_ans_str',\n",
       "       'cmem_n_20reports_is_parsed', 'cmem_n_20reasoning',\n",
       "       'cmem_n_20reports_ans_str', 'cmem_n_30reports_is_parsed',\n",
       "       'cmem_n_30reasoning', 'cmem_n_30reports_ans_str',\n",
       "       'cmem_n_40reports_is_parsed', 'cmem_n_40reasoning',\n",
       "       'cmem_n_40reports_ans_str', 'cmem_n_50reports_is_parsed',\n",
       "       'cmem_n_50reasoning', 'cmem_n_50reports_ans_str',\n",
       "       'cmem_n_60reports_is_parsed', 'cmem_n_60reasoning',\n",
       "       'cmem_n_60reports_ans_str', 'cmem_n_70reports_is_parsed',\n",
       "       'cmem_n_70reasoning', 'cmem_n_70reports_ans_str',\n",
       "       'cmem_n_80reports_is_parsed', 'cmem_n_80reasoning',\n",
       "       'cmem_n_80reports_ans_str', 'cmem_n_90reports_is_parsed',\n",
       "       'cmem_n_90reasoning', 'cmem_n_90reports_ans_str',\n",
       "       'cmem_n_100reports_is_parsed', 'cmem_n_100reasoning',\n",
       "       'cmem_n_100reports_ans_str', 'cmem_n_10reports_reasoning',\n",
       "       'cmem_n_20reports_reasoning', 'cmem_n_30reports_reasoning',\n",
       "       'cmem_n_50reports_reasoning', 'cmem_n_80reports_reasoning',\n",
       "       'cmem_n_100reports_reasoning'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_test_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "kepa_n_results = []\n",
    "\n",
    "kepa_run_lst = [0, 1, 3, 4, 5, 6, 7, 9]\n",
    "\n",
    "for run in kepa_run_lst:\n",
    "    # print(f\"Run {run}, memory 40\")\n",
    "    pred_column = \"gpt4o_n_stage\"\n",
    "\n",
    "    n_test_df = pd.read_csv(\n",
    "        f\"/home/yl3427/cylab/selfCorrectionAgent/result/1112_n03_gpt_test_{run}_outof_8runs.csv\"\n",
    "    ).sort_values(by=\"patient_filename\")\n",
    "\n",
    "    label_column = n_test_df[\"n\"]\n",
    "    n_test_pred_df = n_test_df[pred_column]\n",
    "    kepa_n_results.append(\n",
    "        n03_calculate_metrics(true_labels=label_column, predictions=n_test_pred_df)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N0 0.928(0.006) 0.962(0.051) 0.944(0.026)\n",
      "N1 0.921(0.006) 0.875(0.013) 0.897(0.008)\n",
      "N2 0.777(0.110) 0.786(0.034) 0.778(0.077)\n",
      "N3 0.855(0.047) 0.850(0.016) 0.852(0.031)\n",
      "MacroAvg. 0.870 0.868 0.868\n"
     ]
    }
   ],
   "source": [
    "output_tabular_performance(kepa_n_results, categories=[\"N0\", \"N1\", \"N2\", \"N3\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# other models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "kepa_t_results = []\n",
    "\n",
    "kepa_run_lst = [0, 1, 2, 3, 4, 5, 6, 8]\n",
    "\n",
    "for run in kepa_run_lst:\n",
    "    # print(f\"Run {run}, memory 40\")\n",
    "    pred_column = \"kepa_t_ans_str\"\n",
    "\n",
    "    t_test_df = pd.read_csv(\n",
    "        f\"/home/yl3427/cylab/selfCorrectionAgent/result/1114_t14_med42_v2_test_{run}_outof_10runs.csv\"\n",
    "    ).sort_values(by=\"patient_filename\")\n",
    "\n",
    "    label_column = t_test_df[\"t\"]\n",
    "    t_test_pred_df = t_test_df[pred_column]\n",
    "    kepa_t_results.append(\n",
    "        t14_calculate_metrics(true_labels=label_column, predictions=t_test_pred_df)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "T1 0.813(0.073) 0.759(0.076) 0.783(0.064)\n",
      "T2 0.855(0.031) 0.913(0.023) 0.882(0.016)\n",
      "T3 0.869(0.063) 0.703(0.099) 0.770(0.065)\n",
      "T4 0.630(0.046) 0.615(0.057) 0.621(0.042)\n",
      "MacroAvg. 0.792 0.747 0.764\n"
     ]
    }
   ],
   "source": [
    "output_tabular_performance(kepa_t_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "kepa_n_results = []\n",
    "\n",
    "kepa_run_lst = [0, 1, 3, 4, 5, 6, 7, 9]\n",
    "\n",
    "for run in kepa_run_lst:\n",
    "    # print(f\"Run {run}, memory 40\")\n",
    "    pred_column = \"kepa_n_ans_str\"\n",
    "\n",
    "    n_test_df = pd.read_csv(\n",
    "        f\"/home/yl3427/cylab/selfCorrectionAgent/result/1114_n03_med42_v2_test_{run}_outof_10runs.csv\"\n",
    "    ).sort_values(by=\"patient_filename\")\n",
    "\n",
    "    label_column = n_test_df[\"n\"]\n",
    "    n_test_pred_df = n_test_df[pred_column]\n",
    "    kepa_n_results.append(\n",
    "        n03_calculate_metrics(true_labels=label_column, predictions=n_test_pred_df)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N0 0.950(0.011) 0.821(0.059) 0.879(0.032)\n",
      "N1 0.775(0.056) 0.821(0.032) 0.795(0.022)\n",
      "N2 0.657(0.067) 0.711(0.076) 0.675(0.018)\n",
      "N3 0.759(0.103) 0.858(0.029) 0.800(0.062)\n",
      "MacroAvg. 0.785 0.803 0.787\n"
     ]
    }
   ],
   "source": [
    "output_tabular_performance(kepa_n_results, categories=[\"N0\", \"N1\", \"N2\", \"N3\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### zscot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "zscot_t_df = pd.read_csv(\n",
    "    f\"/home/yl3427/cylab/selfCorrectionAgent/result/1118_t14_med42_v2_test_800.csv\"\n",
    ")\n",
    "zscot_n_df = pd.read_csv(\n",
    "    f\"/home/yl3427/cylab/selfCorrectionAgent/result/1118_n03_med42_v2_test_800.csv\"\n",
    ")\n",
    "\n",
    "t_label_column = zscot_t_df[\"t\"]\n",
    "t_pred_column = zscot_t_df[\"zscot_t_ans_str\"]\n",
    "\n",
    "n_label_column = zscot_n_df[\"n\"]\n",
    "n_pred_column = zscot_n_df[\"zscot_n_ans_str\"]\n",
    "\n",
    "t_results = t14_calculate_metrics(true_labels=t_label_column, predictions=t_pred_column)\n",
    "n_results = n03_calculate_metrics(true_labels=n_label_column, predictions=n_pred_column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'macro_precision': 0.746,\n",
       " 'macro_recall': 0.678,\n",
       " 'macro_f1': 0.703,\n",
       " 'support': 800,\n",
       " 'num_errors': 370}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t_results[\"overall\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'macro_precision': 0.748,\n",
       " 'macro_recall': 0.723,\n",
       " 'macro_f1': 0.724,\n",
       " 'support': 800,\n",
       " 'num_errors': 420}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_results[\"overall\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'macro_precision': 0.786,\n",
       " 'macro_recall': 0.748,\n",
       " 'macro_f1': 0.764,\n",
       " 'support': 800,\n",
       " 'num_errors': 262}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# t14\n",
    "test_df = pd.read_csv(\n",
    "    f\"/home/yl3427/cylab/selfCorrectionAgent/result/1120_t14_rag_raw_med42_v2_800.csv\"\n",
    ")\n",
    "test_df.columns\n",
    "\n",
    "t_label_column = test_df[\"t\"]\n",
    "t_pred_column = test_df[\"t14_rag_raw_t_pred\"]\n",
    "\n",
    "t14_calculate_metrics(true_labels=t_label_column, predictions=t_pred_column)['overall']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'macro_precision': 0.76,\n",
       " 'macro_recall': 0.799,\n",
       " 'macro_f1': 0.759,\n",
       " 'support': 800,\n",
       " 'num_errors': 336}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# n03\n",
    "test_df = pd.read_csv(\n",
    "    f\"/home/yl3427/cylab/selfCorrectionAgent/result/1120_n03_rag_raw_med42_v2_800.csv\"\n",
    ")\n",
    "test_df.columns\n",
    "\n",
    "n_label_column = test_df[\"n\"]\n",
    "n_pred_column = test_df[\"n03_rag_raw_n_pred\"]\n",
    "\n",
    "n03_calculate_metrics(true_labels=n_label_column, predictions=n_pred_column)['overall']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'macro_precision': 0.837,\n",
       " 'macro_recall': 0.799,\n",
       " 'macro_f1': 0.816,\n",
       " 'support': 800,\n",
       " 'num_errors': 206}"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# t14\n",
    "test_df = pd.read_csv(\n",
    "    f\"/home/yl3427/cylab/selfCorrectionAgent/result/1128_t14_ltm_zs_med42_v2_800.csv\"\n",
    ")\n",
    "\n",
    "t_label_column = test_df[\"t\"]\n",
    "t_pred_column = test_df[\"t14_ltm_zs_t_pred\"]\n",
    "\n",
    "t14_calculate_metrics(true_labels=t_label_column, predictions=t_pred_column)['overall']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'macro_precision': 0.801,\n",
       " 'macro_recall': 0.826,\n",
       " 'macro_f1': 0.807,\n",
       " 'support': 800,\n",
       " 'num_errors': 256}"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# n03\n",
    "test_df = pd.read_csv(\n",
    "    f\"/home/yl3427/cylab/selfCorrectionAgent/result/1128_n03_ltm_zs_med42_v2_800.csv\"\n",
    ")\n",
    "test_df.columns\n",
    "\n",
    "n_label_column = test_df[\"n\"]\n",
    "n_pred_column = test_df[\"n03_ltm_zs_n_pred\"]\n",
    "\n",
    "n03_calculate_metrics(true_labels=n_label_column, predictions=n_pred_column)['overall']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'macro_precision': 0.838,\n",
       " 'macro_recall': 0.793,\n",
       " 'macro_f1': 0.812,\n",
       " 'support': 800,\n",
       " 'num_errors': 194}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# t14\n",
    "test_df = pd.read_csv(\n",
    "    f\"/home/yl3427/cylab/selfCorrectionAgent/result/1128_t14_ltm_rag1_med42_v2_800.csv\"\n",
    ")\n",
    "test_df.columns\n",
    "\n",
    "t_label_column = test_df[\"t\"]\n",
    "t_pred_column = test_df[\"t14_ltm_rag1_t_pred\"]\n",
    "\n",
    "t14_calculate_metrics(true_labels=t_label_column, predictions=t_pred_column)['overall']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'macro_precision': 0.845,\n",
       " 'macro_recall': 0.849,\n",
       " 'macro_f1': 0.846,\n",
       " 'support': 800,\n",
       " 'num_errors': 192}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# n03\n",
    "test_df = pd.read_csv(\n",
    "    f\"/home/yl3427/cylab/selfCorrectionAgent/result/1128_n03_ltm_rag1_med42_v2_800.csv\"\n",
    ")\n",
    "test_df.columns\n",
    "\n",
    "n_label_column = test_df[\"n\"]\n",
    "n_pred_column = test_df[\"n03_ltm_rag1_n_pred\"]\n",
    "\n",
    "n03_calculate_metrics(true_labels=n_label_column, predictions=n_pred_column)['overall']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'macro_precision': 0.835,\n",
       " 'macro_recall': 0.772,\n",
       " 'macro_f1': 0.799,\n",
       " 'support': 800,\n",
       " 'num_errors': 220}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# t14\n",
    "test_df = pd.read_csv(\n",
    "    f\"/home/yl3427/cylab/selfCorrectionAgent/result/1128_t14_ltm_rag2_med42_v2_800.csv\"\n",
    ")\n",
    "test_df.columns\n",
    "\n",
    "t_label_column = test_df[\"t\"]\n",
    "t_pred_column = test_df[\"t14_ltm_rag2_t_pred\"]\n",
    "\n",
    "t14_calculate_metrics(true_labels=t_label_column, predictions=t_pred_column)['overall']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'macro_precision': 0.766,\n",
       " 'macro_recall': 0.763,\n",
       " 'macro_f1': 0.761,\n",
       " 'support': 800,\n",
       " 'num_errors': 294}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# n03\n",
    "test_df = pd.read_csv(\n",
    "    f\"/home/yl3427/cylab/selfCorrectionAgent/result/1128_n03_ltm_rag2_med42_v2_800.csv\"\n",
    ")\n",
    "test_df.columns\n",
    "\n",
    "n_label_column = test_df[\"n\"]\n",
    "n_pred_column = test_df[\"n03_ltm_rag2_n_pred\"]\n",
    "\n",
    "n03_calculate_metrics(true_labels=n_label_column, predictions=n_pred_column)['overall']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Re-Run Parsing Error Cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from agent import *\n",
    "from prompt import *\n",
    "from openai import OpenAI\n",
    "import pandas as pd\n",
    "from pydantic import BaseModel, Field\n",
    "from typing import Literal\n",
    "import logging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KEPA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Response_T(BaseModel):\n",
    "    reasoning: str = Field(\n",
    "        description=\"Step-by-step explanation of how you interpreted the report to determine the T stage.\"\n",
    "    )\n",
    "    stage: Literal[\"T1\", \"T2\", \"T3\", \"T4\"] = Field(\n",
    "        description=\"The T stage determined from the report. Stage must be one of 'T1', 'T2', 'T3' or 'T4.'\"\n",
    "    )\n",
    "\n",
    "\n",
    "class Response_N(BaseModel):\n",
    "    reasoning: str = Field(\n",
    "        description=\"Step-by-step explanation of how you interpreted the report to determine the N stage.\"\n",
    "    )\n",
    "    stage: Literal[\"N0\", \"N1\", \"N2\", \"N3\"] = Field(\n",
    "        description=\"The N stage determined from the report. Stage must be one of 'N0', 'N1', 'N2' or 'N3.'\"\n",
    "    )\n",
    "\n",
    "\n",
    "testing_schema_t14 = Response_T.model_json_schema()\n",
    "testing_schema_n03 = Response_N.model_json_schema()\n",
    "\n",
    "client = OpenAI(api_key=\"empty\", base_url=\"http://localhost:8000/v1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_individual_report(\n",
    "    dataset: pd.DataFrame,\n",
    "    patient_filename: str,\n",
    "    memory: str,\n",
    "    label: str,\n",
    "    testing_schema: dict,\n",
    "    model: str = \"m42-health/Llama3-Med42-70B\",\n",
    "):\n",
    "\n",
    "    report = dataset[dataset.patient_filename == patient_filename][\"text\"].values[0]\n",
    "\n",
    "    if label.lower()[0] == \"n\":\n",
    "        prompt = testing_predict_prompt_n03.format(memory=memory, report=report)\n",
    "    else:\n",
    "        prompt = testing_predict_prompt_t14.format(memory=memory, report=report)\n",
    "\n",
    "    filled_prompt = system_instruction + \"\\n\" + prompt\n",
    "    messages = [{\"role\": \"user\", \"content\": filled_prompt}]\n",
    "\n",
    "    response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "        extra_body={\"guided_json\": testing_schema},\n",
    "        temperature=0.1,  # 0.3, 0.5, 0.7, 0.9\n",
    "    )\n",
    "\n",
    "    try:\n",
    "        response = json.loads(response.choices[0].message.content)\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "        return None\n",
    "\n",
    "    dataset.loc[\n",
    "        dataset[\"patient_filename\"] == patient_filename, f\"kepa_{label}_is_parsed\"\n",
    "    ] = True\n",
    "    dataset.loc[\n",
    "        dataset[\"patient_filename\"] == patient_filename, f\"kepa_{label}_ans_str\"\n",
    "    ] = response[\"stage\"]\n",
    "    dataset.loc[\n",
    "        dataset[\"patient_filename\"] == patient_filename, f\"kepa_{label}_reasoning\"\n",
    "    ] = response[\"reasoning\"]\n",
    "\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# T14  [0, 1, 2, 3, 4, 5, 6, 8]\n",
    "for run in [0, 1, 2, 3, 4, 5, 6, 8]:\n",
    "    print(f\"{run}th split\")\n",
    "\n",
    "    # Extract memory for t14\n",
    "    t_train_file_path = (\n",
    "        f\"/home/yl3427/cylab/selfCorrectionAgent/result/t14_memory_dataset{run}.csv\"\n",
    "    )\n",
    "    t_train_data = pd.read_csv(t_train_file_path)\n",
    "\n",
    "    t_memory_dict = {}\n",
    "    for idx, row in t_train_data.iterrows():\n",
    "        t_memory_dict[idx + 1] = row[\"cmem_t_memory_str\"]\n",
    "    t_memory = t_memory_dict.get(40, \"\")  # Use .get to avoid KeyError\n",
    "\n",
    "    test_df = pd.read_csv(\n",
    "        f\"/home/yl3427/cylab/selfCorrectionAgent/result/1114_t14_med42_v2_test_{run}_outof_10runs.csv\"\n",
    "    )\n",
    "    unparsed_df = test_df[\n",
    "        ~test_df[\"kepa_t_is_parsed\"].astype(bool) | test_df[\"kepa_t_ans_str\"].isna()\n",
    "    ]\n",
    "\n",
    "    for idx, row in unparsed_df.iterrows():\n",
    "        # if run == 8 and idx <= 149:\n",
    "        #     continue\n",
    "        patient_filename = row[\"patient_filename\"]\n",
    "        print(f\"Processing patient: {patient_filename} (Index: {idx})\")\n",
    "\n",
    "        print(f\"Before: {row['kepa_t_ans_str']}\")\n",
    "\n",
    "        updated_df = test_individual_report(\n",
    "            dataset=test_df,\n",
    "            patient_filename=patient_filename,\n",
    "            memory=t_memory,\n",
    "            label=\"t\",\n",
    "            testing_schema=testing_schema_t14,\n",
    "        )\n",
    "\n",
    "        if updated_df is None:\n",
    "            print(f\"Failed to process patient: {patient_filename}. Skipping...\")\n",
    "            continue\n",
    "        else:\n",
    "            test_df = updated_df  # Only assign if not None\n",
    "\n",
    "        after_stage = test_df.loc[\n",
    "            test_df[\"patient_filename\"] == patient_filename, \"kepa_t_ans_str\"\n",
    "        ].values\n",
    "        after_reasoning = test_df.loc[\n",
    "            test_df[\"patient_filename\"] == patient_filename, \"kepa_t_reasoning\"\n",
    "        ].values\n",
    "        label_value = test_df.loc[\n",
    "            test_df[\"patient_filename\"] == patient_filename, \"t\"\n",
    "        ].values\n",
    "\n",
    "        print(f\"After Stage: {after_stage}\")\n",
    "        print(f\"After Reasoning: {after_reasoning}\")\n",
    "        print(f\"Label: {label_value}\")\n",
    "\n",
    "        rerun_df_path = f\"/home/yl3427/cylab/selfCorrectionAgent/result/1114_t14_med42_v2_test_{run}_outof_10runs.csv\"\n",
    "        test_df.to_csv(rerun_df_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# N03 [0, 1, 3, 4, 5, 6, 7, 9]\n",
    "for run in [0, 1, 3, 4, 5, 6, 7, 9]:\n",
    "    print(f\"{run}th split\")\n",
    "\n",
    "    # extract memory for t14\n",
    "    n_train_file_path = (\n",
    "        f\"/home/yl3427/cylab/selfCorrectionAgent/result/n03_memory_dataset{run}.csv\"\n",
    "    )\n",
    "    n_train_data = pd.read_csv(n_train_file_path)\n",
    "\n",
    "    n_memory_dict = {}\n",
    "    for idx, row in n_train_data.iterrows():\n",
    "        n_memory_dict[idx + 1] = row[\"cmem_n_memory_str\"]\n",
    "    n_memory = n_memory_dict[40]\n",
    "\n",
    "    test_df = pd.read_csv(\n",
    "        f\"/home/yl3427/cylab/selfCorrectionAgent/result/1114_n03_med42_v2_test_{run}_outof_10runs.csv\"\n",
    "    )\n",
    "    unparsed_df = test_df[\n",
    "        ~test_df[\"kepa_n_is_parsed\"].astype(bool) | test_df[\"kepa_n_ans_str\"].isna()\n",
    "    ]\n",
    "\n",
    "    for idx, row in unparsed_df.iterrows():\n",
    "        # if run == 8 and idx <= 149:\n",
    "        #     continue\n",
    "\n",
    "        patient_filename = row[\"patient_filename\"]\n",
    "        print(f\"Processing patient: {patient_filename} (Index: {idx})\")\n",
    "\n",
    "        print(f\"Before: {row['kepa_n_ans_str']}\")\n",
    "\n",
    "        updated_df = test_individual_report(\n",
    "            dataset=test_df,\n",
    "            patient_filename=patient_filename,\n",
    "            memory=n_memory,\n",
    "            label=\"n\",\n",
    "            testing_schema=testing_schema_n03,\n",
    "        )\n",
    "\n",
    "        if updated_df is None:\n",
    "            print(f\"Failed to process patient: {patient_filename}. Skipping...\")\n",
    "            continue\n",
    "        else:\n",
    "            test_df = updated_df\n",
    "\n",
    "        after_stage = test_df.loc[\n",
    "            test_df[\"patient_filename\"] == patient_filename, \"kepa_n_ans_str\"\n",
    "        ].values\n",
    "        after_reasoning = test_df.loc[\n",
    "            test_df[\"patient_filename\"] == patient_filename, \"kepa_n_reasoning\"\n",
    "        ].values\n",
    "        label_value = test_df.loc[\n",
    "            test_df[\"patient_filename\"] == patient_filename, \"n\"\n",
    "        ].values\n",
    "\n",
    "        print(f\"After Stage: {after_stage}\")\n",
    "        print(f\"After Reasoning: {after_reasoning}\")\n",
    "        print(f\"Label: {label_value}\")\n",
    "\n",
    "        rerun_df_path = f\"/home/yl3427/cylab/selfCorrectionAgent/result/1114_n03_med42_v2_test_{run}_outof_10runs.csv\"\n",
    "        test_df.to_csv(rerun_df_path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ZSCOT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Response_T(BaseModel):\n",
    "    reasoning: str = Field(\n",
    "        description=\"Step-by-step explanation of how you interpreted the report to determine the T stage.\"\n",
    "    )\n",
    "    stage: Literal[\"T1\", \"T2\", \"T3\", \"T4\"] = Field(\n",
    "        description=\"The T stage determined from the report. Stage must be one of 'T1', 'T2', 'T3' or 'T4.'\"\n",
    "    )\n",
    "\n",
    "\n",
    "class Response_N(BaseModel):\n",
    "    reasoning: str = Field(\n",
    "        description=\"Step-by-step explanation of how you interpreted the report to determine the N stage.\"\n",
    "    )\n",
    "    stage: Literal[\"N0\", \"N1\", \"N2\", \"N3\"] = Field(\n",
    "        description=\"The N stage determined from the report. Stage must be one of 'N0', 'N1', 'N2' or 'N3.'\"\n",
    "    )\n",
    "\n",
    "\n",
    "testing_schema_t14 = Response_T.model_json_schema()\n",
    "testing_schema_n03 = Response_N.model_json_schema()\n",
    "\n",
    "client = OpenAI(api_key=\"empty\", base_url=\"http://localhost:8000/v1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_individual_report(\n",
    "    dataset: pd.DataFrame,\n",
    "    patient_filename: str,\n",
    "    label: str,\n",
    "    testing_schema: dict,\n",
    "    model: str = \"m42-health/Llama3-Med42-70B\",\n",
    "):\n",
    "\n",
    "    report = dataset[dataset.patient_filename == patient_filename][\"text\"].values[0]\n",
    "\n",
    "    if label.lower()[0] == \"n\":\n",
    "        prompt = zscot_predict_prompt_n03.format(report=report)\n",
    "    else:\n",
    "        prompt = zscot_predict_prompt_t14.format(report=report)\n",
    "\n",
    "    filled_prompt = prompt_template.format(\n",
    "        system_instruction=system_instruction, prompt=prompt\n",
    "    )\n",
    "    messages = [{\"role\": \"user\", \"content\": filled_prompt}]\n",
    "\n",
    "    response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "        extra_body={\"guided_json\": testing_schema},\n",
    "        temperature=0.1,  # 0.3, 0.5, 0.7, 0.9,\n",
    "        # top_p = 1.0,\n",
    "    )\n",
    "\n",
    "    try:\n",
    "        response = json.loads(response.choices[0].message.content)\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "        return None\n",
    "\n",
    "    dataset.loc[\n",
    "        dataset[\"patient_filename\"] == patient_filename, f\"zscot_{label}_is_parsed\"\n",
    "    ] = True\n",
    "    dataset.loc[\n",
    "        dataset[\"patient_filename\"] == patient_filename, f\"zscot_{label}_ans_str\"\n",
    "    ] = response[\"stage\"]\n",
    "    dataset.loc[\n",
    "        dataset[\"patient_filename\"] == patient_filename, f\"zscot_{label}_reasoning\"\n",
    "    ] = response[\"reasoning\"]\n",
    "\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "### T14\n",
    "test_df = pd.read_csv(\n",
    "    f\"/home/yl3427/cylab/selfCorrectionAgent/result/1118_t14_med42_v2_test_800.csv\"\n",
    ")\n",
    "unparsed_df = test_df[\n",
    "    ~test_df[\"zscot_t_is_parsed\"].astype(bool) | test_df[\"zscot_t_ans_str\"].isna()\n",
    "]\n",
    "\n",
    "for idx, row in unparsed_df.iterrows():\n",
    "    # if idx <= 245:\n",
    "    #     continue\n",
    "\n",
    "    patient_filename = row[\"patient_filename\"]\n",
    "    print(f\"Processing patient: {patient_filename} (Index: {idx})\")\n",
    "\n",
    "    print(f\"Before: {row['zscot_t_ans_str']}\")\n",
    "\n",
    "    updated_df = test_individual_report(\n",
    "        dataset=test_df,\n",
    "        patient_filename=patient_filename,\n",
    "        label=\"t\",\n",
    "        testing_schema=testing_schema_t14,\n",
    "    )\n",
    "\n",
    "    if updated_df is None:\n",
    "        print(f\"Failed to process patient: {patient_filename}. Skipping...\")\n",
    "        continue\n",
    "    else:\n",
    "        test_df = updated_df  # Only assign if not None\n",
    "\n",
    "    after_stage = test_df.loc[\n",
    "        test_df[\"patient_filename\"] == patient_filename, \"zscot_t_ans_str\"\n",
    "    ].values\n",
    "    after_reasoning = test_df.loc[\n",
    "        test_df[\"patient_filename\"] == patient_filename, \"zscot_t_reasoning\"\n",
    "    ].values\n",
    "    label_value = test_df.loc[\n",
    "        test_df[\"patient_filename\"] == patient_filename, \"t\"\n",
    "    ].values\n",
    "\n",
    "    print(f\"After Stage: {after_stage}\")\n",
    "    print(f\"After Reasoning: {after_reasoning}\")\n",
    "    print(f\"Label: {label_value}\")\n",
    "\n",
    "    rerun_df_path = (\n",
    "        f\"/home/yl3427/cylab/selfCorrectionAgent/result/1118_t14_med42_v2_test_800.csv\"\n",
    "    )\n",
    "    test_df.to_csv(rerun_df_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "### N03\n",
    "test_df = pd.read_csv(\n",
    "    f\"/home/yl3427/cylab/selfCorrectionAgent/result/1118_n03_med42_v2_test_800.csv\"\n",
    ")\n",
    "unparsed_df = test_df[\n",
    "    ~test_df[\"zscot_n_is_parsed\"].astype(bool) | test_df[\"zscot_n_ans_str\"].isna()\n",
    "]\n",
    "\n",
    "for idx, row in unparsed_df.iterrows():\n",
    "\n",
    "    patient_filename = row[\"patient_filename\"]\n",
    "    print(f\"Processing patient: {patient_filename} (Index: {idx})\")\n",
    "\n",
    "    print(f\"Before: {row['zscot_n_ans_str']}\")\n",
    "\n",
    "    updated_df = test_individual_report(\n",
    "        dataset=test_df,\n",
    "        patient_filename=patient_filename,\n",
    "        label=\"n\",\n",
    "        testing_schema=testing_schema_n03,\n",
    "    )\n",
    "\n",
    "    if updated_df is None:\n",
    "        print(f\"Failed to process patient: {patient_filename}. Skipping...\")\n",
    "        continue\n",
    "    else:\n",
    "        test_df = updated_df  # Only assign if not None\n",
    "\n",
    "    after_stage = test_df.loc[\n",
    "        test_df[\"patient_filename\"] == patient_filename, \"zscot_n_ans_str\"\n",
    "    ].values\n",
    "    after_reasoning = test_df.loc[\n",
    "        test_df[\"patient_filename\"] == patient_filename, \"zscot_n_reasoning\"\n",
    "    ].values\n",
    "    label_value = test_df.loc[\n",
    "        test_df[\"patient_filename\"] == patient_filename, \"n\"\n",
    "    ].values\n",
    "\n",
    "    print(f\"After Stage: {after_stage}\")\n",
    "    print(f\"After Reasoning: {after_reasoning}\")\n",
    "    print(f\"Label: {label_value}\")\n",
    "\n",
    "    rerun_df_path = (\n",
    "        f\"/home/yl3427/cylab/selfCorrectionAgent/result/1118_n03_med42_v2_test_800.csv\"\n",
    "    )\n",
    "    test_df.to_csv(rerun_df_path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Response_T(BaseModel):\n",
    "    reasoning: str = Field(\n",
    "        description=\"Step-by-step explanation of how you interpreted the report to determine the T stage.\"\n",
    "    )\n",
    "    stage: Literal[\"T1\", \"T2\", \"T3\", \"T4\"] = Field(\n",
    "        description=\"The T stage determined from the report. Stage must be one of 'T1', 'T2', 'T3' or 'T4.'\"\n",
    "    )\n",
    "\n",
    "\n",
    "class Response_N(BaseModel):\n",
    "    reasoning: str = Field(\n",
    "        description=\"Step-by-step explanation of how you interpreted the report to determine the N stage.\"\n",
    "    )\n",
    "    stage: Literal[\"N0\", \"N1\", \"N2\", \"N3\"] = Field(\n",
    "        description=\"The N stage determined from the report. Stage must be one of 'N0', 'N1', 'N2' or 'N3.'\"\n",
    "    )\n",
    "\n",
    "\n",
    "testing_schema_t14 = Response_T.model_json_schema()\n",
    "testing_schema_n03 = Response_N.model_json_schema()\n",
    "\n",
    "client = OpenAI(api_key=\"empty\", base_url=\"http://localhost:8000/v1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_individual_report(\n",
    "    dataset: pd.DataFrame,\n",
    "    patient_filename: str,\n",
    "    label: str,\n",
    "    model: str = \"m42-health/Llama3-Med42-70B\",\n",
    "):\n",
    "\n",
    "    report = dataset[dataset.patient_filename == patient_filename][\"text\"].values[0]\n",
    "\n",
    "    if label.lower()[0] == \"n\":\n",
    "        prompt = rag_n03.format(report=report)\n",
    "        test_name=\"n03_rag_raw\" \n",
    "        testing_schema = testing_schema_n03\n",
    "    else:\n",
    "        prompt = rag_t14.format(report=report)\n",
    "        test_name=\"t14_rag_raw\"\n",
    "        testing_schema = testing_schema_t14\n",
    "\n",
    "    filled_prompt = prompt_template_med42.format(\n",
    "        system_instruction=system_instruction, prompt=prompt\n",
    "    )\n",
    "    messages = [{\"role\": \"user\", \"content\": filled_prompt}]\n",
    "\n",
    "    response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "        extra_body={\"guided_json\": testing_schema},\n",
    "        temperature=0.1,  # 0.3, 0.5, 0.7, 0.9,\n",
    "        # top_p = 1.0,\n",
    "    )\n",
    "\n",
    "    try:\n",
    "        response = json.loads(response.choices[0].message.content)\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "        return None\n",
    "\n",
    "    dataset.loc[\n",
    "        dataset[\"patient_filename\"] == patient_filename, f\"{test_name}_{label}_is_parsed\"\n",
    "    ] = True\n",
    "    dataset.loc[\n",
    "        dataset[\"patient_filename\"] == patient_filename, f\"{test_name}_{label}_pred\"\n",
    "    ] = response[\"stage\"]\n",
    "    dataset.loc[\n",
    "        dataset[\"patient_filename\"] == patient_filename, f\"{test_name}_{label}_reasoning\"\n",
    "    ] = response[\"reasoning\"]\n",
    "\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### T14\n",
    "test_df = pd.read_csv(\n",
    "    f\"/home/yl3427/cylab/selfCorrectionAgent/result/1120_t14_rag_raw_med42_v2_800.csv\"\n",
    ")\n",
    "unparsed_df = test_df[\n",
    "    ~test_df[\"t14_rag_raw_t_is_parsed\"].astype(bool) | test_df[\"t14_rag_raw_t_pred\"].isna()\n",
    "]\n",
    "\n",
    "for idx, row in unparsed_df.iterrows():\n",
    "    # if idx <= 245:\n",
    "    #     continue\n",
    "\n",
    "    patient_filename = row[\"patient_filename\"]\n",
    "    print(f\"Processing patient: {patient_filename} (Index: {idx})\")\n",
    "\n",
    "    print(f\"Before: {row['t14_rag_raw_t_pred']}\")\n",
    "\n",
    "    updated_df = test_individual_report(\n",
    "        dataset=test_df,\n",
    "        patient_filename=patient_filename,\n",
    "        label=\"t\",\n",
    "    )\n",
    "\n",
    "    if updated_df is None:\n",
    "        print(f\"Failed to process patient: {patient_filename}. Skipping...\")\n",
    "        continue\n",
    "    else:\n",
    "        test_df = updated_df  # Only assign if not None\n",
    "\n",
    "    after_stage = test_df.loc[\n",
    "        test_df[\"patient_filename\"] == patient_filename, \"t14_rag_raw_t_pred\"\n",
    "    ].values\n",
    "    after_reasoning = test_df.loc[\n",
    "        test_df[\"patient_filename\"] == patient_filename, \"t14_rag_raw_t_reasoning\"\n",
    "    ].values\n",
    "    label_value = test_df.loc[\n",
    "        test_df[\"patient_filename\"] == patient_filename, \"t\"\n",
    "    ].values\n",
    "\n",
    "    print(f\"After Stage: {after_stage}\")\n",
    "    print(f\"After Reasoning: {after_reasoning}\")\n",
    "    print(f\"Label: {label_value}\")\n",
    "\n",
    "    rerun_df_path = (\n",
    "        f\"/home/yl3427/cylab/selfCorrectionAgent/result/1120_t14_rag_raw_med42_v2_800.csv\"\n",
    "    )\n",
    "    test_df.to_csv(rerun_df_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### N03\n",
    "test_df = pd.read_csv(\n",
    "    f\"/home/yl3427/cylab/selfCorrectionAgent/result/1120_n03_rag_raw_med42_v2_800.csv\"\n",
    ")\n",
    "unparsed_df = test_df[\n",
    "    ~test_df[\"n03_rag_raw_n_is_parsed\"].astype(bool) | test_df[\"n03_rag_raw_n_pred\"].isna()\n",
    "]\n",
    "\n",
    "for idx, row in unparsed_df.iterrows():\n",
    "\n",
    "    patient_filename = row[\"patient_filename\"]\n",
    "    print(f\"Processing patient: {patient_filename} (Index: {idx})\")\n",
    "\n",
    "    print(f\"Before: {row['n03_rag_raw_n_pred']}\")\n",
    "\n",
    "    updated_df = test_individual_report(\n",
    "        dataset=test_df,\n",
    "        patient_filename=patient_filename,\n",
    "        label=\"n\",\n",
    "    )\n",
    "\n",
    "    if updated_df is None:\n",
    "        print(f\"Failed to process patient: {patient_filename}. Skipping...\")\n",
    "        continue\n",
    "    else:\n",
    "        test_df = updated_df  # Only assign if not None\n",
    "\n",
    "    after_stage = test_df.loc[\n",
    "        test_df[\"patient_filename\"] == patient_filename, \"n03_rag_raw_n_pred\"\n",
    "    ].values\n",
    "    after_reasoning = test_df.loc[\n",
    "        test_df[\"patient_filename\"] == patient_filename, \"n03_rag_raw_n_reasoning\"\n",
    "    ].values\n",
    "    label_value = test_df.loc[\n",
    "        test_df[\"patient_filename\"] == patient_filename, \"n\"\n",
    "    ].values\n",
    "\n",
    "    print(f\"After Stage: {after_stage}\")\n",
    "    print(f\"After Reasoning: {after_reasoning}\")\n",
    "    print(f\"Label: {label_value}\")\n",
    "\n",
    "    rerun_df_path = (\n",
    "        f\"/home/yl3427/cylab/selfCorrectionAgent/result/1120_n03_rag_raw_med42_v2_800.csv\"\n",
    "    )\n",
    "    test_df.to_csv(rerun_df_path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Response_T(BaseModel):\n",
    "    reasoning: str = Field(\n",
    "        description=\"Step-by-step explanation of how you interpreted the report to determine the T stage.\"\n",
    "    )\n",
    "    stage: Literal[\"T1\", \"T2\", \"T3\", \"T4\"] = Field(\n",
    "        description=\"The T stage determined from the report. Stage must be one of 'T1', 'T2', 'T3' or 'T4.'\"\n",
    "    )\n",
    "\n",
    "\n",
    "class Response_N(BaseModel):\n",
    "    reasoning: str = Field(\n",
    "        description=\"Step-by-step explanation of how you interpreted the report to determine the N stage.\"\n",
    "    )\n",
    "    stage: Literal[\"N0\", \"N1\", \"N2\", \"N3\"] = Field(\n",
    "        description=\"The N stage determined from the report. Stage must be one of 'N0', 'N1', 'N2' or 'N3.'\"\n",
    "    )\n",
    "\n",
    "\n",
    "testing_schema_t14 = Response_T.model_json_schema()\n",
    "testing_schema_n03 = Response_N.model_json_schema()\n",
    "\n",
    "client = OpenAI(api_key=\"empty\", base_url=\"http://localhost:8000/v1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../context.json\", \"r\") as f:\n",
    "    context = json.load(f)\n",
    "\n",
    "rag_raw_t14 = context[\"rag_raw_t14\"]\n",
    "rag_raw_n03 = context[\"rag_raw_n03\"]\n",
    "ltm_zs_t14 = context[\"ltm_zs_t14\"]\n",
    "ltm_zs_n03 = context[\"ltm_zs_n03\"]\n",
    "ltm_rag1_t14 = context[\"ltm_rag1_t14\"]\n",
    "ltm_rag1_n03 = context[\"ltm_rag1_n03\"]\n",
    "ltm_rag2_t14 = context[\"ltm_rag2_t14\"]\n",
    "ltm_rag2_n03 = context[\"ltm_rag2_n03\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_individual_report(\n",
    "    dataset: pd.DataFrame,\n",
    "    patient_filename: str,\n",
    "    label: str,\n",
    "    model: str = \"m42-health/Llama3-Med42-70B\",\n",
    "):\n",
    "\n",
    "    report = dataset[dataset.patient_filename == patient_filename][\"text\"].values[0]\n",
    "\n",
    "    if label.lower()[0] == \"n\":\n",
    "        prompt = ltm_n03.format(report=report, context=ltm_rag1_n03)\n",
    "        test_name=\"n03_ltm_rag1\" \n",
    "        testing_schema = testing_schema_n03\n",
    "    else:\n",
    "        prompt = ltm_t14.format(report=report, context=ltm_rag1_t14)\n",
    "        test_name=\"t14_ltm_rag1\"\n",
    "        testing_schema = testing_schema_t14\n",
    "\n",
    "    filled_prompt = prompt_template_med42.format(\n",
    "        system_instruction=system_instruction, prompt=prompt\n",
    "    )\n",
    "    messages = [{\"role\": \"user\", \"content\": filled_prompt}]\n",
    "\n",
    "    response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "        extra_body={\"guided_json\": testing_schema},\n",
    "        temperature=0.1,  # 0.3, 0.5, 0.7, 0.9,\n",
    "        # top_p = 1.0,\n",
    "    )\n",
    "\n",
    "    try:\n",
    "        response = json.loads(response.choices[0].message.content)\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "        return None\n",
    "\n",
    "    dataset.loc[\n",
    "        dataset[\"patient_filename\"] == patient_filename, f\"{test_name}_{label}_is_parsed\"\n",
    "    ] = True\n",
    "    dataset.loc[\n",
    "        dataset[\"patient_filename\"] == patient_filename, f\"{test_name}_{label}_pred\"\n",
    "    ] = response[\"stage\"]\n",
    "    dataset.loc[\n",
    "        dataset[\"patient_filename\"] == patient_filename, f\"{test_name}_{label}_reasoning\"\n",
    "    ] = response[\"reasoning\"]\n",
    "\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "### T14\n",
    "test_df = pd.read_csv(\n",
    "    f\"/home/yl3427/cylab/selfCorrectionAgent/result/1128_t14_ltm_rag1_med42_v2_800.csv\"\n",
    ")\n",
    "unparsed_df = test_df[\n",
    "    (~test_df[\"t14_ltm_rag1_t_is_parsed\"].astype(bool)) | (test_df[\"t14_ltm_rag1_t_pred\"].isna())\n",
    "]\n",
    "\n",
    "for idx, row in unparsed_df.iterrows():\n",
    "\n",
    "    patient_filename = row[\"patient_filename\"]\n",
    "    print(f\"Processing patient: {patient_filename} (Index: {idx})\")\n",
    "\n",
    "    print(f\"Before: {row['t14_ltm_rag1_t_pred']}\")\n",
    "\n",
    "    updated_df = test_individual_report(\n",
    "        dataset=test_df,\n",
    "        patient_filename=patient_filename,\n",
    "        label=\"t\",\n",
    "    )\n",
    "\n",
    "    if updated_df is None:\n",
    "        print(f\"Failed to process patient: {patient_filename}. Skipping...\")\n",
    "        continue\n",
    "    else:\n",
    "        test_df = updated_df  # Only assign if not None\n",
    "\n",
    "    after_stage = test_df.loc[\n",
    "        test_df[\"patient_filename\"] == patient_filename, \"t14_ltm_rag1_t_pred\"\n",
    "    ].values\n",
    "    after_reasoning = test_df.loc[\n",
    "        test_df[\"patient_filename\"] == patient_filename, \"t14_ltm_rag1_t_reasoning\"\n",
    "    ].values\n",
    "    label_value = test_df.loc[\n",
    "        test_df[\"patient_filename\"] == patient_filename, \"t\"\n",
    "    ].values\n",
    "\n",
    "    print(f\"After Stage: {after_stage}\")\n",
    "    print(f\"After Reasoning: {after_reasoning}\")\n",
    "    print(f\"Label: {label_value}\")\n",
    "\n",
    "    rerun_df_path = (\n",
    "        f\"/home/yl3427/cylab/selfCorrectionAgent/result/1128_t14_ltm_rag1_med42_v2_800.csv\"\n",
    "    )\n",
    "    test_df.to_csv(rerun_df_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "### N03\n",
    "test_df = pd.read_csv(\n",
    "    f\"/home/yl3427/cylab/selfCorrectionAgent/result/1128_n03_ltm_rag1_med42_v2_800.csv\"\n",
    ")\n",
    "unparsed_df = test_df[\n",
    "    (~test_df[\"n03_ltm_rag1_n_is_parsed\"].astype(bool)) | (test_df[\"n03_ltm_rag1_n_pred\"].isna())\n",
    "]\n",
    "\n",
    "for idx, row in unparsed_df.iterrows():\n",
    "\n",
    "    patient_filename = row[\"patient_filename\"]\n",
    "    print(f\"Processing patient: {patient_filename} (Index: {idx})\")\n",
    "\n",
    "    print(f\"Before: {row['n03_ltm_rag1_n_pred']}\")\n",
    "\n",
    "    updated_df = test_individual_report(\n",
    "        dataset=test_df,\n",
    "        patient_filename=patient_filename,\n",
    "        label=\"n\",\n",
    "    )\n",
    "\n",
    "    if updated_df is None:\n",
    "        print(f\"Failed to process patient: {patient_filename}. Skipping...\")\n",
    "        continue\n",
    "    else:\n",
    "        test_df = updated_df  # Only assign if not None\n",
    "\n",
    "    after_stage = test_df.loc[\n",
    "        test_df[\"patient_filename\"] == patient_filename, \"n03_ltm_rag1_n_pred\"\n",
    "    ].values\n",
    "    after_reasoning = test_df.loc[\n",
    "        test_df[\"patient_filename\"] == patient_filename, \"n03_ltm_rag1_n_reasoning\"\n",
    "    ].values\n",
    "    label_value = test_df.loc[\n",
    "        test_df[\"patient_filename\"] == patient_filename, \"n\"\n",
    "    ].values\n",
    "\n",
    "    print(f\"After Stage: {after_stage}\")\n",
    "    print(f\"After Reasoning: {after_reasoning}\")\n",
    "    print(f\"Label: {label_value}\")\n",
    "\n",
    "    rerun_df_path = (\n",
    "        f\"/home/yl3427/cylab/selfCorrectionAgent/result/1128_n03_ltm_rag1_med42_v2_800.csv\"\n",
    "    )\n",
    "    test_df.to_csv(rerun_df_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "test_df = pd.read_csv(\n",
    "    f\"/home/yl3427/cylab/selfCorrectionAgent/result/1120_t14_rag_raw_med42_v2_800.csv\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "800"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df[\"t14_rag_raw_t_is_parsed\"].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = OpenAI(api_key=\"empty\", base_url=\"http://localhost:8000/v1\")\n",
    "messages = [{\"role\": \"user\", \"content\": \"What is SOAP format?\"}]\n",
    "\n",
    "class Assesssment(BaseModel):\n",
    "    assesssment: str = Field(\n",
    "        description=\"Explain the SOAP format.\"\n",
    "    )\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=\"m42-health/Llama3-Med42-70B\",\n",
    "    messages=messages,\n",
    "    # extra_body={\"guided_json\": Assesssment.model_json_schema()},\n",
    "    temperature=0.1,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "umls_env_py310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
