{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from metrics import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_mean_std(results, cat):\n",
    "    precision_list = [result[cat]['precision'] for result in results]\n",
    "    recall_list = [result[cat]['recall'] for result in results]\n",
    "    f1_list = [result[cat]['f1'] for result in results]\n",
    "    support_list = [result[cat]['support'] for result in results]\n",
    "    num_errors_list = [result[cat]['num_errors'] for result in results]\n",
    "    \n",
    "    mean_precision = sum(precision_list) / len(precision_list)\n",
    "    mean_recall = sum(recall_list) / len(recall_list)\n",
    "    mean_f1 = sum(f1_list) / len(f1_list)\n",
    "    \n",
    "    std_precision = (sum([(x - mean_precision)**2 for x in precision_list]) / len(precision_list))**0.5\n",
    "    std_recall = (sum([(x - mean_recall)**2 for x in recall_list]) / len(recall_list))**0.5\n",
    "    std_f1 = (sum([(x - mean_f1)**2 for x in f1_list]) / len(f1_list))**0.5\n",
    "    \n",
    "    return {\n",
    "        'mean_precision': round(mean_precision, 3),\n",
    "        'mean_recall': round(mean_recall, 3),\n",
    "        'mean_f1': round(mean_f1, 3),\n",
    "        'std_precision': round(std_precision, 3),\n",
    "        'std_recall': round(std_recall, 3),\n",
    "        'std_f1': round(std_f1, 3),\n",
    "        'sum_support': sum(support_list),\n",
    "        'sum_num_errors': sum(num_errors_list),\n",
    "        'raw_mean_precision': mean_precision,\n",
    "        'raw_mean_recall': mean_recall,\n",
    "        'raw_mean_f1': mean_f1,\n",
    "    }\n",
    "\n",
    "def output_tabular_performance(results, categories = ['T1', 'T2', 'T3', 'T4']):\n",
    "    precisions =[]\n",
    "    recalls = []\n",
    "    f1s = []\n",
    "\n",
    "    for category in categories:\n",
    "        eval = calculate_mean_std(results, category)\n",
    "        print(\"{} {:.3f}({:.3f}) {:.3f}({:.3f}) {:.3f}({:.3f})\".format(category, eval[\"mean_precision\"], eval[\"std_precision\"], eval[\"mean_recall\"], eval[\"std_recall\"], eval[\"mean_f1\"], eval[\"std_f1\"]))\n",
    "        \n",
    "        # for calculating macro average\n",
    "        precisions.append(eval['raw_mean_precision'])\n",
    "        recalls.append(eval['raw_mean_recall'])\n",
    "        f1s.append(eval['raw_mean_f1'])\n",
    "\n",
    "    print(\"MacroAvg. {:.3f} {:.3f} {:.3f}\".format(round(sum(precisions)/len(precisions), 3), round(sum(recalls)/len(recalls), 3), round(sum(f1s)/len(f1s), 3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# kepa (reported in the draft)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "kepa_t_results = []\n",
    "zs_t_results = []\n",
    "zscot_t_results = []\n",
    "\n",
    "\n",
    "kepa_run_lst = [0, 1, 2, 3, 4, 5, 6, 8]\n",
    "\n",
    "for run in kepa_run_lst:\n",
    "    # print(f\"Run {run}, memory 40\")\n",
    "    pred_column = \"cmem_t_40reports_ans_str\"\n",
    "    \n",
    "    t_test_df = pd.read_csv(f\"/home/yl3427/cylab/selfCorrectionAgent/result/0718_t14_dynamic_test_{run}_outof_10runs.csv\").sort_values(by=\"patient_filename\")\n",
    "    t_zs_df = pd.read_csv(\"/home/yl3427/cylab/selfCorrectionAgent/result/0716_t14_zs_test_800.csv\").sort_values(by=\"patient_filename\")\n",
    "    t_zscot_df = pd.read_csv(\"/home/yl3427/cylab/selfCorrectionAgent/result/0716_t14_zscot_test_800.csv\").sort_values(by=\"patient_filename\")\n",
    "\n",
    "    split_ids = t_test_df.patient_filename\n",
    "\n",
    "    label_column = t_test_df['t']\n",
    "    t_test_pred_df = t_test_df[t_test_df.patient_filename.isin(split_ids)][pred_column]\n",
    "    kepa_t_results.append(t14_calculate_metrics(true_labels=label_column, predictions=t_test_pred_df))\n",
    "\n",
    "    t_zs_pred_df = t_zs_df[t_zs_df.patient_filename.isin(split_ids)]['zs_t_ans_str']\n",
    "    zs_t_results.append(t14_calculate_metrics(true_labels=label_column, predictions=t_zs_pred_df))\n",
    "\n",
    "    t_zscot_pred_df = t_zscot_df[t_zscot_df.patient_filename.isin(split_ids)]['zs_t_ans_str']\n",
    "    zscot_t_results.append(t14_calculate_metrics(true_labels=label_column, predictions=t_zscot_pred_df))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "T1 0.904(0.017) 0.812(0.040) 0.855(0.018)\n",
      "T2 0.882(0.022) 0.938(0.018) 0.909(0.005)\n",
      "T3 0.834(0.054) 0.810(0.058) 0.818(0.018)\n",
      "T4 0.807(0.082) 0.634(0.038) 0.707(0.029)\n",
      "MacroAvg. 0.857 0.799 0.822\n"
     ]
    }
   ],
   "source": [
    "output_tabular_performance(kepa_t_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "kepa_n_results = []\n",
    "zs_n_results = []\n",
    "zscot_n_results = []\n",
    "\n",
    "kepa_run_lst = [0, 1, 3, 4, 5, 6, 7, 9]\n",
    "\n",
    "for run in kepa_run_lst:\n",
    "    # print(f\"Run {run}, memory 40\")\n",
    "    pred_column = \"cmem_n_40reports_ans_str\"\n",
    "    \n",
    "    n_test_df = pd.read_csv(f\"/home/yl3427/cylab/selfCorrectionAgent/result/0718_n03_dynamic_test_{run}_outof_10runs.csv\").sort_values(by=\"patient_filename\")\n",
    "    n_zs_df = pd.read_csv(\"/home/yl3427/cylab/selfCorrectionAgent/result/0716_n03_zs_test_800.csv\").sort_values(by=\"patient_filename\")\n",
    "    n_zscot_df = pd.read_csv(\"/home/yl3427/cylab/selfCorrectionAgent/result/0716_n03_zscot_test_800.csv\").sort_values(by=\"patient_filename\")\n",
    "\n",
    "    split_ids = n_test_df.patient_filename\n",
    "\n",
    "    label_column = n_test_df['n']\n",
    "    n_test_pred_df = n_test_df[n_test_df.patient_filename.isin(split_ids)][pred_column]\n",
    "    kepa_n_results.append(n03_calculate_metrics(true_labels=label_column, predictions=n_test_pred_df))\n",
    "\n",
    "    n_zs_pred_df = n_zs_df[n_zs_df.patient_filename.isin(split_ids)]['zs_n_ans_str']\n",
    "    zs_n_results.append(n03_calculate_metrics(true_labels=label_column, predictions=n_zs_pred_df))\n",
    "\n",
    "    n_zscot_pred_df = n_zscot_df[n_zscot_df.patient_filename.isin(split_ids)]['zs_n_ans_str']\n",
    "    zscot_n_results.append(n03_calculate_metrics(true_labels=label_column, predictions=n_zscot_pred_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N0 0.944(0.008) 0.952(0.018) 0.948(0.011)\n",
      "N1 0.885(0.020) 0.883(0.026) 0.884(0.010)\n",
      "N2 0.713(0.031) 0.745(0.054) 0.727(0.022)\n",
      "N3 0.886(0.058) 0.784(0.042) 0.830(0.017)\n",
      "MacroAvg. 0.857 0.841 0.847\n"
     ]
    }
   ],
   "source": [
    "output_tabular_performance(kepa_n_results, categories=['N0', 'N1', 'N2', 'N3'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['patient_filename', 't', 'text', 'gpt4o_t_reasoning', 'gpt4o_t_stage'], dtype='object')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t_test_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "kepa_t_results = []\n",
    "\n",
    "kepa_run_lst = [0, 1, 2, 3, 4, 5, 6, 8]\n",
    "\n",
    "for run in kepa_run_lst:\n",
    "    # print(f\"Run {run}, memory 40\")\n",
    "    pred_column = 'gpt4o_t_stage'\n",
    "    \n",
    "    t_test_df = pd.read_csv(f\"/home/yl3427/cylab/selfCorrectionAgent/result/1112_t14_gpt_test_{run}_outof_8runs.csv\").sort_values(by=\"patient_filename\")\n",
    "   \n",
    "    label_column = t_test_df['t']\n",
    "    t_test_pred_df = t_test_df[pred_column]\n",
    "    kepa_t_results.append(t14_calculate_metrics(true_labels=label_column, predictions=t_test_pred_df))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "T1 0.902(0.009) 0.903(0.025) 0.902(0.013)\n",
      "T2 0.935(0.023) 0.939(0.015) 0.936(0.007)\n",
      "T3 0.905(0.048) 0.813(0.080) 0.852(0.039)\n",
      "T4 0.622(0.136) 0.728(0.052) 0.659(0.052)\n",
      "MacroAvg. 0.841 0.846 0.837\n"
     ]
    }
   ],
   "source": [
    "output_tabular_performance(kepa_t_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['patient_filename', 'n', 'text', 'gpt4o_n_reasoning', 'gpt4o_n_stage'], dtype='object')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_test_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "kepa_n_results = []\n",
    "\n",
    "kepa_run_lst = [0, 1, 3, 4, 5, 6, 7, 9]\n",
    "\n",
    "for run in kepa_run_lst:\n",
    "    # print(f\"Run {run}, memory 40\")\n",
    "    pred_column = 'gpt4o_n_stage'\n",
    "    \n",
    "    n_test_df = pd.read_csv(f\"/home/yl3427/cylab/selfCorrectionAgent/result/1112_n03_gpt_test_{run}_outof_8runs.csv\").sort_values(by=\"patient_filename\")\n",
    "    \n",
    "    label_column = n_test_df['n']\n",
    "    n_test_pred_df = n_test_df[pred_column]\n",
    "    kepa_n_results.append(n03_calculate_metrics(true_labels=label_column, predictions=n_test_pred_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N0 0.928(0.006) 0.962(0.051) 0.944(0.026)\n",
      "N1 0.921(0.006) 0.875(0.013) 0.897(0.008)\n",
      "N2 0.777(0.110) 0.786(0.034) 0.778(0.077)\n",
      "N3 0.855(0.047) 0.850(0.016) 0.852(0.031)\n",
      "MacroAvg. 0.870 0.868 0.868\n"
     ]
    }
   ],
   "source": [
    "output_tabular_performance(kepa_n_results, categories=['N0', 'N1', 'N2', 'N3'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# other models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yl3427/miniconda3/envs/umls_env_py310/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import InferenceClient\n",
    "import pandas as pd\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = InferenceClient(model=\"http://127.0.0.1:8081\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"What are the symptoms of diabetes ?\"\n",
    "prompt_template=f'''\n",
    "<|system|>: You are a helpful medical assistant created by M42 Health in the UAE.\n",
    "<|prompter|>:{prompt}\n",
    "<|assistant|>:\n",
    "'''\n",
    "prompt.format(system_instruction='You are a helpful assistant', prompt='What is the name of you, the LLM model?')\n",
    "res = client.text_generation(prompt=prompt_template, do_sample=False, max_new_tokens=1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'str'>\n",
      " The symptoms of diabetes can vary depending on the type of diabetes you have. Here are some common symptoms of both type 1 and type 2 diabetes:\n",
      "\n",
      "\n",
      "1. Frequent urination\n",
      "2. Increased thirst\n",
      "3. Increased hunger\n",
      "4. Unexplained weight loss\n",
      "5. Fatigue\n",
      "6. Blurred vision\n",
      "7. Slow-healing sores\n",
      "8. Frequent infections\n",
      "9. Presence of ketones in the urine (ketonuria)\n",
      "10. Irritability\n",
      "\n",
      "\n",
      "However, it is important to note that not everyone with diabetes will experience all of these symptoms, and some people may have no symptoms at all. This is why regular check-ups and screenings are important for early detection and management of the condition.\n"
     ]
    }
   ],
   "source": [
    "print(type(res))\n",
    "print(res)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "umls_env_py310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
