{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# reasoning check"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### T14"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/home/yl3427/cylab/selfCorrectionAgent/t14_dynamic_test_0_with_reas.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      3\u001b[0m train_data \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/home/yl3427/cylab/selfCorrectionAgent/result/t14_train_0.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      4\u001b[0m memory_idx \u001b[38;5;241m=\u001b[39m []\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(f\"/home/yl3427/cylab/selfCorrectionAgent/t14_dynamic_test_0_with_reas.csv\")\n",
    "\n",
    "train_data = pd.read_csv(f\"/home/yl3427/cylab/selfCorrectionAgent/result/t14_train_0.csv\")\n",
    "memory_idx = []\n",
    "memory_dict = {}\n",
    "for idx, row in train_data.iterrows():\n",
    "    if row[\"cmem_t_is_updated\"] == True:\n",
    "        memory_idx.append(idx)\n",
    "        memory_dict[idx] = row[\"cmem_t_memory_str\"]\n",
    "memory_idx = memory_idx[80::5]\n",
    "\n",
    "# Check for parsing error: OK\n",
    "for i in memory_idx:\n",
    "    if len(df[df[f\"cmem_t_{i}reports_is_parsed\"]==False]) > 0:\n",
    "        print(f\"parsing error at memory {i}\")\n",
    "\n",
    "\n",
    "# gather y-axis data\n",
    "precision_lst = []\n",
    "recall_lst = []\n",
    "f1_lst = []\n",
    "for i in memory_idx:\n",
    "    precision, recall, f1 = t14_performance_report(df, f'cmem_t_{i}reports_ans_str')\n",
    "    precision_lst.append(precision)\n",
    "    recall_lst.append(recall)\n",
    "    f1_lst.append(f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[[\"Unnamed: 0\",\"t\", \"cmem_t_85reports_ans_str\", \"cmem_t_90reports_ans_str\", \"cmem_t_95reports_ans_str\"]]\n",
    "# 5, 7&8(substage), 22, 25\n",
    "# 1154, 822, 923 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zs_df = pd.read_csv(\"/secure/shared_data/rag_tnm_results/t14_results/mixtral_all_brca_luad_zs_results.csv\")\n",
    "filtered_zs_df = zs_df[zs_df[\"patient_filename\"].isin(df['patient_filename'])]\n",
    "filtered_zs_df[['Unnamed: 0', 't', 'ans_str']]\n",
    "# 783, 822, 923, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df[\"Unnamed: 0\"].isin([1154, 822, 923, 783])].to_csv(\"t_reasons.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#85, 90, 95\n",
    "memory_dict[85]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "memory_dict[90]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "memory_dict[95]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### n03"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from src.metrics import *\n",
    "\n",
    "df = pd.read_csv(f\"/home/yl3427/cylab/selfCorrectionAgent/n03_dynamic_test_0_with_reas.csv\")\n",
    "\n",
    "train_data = pd.read_csv(f\"/home/yl3427/cylab/selfCorrectionAgent/result/n03_train_0.csv\")\n",
    "\n",
    "memory_idx = []\n",
    "memory_dict = {}\n",
    "for idx, row in train_data.iterrows():\n",
    "    if row[\"cmem_n_is_updated\"] == True:\n",
    "        memory_idx.append(idx)\n",
    "        memory_dict[idx] = row[\"cmem_n_memory_str\"]\n",
    "memory_idx = memory_idx[80::5]\n",
    "\n",
    "# Check for parsing error: OK\n",
    "for i in memory_idx:\n",
    "    if len(df[df[f\"cmem_n_{i}reports_is_parsed\"]==False]) > 0:\n",
    "        print(f\"parsing error at memory {i}\")\n",
    "\n",
    "df = df[['Unnamed: 0', 'text', 'n', 'cmem_n_89reports_ans_str', 'cmem_n_89reasoning','cmem_n_96reports_ans_str', 'cmem_n_96reasoning']]\n",
    "\n",
    "# gather y-axis data\n",
    "precision_lst = []\n",
    "recall_lst = []\n",
    "f1_lst = []\n",
    "for i in memory_idx:\n",
    "    precision, recall, f1 = n03_performance_report(df, f'cmem_n_{i}reports_ans_str')\n",
    "    precision_lst.append(precision)\n",
    "    recall_lst.append(recall)\n",
    "    f1_lst.append(f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "memory_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "memory_dict[89]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "memory_dict[96]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[[\"Unnamed: 0\",\"n\", \"cmem_n_89reports_ans_str\", \"cmem_n_96reports_ans_str\"]]\n",
    "# 957, 938, 1174, 814"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.metrics import t14_performance_report, n03_performance_report, relax_t14_performance_report, relax_n03_performance_report\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df[\"Unnamed: 0\"].isin([957, 938, 1174, 814])].to_csv(\"n_reasons.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 10 times: Performance Graph (using \"_performance_report\" function, without zs benchmark)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### T14"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from metrics import *\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "split_idx = 10\n",
    "\n",
    "for num in range(split_idx):\n",
    "    result_df = pd.read_csv(f\"/home/yl3427/cylab/selfCorrectionAgent/t14_dynamic_test_{num}_outof_10runs.csv\")\n",
    "    train_df = pd.read_csv(f\"/home/yl3427/cylab/selfCorrectionAgent/result/t14_train_{num}.csv\")\n",
    "\n",
    "    memory_dict = []\n",
    "    for idx, row in train_df.iterrows():\n",
    "        memory_dict.append((idx+1,row['cmem_t_memory_str']))\n",
    "    memory_dict = memory_dict[9::10]\n",
    "\n",
    "    # Check for parsing error: \n",
    "    for i, _ in memory_dict:\n",
    "        if len(result_df[result_df[f\"cmem_t_{i}reports_is_parsed\"]==False]) > 0:\n",
    "            print(f\"parsing error at memory {i}\")\n",
    "            print()\n",
    "            # parsing error at memory 10\n",
    "        \n",
    "    # gather y-axis data\n",
    "    precision_lst = []\n",
    "    recall_lst = []\n",
    "    f1_lst = []\n",
    "\n",
    "    x_idx = []\n",
    "    for i, _ in memory_dict:\n",
    "        x_idx.append(i)\n",
    "        print(f\"memory at {i}\")\n",
    "        # precision, recall, f1 = t14_performance_report(result_df, f'cmem_t_{i}reports_ans_str')\n",
    "        result = t14_calculate_metrics(result_df['t'], result_df[f'cmem_t_{i}reports_ans_str'])['overall']\n",
    "        precision_lst.append(result['macro_precision'])\n",
    "        recall_lst.append(result['macro_recall'])\n",
    "        f1_lst.append(result['macro_f1'])\n",
    "\n",
    "\n",
    "    plt.figure(figsize=(15, 10))\n",
    "\n",
    "    plt.plot(x_idx, precision_lst, label='Memory Precision', color='blue', marker='o')\n",
    "    plt.plot(x_idx, recall_lst, label='Memory Recall', color='green', marker='o')\n",
    "    plt.plot(x_idx, f1_lst, label='Memory F1 Score', color='red', marker='o')\n",
    "\n",
    "    # plt.axhline(y=zs_precision, color='blue', linestyle='--', label='Zero-shot Precision')\n",
    "    # plt.axhline(y=zs_recall, color='green', linestyle='--', label='Zero-shot Recall')\n",
    "    # plt.axhline(y=zs_f1, color='red', linestyle='--', label='Zero-shot F1 Score')\n",
    "\n",
    "    # for i in range(len(memory_idx)):\n",
    "    #     if (precision_lst[i] < zs_precision) and (recall_lst[i] < zs_recall):\n",
    "    #         plt.annotate(f'{memory_idx[i]}', \n",
    "    #                      (memory_idx[i], f1_lst[i]), \n",
    "    #                      textcoords=\"offset points\", \n",
    "    #                      xytext=(0,10), \n",
    "    #                      ha='center', \n",
    "    #                      fontsize=10, \n",
    "    #                      color='red')\n",
    "\n",
    "    plt.xlabel(f'# of Reports for Memory (t14_train_{num}.csv)')\n",
    "    plt.ylabel('Scores')\n",
    "    plt.title(f'Testing Results on 700 test Reports (t14_test_{num}.csv)')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_zs_precision_avg = 0\n",
    "n_zs_recall_avg = 0\n",
    "n_zs_f1_avg = 0\n",
    "\n",
    "n_zs_precision_cumulative = 0\n",
    "n_zs_recall_cumulative = 0\n",
    "n_zs_f1_cumulative = 0\n",
    "\n",
    "for i in range(10):\n",
    "    result_df = pd.read_csv(f\"/home/yl3427/cylab/selfCorrectionAgent/result/t14_test_{i}.csv\").sort_values(by='patient_filename')\n",
    "    t_zs_df = pd.read_csv(\"/secure/shared_data/rag_tnm_results/t14_results/mixtral_all_brca_luad_zs_results.csv\")\n",
    "    t_zs_df = t_zs_df[t_zs_df[\"patient_filename\"].isin(result_df['patient_filename'])].sort_values(by='patient_filename')\n",
    "    t_zs_result = t14_calculate_metrics(result_df['t'], t_zs_df['ans_str'])['overall']\n",
    "    n_zs_precision_avg += t_zs_result['macro_precision']\n",
    "    n_zs_recall_avg += t_zs_result['macro_recall']\n",
    "    n_zs_f1_avg += t_zs_result['macro_f1']\n",
    "\n",
    "    result_df = pd.read_csv(f\"/home/yl3427/cylab/selfCorrectionAgent/result/n03_test_{i}.csv\").sort_values(by='patient_filename') \n",
    "    n_zs_df = pd.read_csv(\"/secure/shared_data/rag_tnm_results/n03_results/mixtral_all_brca_zs_results.csv\")\n",
    "    n_zs_df = n_zs_df[n_zs_df[\"patient_filename\"].isin(result_df['patient_filename'])].sort_values(by='patient_filename')\n",
    "    n_zs_result = n03_calculate_metrics(result_df['n'], n_zs_df['ans_str'])['overall']\n",
    "    n_zs_precision_cumulative += n_zs_result['macro_precision']\n",
    "    n_zs_recall_cumulative += n_zs_result['macro_recall']\n",
    "    n_zs_f1_cumulative += n_zs_result['macro_f1']\n",
    "\n",
    "# average\n",
    "n_zs_precision_avg = n_zs_precision_avg / 10\n",
    "n_zs_recall_avg = n_zs_recall_avg / 10\n",
    "n_zs_f1_avg = n_zs_f1_avg / 10\n",
    "\n",
    "n_zs_precision_avg = n_zs_precision_cumulative / 10\n",
    "n_zs_recall_avg = n_zs_recall_cumulative / 10\n",
    "n_zs_f1_avg = n_zs_f1_cumulative / 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Average (with old metric)\n",
    "\n",
    "from metrics import t14_performance_report\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "precision_cumulative = []\n",
    "recall_cumulative = []\n",
    "f1_cumulative = []\n",
    "\n",
    "n_zs_precision_avg = 0\n",
    "n_zs_recall_avg = 0\n",
    "n_zs_f1_avg = 0\n",
    "\n",
    "x_num = x_idx = np.array(range(1, 11))\n",
    "x_axis = x_num * 10\n",
    "\n",
    "split_idx = 10\n",
    "\n",
    "for num in range(split_idx):\n",
    "    if num == 7:\n",
    "        continue\n",
    "    result_df = pd.read_csv(f\"/home/yl3427/cylab/selfCorrectionAgent/t14_dynamic_test_{num}_outof_10runs.csv\").sort_values(by='patient_filename')\n",
    "    train_df = pd.read_csv(f\"/home/yl3427/cylab/selfCorrectionAgent/result/t14_train_{num}.csv\")\n",
    "\n",
    "    for i in x_axis:\n",
    "        if len(result_df[result_df[f\"cmem_t_{i}reports_is_parsed\"] == False]) > 0:\n",
    "            print(f\"parsing error at memory {i}\")\n",
    "            print()\n",
    "\n",
    "    for i in x_num:\n",
    "        precision, recall, f1 = t14_performance_report(result_df, f'cmem_t_{i*10}reports_ans_str')\n",
    "        if num == 0:\n",
    "            precision_cumulative.append(precision)\n",
    "            recall_cumulative.append(recall)\n",
    "            f1_cumulative.append(f1)\n",
    "        else:\n",
    "            precision_cumulative[i-1] += precision\n",
    "            recall_cumulative[i-1] += recall\n",
    "            f1_cumulative[i-1] += f1\n",
    "\n",
    "    \n",
    "    t_zs_df = pd.read_csv(\"/secure/shared_data/rag_tnm_results/t14_results/mixtral_all_brca_luad_zs_results.csv\")\n",
    "    t_zs_df = t_zs_df[t_zs_df[\"patient_filename\"].isin(result_df['patient_filename'])].sort_values(by='patient_filename')\n",
    "    zs_precision, zs_recall, zs_f1 = t14_performance_report(t_zs_df, 'ans_str')\n",
    "    n_zs_precision_avg += zs_precision\n",
    "    n_zs_recall_avg += zs_recall\n",
    "    n_zs_f1_avg += zs_f1\n",
    "\n",
    "precision_avg = [p / 9 for p in precision_cumulative]\n",
    "recall_avg = [r / 9 for r in recall_cumulative]\n",
    "f1_avg = [f / 9 for f in f1_cumulative]\n",
    "\n",
    "n_zs_precision_avg = n_zs_precision_avg / 9\n",
    "n_zs_recall_avg = n_zs_recall_avg / 9\n",
    "n_zs_f1_avg = n_zs_f1_avg / 9\n",
    "\n",
    "\n",
    "plt.figure(figsize=(15, 10))\n",
    "\n",
    "plt.plot(x_axis, precision_avg, label='Average Memory Precision', color='blue', marker='o')\n",
    "plt.plot(x_axis, recall_avg, label='Average Memory Recall', color='green', marker='o')\n",
    "plt.plot(x_axis, f1_avg, label='Average Memory F1 Score', color='red', marker='o')\n",
    "\n",
    "for i, value in enumerate(precision_avg):\n",
    "    plt.text(x_axis[i], value, f'{value:.3f}', fontsize=9, ha='center', va='bottom', color='blue')\n",
    "for i, value in enumerate(recall_avg):\n",
    "    plt.text(x_axis[i], value, f'{value:.3f}', fontsize=9, ha='center', va='bottom', color='green')\n",
    "for i, value in enumerate(f1_avg):\n",
    "    plt.text(x_axis[i], value, f'{value:.3f}', fontsize=9, ha='center', va='bottom', color='red')\n",
    "\n",
    "plt.axhline(y=n_zs_precision_avg, color='blue', linestyle='--', label='Zero-shot Precision')\n",
    "plt.axhline(y=n_zs_recall_avg, color='green', linestyle='--', label='Zero-shot Recall')\n",
    "plt.axhline(y=n_zs_f1_avg, color='red', linestyle='--', label='Zero-shot F1 Score')\n",
    "\n",
    "plt.text(x_axis[-1] + 2, n_zs_precision_avg, f'{n_zs_precision_avg:.3f}', fontsize=9, ha='left', va='center', color='blue')\n",
    "plt.text(x_axis[-1] + 2, n_zs_recall_avg, f'{n_zs_recall_avg:.3f}', fontsize=9, ha='left', va='center', color='green')\n",
    "plt.text(x_axis[-1] + 2, n_zs_f1_avg, f'{n_zs_f1_avg:.3f}', fontsize=9, ha='left', va='center', color='red')\n",
    "\n",
    "plt.xlabel('# of Reports for Memory')\n",
    "plt.ylabel('Scores')\n",
    "# plt.title(f'{split_idx} Average Testing Results on 700 Test Reports (t14) - old metric')\n",
    "plt.title(f'9 (excluding the 8th file) Average Testing Results on 700 Test Reports (t14) - old metric')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from metrics import t14_performance_report\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Average (with new metric)\n",
    "\n",
    "precision_cumulative = []\n",
    "recall_cumulative = []\n",
    "f1_cumulative = []\n",
    "\n",
    "n_zs_precision_avg = 0\n",
    "n_zs_recall_avg = 0\n",
    "n_zs_f1_avg = 0\n",
    "\n",
    "x_num = x_idx = np.array(range(1, 11))\n",
    "x_axis = x_num * 10\n",
    "\n",
    "for num in range(split_idx):\n",
    "    if num == 7:\n",
    "        continue\n",
    "    result_df = pd.read_csv(f\"/home/yl3427/cylab/selfCorrectionAgent/t14_dynamic_test_{num}_outof_10runs.csv\").sort_values(by='patient_filename')\n",
    "    train_df = pd.read_csv(f\"/home/yl3427/cylab/selfCorrectionAgent/result/t14_train_{num}.csv\")\n",
    "\n",
    "    for i in x_axis:\n",
    "        if len(result_df[result_df[f\"cmem_t_{i}reports_is_parsed\"] == False]) > 0:\n",
    "            print(f\"parsing error at memory {i}\")\n",
    "            print()\n",
    "\n",
    "    for i in x_num:\n",
    "        result = t14_calculate_metrics(result_df['t'], result_df[f'cmem_t_{i*10}reports_ans_str'])['overall']\n",
    "        if num == 0:\n",
    "            precision_cumulative.append(result['macro_precision'])\n",
    "            recall_cumulative.append(result['macro_recall'])\n",
    "            f1_cumulative.append(result['macro_f1'])\n",
    "        else:\n",
    "            precision_cumulative[i-1] += result['macro_precision']\n",
    "            recall_cumulative[i-1] += result['macro_recall']\n",
    "            f1_cumulative[i-1] += result['macro_f1']\n",
    "\n",
    "    t_zs_df = pd.read_csv(\"/secure/shared_data/rag_tnm_results/t14_results/mixtral_all_brca_luad_zs_results.csv\")\n",
    "    t_zs_df = t_zs_df[t_zs_df[\"patient_filename\"].isin(result_df['patient_filename'])].sort_values(by='patient_filename')\n",
    "    t_zs_result = t14_calculate_metrics(t_zs_df['t'], t_zs_df['ans_str'])['overall']\n",
    "    n_zs_precision_avg += t_zs_result['macro_precision']\n",
    "    n_zs_recall_avg += t_zs_result['macro_recall']\n",
    "    n_zs_f1_avg += t_zs_result['macro_f1']\n",
    "\n",
    "# average\n",
    "precision_avg = [p / 9 for p in precision_cumulative]\n",
    "recall_avg = [r / 9 for r in recall_cumulative]\n",
    "f1_avg = [f / 9 for f in f1_cumulative]\n",
    "\n",
    "n_zs_precision_avg = n_zs_precision_avg / 9\n",
    "n_zs_recall_avg = n_zs_recall_avg / 9\n",
    "n_zs_f1_avg = n_zs_f1_avg / 9\n",
    "\n",
    "plt.figure(figsize=(15, 10))\n",
    "\n",
    "plt.plot(x_axis, precision_avg, label='Average Memory Precision', color='blue', marker='o')\n",
    "plt.plot(x_axis, recall_avg, label='Average Memory Recall', color='green', marker='o')\n",
    "plt.plot(x_axis, f1_avg, label='Average Memory F1 Score', color='red', marker='o')\n",
    "\n",
    "for i, value in enumerate(precision_avg):\n",
    "    plt.text(x_axis[i], value, f'{value:.3f}', fontsize=9, ha='center', va='bottom', color='blue')\n",
    "for i, value in enumerate(recall_avg):\n",
    "    plt.text(x_axis[i], value, f'{value:.3f}', fontsize=9, ha='center', va='bottom', color='green')\n",
    "for i, value in enumerate(f1_avg):\n",
    "    plt.text(x_axis[i], value, f'{value:.3f}', fontsize=9, ha='center', va='bottom', color='red')\n",
    "\n",
    "plt.axhline(y=n_zs_precision_avg, color='blue', linestyle='--', label='Zero-shot Precision')\n",
    "plt.axhline(y=n_zs_recall_avg, color='green', linestyle='--', label='Zero-shot Recall')\n",
    "plt.axhline(y=n_zs_f1_avg, color='red', linestyle='--', label='Zero-shot F1 Score')\n",
    "\n",
    "plt.text(x_axis[-1] + 2, n_zs_precision_avg, f'{n_zs_precision_avg:.3f}', fontsize=9, ha='left', va='center', color='blue')\n",
    "plt.text(x_axis[-1] + 2, n_zs_recall_avg, f'{n_zs_recall_avg:.3f}', fontsize=9, ha='left', va='center', color='green')\n",
    "plt.text(x_axis[-1] + 2, n_zs_f1_avg, f'{n_zs_f1_avg:.3f}', fontsize=9, ha='left', va='center', color='red')\n",
    "\n",
    "plt.xlabel('# of Reports for Memory')\n",
    "plt.ylabel('Scores')\n",
    "plt.title(f'9 (excluding the 8th file) Average Testing Results on 700 Test Reports (t14) - new metric')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### N03"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from metrics import *\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "split_idx = 10\n",
    "\n",
    "for num in range(split_idx):\n",
    "    result_df = pd.read_csv(f\"/home/yl3427/cylab/selfCorrectionAgent/n03_dynamic_test_{num}_outof_10runs.csv\")\n",
    "    train_df = pd.read_csv(f\"/home/yl3427/cylab/selfCorrectionAgent/result/n03_train_{num}.csv\")\n",
    "\n",
    "    memory_dict = []\n",
    "    for idx, row in train_df.iterrows():\n",
    "        memory_dict.append((idx+1,row['cmem_n_memory_str']))\n",
    "    memory_dict = memory_dict[9::10]\n",
    "\n",
    "    # Check for parsing error: \n",
    "    for i, _ in memory_dict:\n",
    "        if len(result_df[result_df[f\"cmem_n_{i}reports_is_parsed\"]==False]) > 0:\n",
    "            print(f\"parsing error at memory {i}\")\n",
    "            print()\n",
    "            # parsing error at memory 10\n",
    "        \n",
    "    # gather y-axis data\n",
    "    precision_lst = []\n",
    "    recall_lst = []\n",
    "    f1_lst = []\n",
    "\n",
    "    x_idx = []\n",
    "    for i, _ in memory_dict:\n",
    "        x_idx.append(i)\n",
    "        print(f\"memory at {i}\")\n",
    "        precision, recall, f1 = n03_performance_report(result_df, f'cmem_n_{i}reports_ans_str')\n",
    "        precision_lst.append(precision)\n",
    "        recall_lst.append(recall)\n",
    "        f1_lst.append(f1)\n",
    "\n",
    "\n",
    "    plt.figure(figsize=(15, 10))\n",
    "\n",
    "    plt.plot(x_idx, precision_lst, label='Memory Precision', color='blue', marker='o')\n",
    "    plt.plot(x_idx, recall_lst, label='Memory Recall', color='green', marker='o')\n",
    "    plt.plot(x_idx, f1_lst, label='Memory F1 Score', color='red', marker='o')\n",
    "\n",
    "    # plt.axhline(y=zs_precision, color='blue', linestyle='--', label='Zero-shot Precision')\n",
    "    # plt.axhline(y=zs_recall, color='green', linestyle='--', label='Zero-shot Recall')\n",
    "    # plt.axhline(y=zs_f1, color='red', linestyle='--', label='Zero-shot F1 Score')\n",
    "\n",
    "    # for i in range(len(memory_idx)):\n",
    "    #     if (precision_lst[i] < zs_precision) and (recall_lst[i] < zs_recall):\n",
    "    #         plt.annotate(f'{memory_idx[i]}', \n",
    "    #                      (memory_idx[i], f1_lst[i]), \n",
    "    #                      textcoords=\"offset points\", \n",
    "    #                      xytext=(0,10), \n",
    "    #                      ha='center', \n",
    "    #                      fontsize=10, \n",
    "    #                      color='red')\n",
    "\n",
    "    plt.xlabel(f'# of Reports for Memory (n03_train_{num}.csv)')\n",
    "    plt.ylabel('Scores')\n",
    "    plt.title(f'Testing Results on 700 test Reports (n03_test_{num}.csv)')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Average (with old metric)\n",
    "\n",
    "from metrics import n03_performance_report\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "split_idx = 10\n",
    "\n",
    "precision_cumulative = []\n",
    "recall_cumulative = []\n",
    "f1_cumulative = []\n",
    "\n",
    "n_zs_precision_cumulative = 0\n",
    "n_zs_recall_cumulative = 0\n",
    "n_zs_f1_cumulative = 0\n",
    "\n",
    "x_num = x_idx = np.array(range(1, 11))\n",
    "x_axis = x_num * 10\n",
    "\n",
    "\n",
    "for num in range(split_idx):\n",
    "    result_df = pd.read_csv(f\"/home/yl3427/cylab/selfCorrectionAgent/n03_dynamic_test_{num}_outof_10runs.csv\")\n",
    "    train_df = pd.read_csv(f\"/home/yl3427/cylab/selfCorrectionAgent/result/n03_train_{num}.csv\")\n",
    "\n",
    "    for i in x_axis:\n",
    "        if len(result_df[result_df[f\"cmem_n_{i}reports_is_parsed\"] == False]) > 0:\n",
    "            print(f\"parsing error at memory {i}\")\n",
    "            print()\n",
    "\n",
    "    for i in x_num:\n",
    "        precision, recall, f1 = n03_performance_report(result_df, f'cmem_n_{i*10}reports_ans_str')\n",
    "        if num == 0:\n",
    "            precision_cumulative.append(precision)\n",
    "            recall_cumulative.append(recall)\n",
    "            f1_cumulative.append(f1)\n",
    "        else:\n",
    "            precision_cumulative[i-1] += precision\n",
    "            recall_cumulative[i-1] += recall\n",
    "            f1_cumulative[i-1] += f1\n",
    "\n",
    "    n_zs_df = pd.read_csv(\"/secure/shared_data/rag_tnm_results/n03_results/mixtral_all_brca_zs_results.csv\")\n",
    "    n_zs_df = n_zs_df[n_zs_df[\"patient_filename\"].isin(result_df['patient_filename'])].sort_values(by='patient_filename')\n",
    "    zs_precision, zs_recall, zs_f1 = n03_performance_report(n_zs_df, 'ans_str')\n",
    "    n_zs_precision_cumulative += zs_precision\n",
    "    n_zs_recall_cumulative += zs_recall\n",
    "    n_zs_f1_cumulative += zs_f1\n",
    "\n",
    "\n",
    "precision_avg = [p / split_idx for p in precision_cumulative]\n",
    "recall_avg = [r / split_idx for r in recall_cumulative]\n",
    "f1_avg = [f / split_idx for f in f1_cumulative]\n",
    "\n",
    "n_zs_precision_avg = n_zs_precision_cumulative / 10\n",
    "n_zs_recall_avg = n_zs_recall_cumulative / 10\n",
    "n_zs_f1_avg = n_zs_f1_cumulative / 10\n",
    "\n",
    "plt.figure(figsize=(15, 10))\n",
    "\n",
    "plt.plot(x_axis, precision_avg, label='Average Memory Precision', color='blue', marker='o')\n",
    "plt.plot(x_axis, recall_avg, label='Average Memory Recall', color='green', marker='o')\n",
    "plt.plot(x_axis, f1_avg, label='Average Memory F1 Score', color='red', marker='o')\n",
    "\n",
    "for i, value in enumerate(precision_avg):\n",
    "    plt.text(x_axis[i], value, f'{value:.3f}', fontsize=9, ha='center', va='bottom', color='blue')\n",
    "for i, value in enumerate(recall_avg):\n",
    "    plt.text(x_axis[i], value, f'{value:.3f}', fontsize=9, ha='center', va='bottom', color='green')\n",
    "for i, value in enumerate(f1_avg):\n",
    "    plt.text(x_axis[i], value, f'{value:.3f}', fontsize=9, ha='center', va='bottom', color='red')\n",
    "\n",
    "plt.axhline(y=n_zs_precision_avg, color='blue', linestyle='--', label='Zero-shot Precision')\n",
    "plt.axhline(y=n_zs_recall_avg, color='green', linestyle='--', label='Zero-shot Recall')\n",
    "plt.axhline(y=n_zs_f1_avg, color='red', linestyle='--', label='Zero-shot F1 Score')\n",
    "\n",
    "plt.text(x_axis[-1] + 2, n_zs_precision_avg, f'{n_zs_precision_avg:.3f}', fontsize=9, ha='left', va='center', color='blue')\n",
    "plt.text(x_axis[-1] + 2, n_zs_recall_avg, f'{n_zs_recall_avg:.3f}', fontsize=9, ha='left', va='center', color='green')\n",
    "plt.text(x_axis[-1] + 2, n_zs_f1_avg, f'{n_zs_f1_avg:.3f}', fontsize=9, ha='left', va='center', color='red')\n",
    "\n",
    "\n",
    "plt.xlabel('# of Reports for Memory')\n",
    "plt.ylabel('Scores')\n",
    "plt.title(f'{split_idx} Average Testing Results on 700 Test Reports (n03) - old metric')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Average (with new metric)\n",
    "\n",
    "from metrics import n03_performance_report\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "split_idx = 10\n",
    "\n",
    "precision_cumulative = []\n",
    "recall_cumulative = []\n",
    "f1_cumulative = []\n",
    "\n",
    "n_zs_precision_cumulative = 0\n",
    "n_zs_recall_cumulative = 0\n",
    "n_zs_f1_cumulative = 0\n",
    "\n",
    "x_num = x_idx = np.array(range(1, 11))\n",
    "x_axis = x_num * 10\n",
    "\n",
    "\n",
    "for num in range(split_idx):\n",
    "    result_df = pd.read_csv(f\"/home/yl3427/cylab/selfCorrectionAgent/n03_dynamic_test_{num}_outof_10runs.csv\")\n",
    "    train_df = pd.read_csv(f\"/home/yl3427/cylab/selfCorrectionAgent/result/n03_train_{num}.csv\")\n",
    "\n",
    "    for i in x_axis:\n",
    "        if len(result_df[result_df[f\"cmem_n_{i}reports_is_parsed\"] == False]) > 0:\n",
    "            print(f\"parsing error at memory {i}\")\n",
    "            print()\n",
    "\n",
    "    for i in x_num:\n",
    "        result = n03_calculate_metrics(result_df['n'], result_df[f'cmem_n_{i*10}reports_ans_str'])['overall']\n",
    "        if num == 0:\n",
    "            precision_cumulative.append(result['macro_precision'])\n",
    "            recall_cumulative.append(result['macro_recall'])\n",
    "            f1_cumulative.append(result['macro_f1'])\n",
    "        else:\n",
    "            precision_cumulative[i-1] += result['macro_precision']\n",
    "            recall_cumulative[i-1] += result['macro_recall']\n",
    "            f1_cumulative[i-1] += result['macro_f1']\n",
    "\n",
    "    n_zs_df = pd.read_csv(\"/secure/shared_data/rag_tnm_results/n03_results/mixtral_all_brca_zs_results.csv\")\n",
    "    n_zs_df = n_zs_df[n_zs_df[\"patient_filename\"].isin(result_df['patient_filename'])].sort_values(by='patient_filename')\n",
    "    result = n03_calculate_metrics(n_zs_df['n'], n_zs_df['ans_str'])['overall']\n",
    "    n_zs_precision_cumulative += result['macro_precision']\n",
    "    n_zs_recall_cumulative += result['macro_recall']\n",
    "    n_zs_f1_cumulative += result['macro_f1']\n",
    "\n",
    "\n",
    "precision_avg = [p / split_idx for p in precision_cumulative]\n",
    "recall_avg = [r / split_idx for r in recall_cumulative]\n",
    "f1_avg = [f / split_idx for f in f1_cumulative]\n",
    "\n",
    "n_zs_precision_avg = n_zs_precision_cumulative / 10\n",
    "n_zs_recall_avg = n_zs_recall_cumulative / 10\n",
    "n_zs_f1_avg = n_zs_f1_cumulative / 10\n",
    "\n",
    "plt.figure(figsize=(15, 10))\n",
    "\n",
    "plt.plot(x_axis, precision_avg, label='Average Memory Precision', color='blue', marker='o')\n",
    "plt.plot(x_axis, recall_avg, label='Average Memory Recall', color='green', marker='o')\n",
    "plt.plot(x_axis, f1_avg, label='Average Memory F1 Score', color='red', marker='o')\n",
    "\n",
    "for i, value in enumerate(precision_avg):\n",
    "    plt.text(x_axis[i], value, f'{value:.3f}', fontsize=9, ha='center', va='bottom', color='blue')\n",
    "for i, value in enumerate(recall_avg):\n",
    "    plt.text(x_axis[i], value, f'{value:.3f}', fontsize=9, ha='center', va='bottom', color='green')\n",
    "for i, value in enumerate(f1_avg):\n",
    "    plt.text(x_axis[i], value, f'{value:.3f}', fontsize=9, ha='center', va='bottom', color='red')\n",
    "\n",
    "plt.axhline(y=n_zs_precision_avg, color='blue', linestyle='--', label='Zero-shot Precision')\n",
    "plt.axhline(y=n_zs_recall_avg, color='green', linestyle='--', label='Zero-shot Recall')\n",
    "plt.axhline(y=n_zs_f1_avg, color='red', linestyle='--', label='Zero-shot F1 Score')\n",
    "\n",
    "plt.text(x_axis[-1] + 2, n_zs_precision_avg, f'{n_zs_precision_avg:.3f}', fontsize=9, ha='left', va='center', color='blue')\n",
    "plt.text(x_axis[-1] + 2, n_zs_recall_avg, f'{n_zs_recall_avg:.3f}', fontsize=9, ha='left', va='center', color='green')\n",
    "plt.text(x_axis[-1] + 2, n_zs_f1_avg, f'{n_zs_f1_avg:.3f}', fontsize=9, ha='left', va='center', color='red')\n",
    "\n",
    "\n",
    "plt.xlabel('# of Reports for Memory')\n",
    "plt.ylabel('Scores')\n",
    "plt.title(f'{split_idx} Average Testing Results on 700 Test Reports (n03) - new metric')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "    print(f\"t14_dynamic_test_{i}\")\n",
    "    result_df = pd.read_csv(f\"/home/yl3427/cylab/selfCorrectionAgent/t14_dynamic_test_{i}_outof_10runs.csv\")\n",
    "    for j in range(10, 101, 10):\n",
    "        print()\n",
    "        print(f\"memory at {j}\")\n",
    "        result = t14_performance_report(result_df, f'cmem_t_{j}reports_ans_str')\n",
    "        print(result)\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "    print(f\"n03_dynamic_test_{i}\")\n",
    "    result_df = pd.read_csv(f\"/home/yl3427/cylab/selfCorrectionAgent/n03_dynamic_test_{i}_outof_10runs.csv\")\n",
    "    for j in range(10, 101, 10):\n",
    "        print()\n",
    "        print(f\"memory at {j}\")\n",
    "        result = n03_performance_report(result_df, f'cmem_n_{j}reports_ans_str')\n",
    "        print(result)\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# filtering data for qualitative analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_zs_precision_avg = 0\n",
    "n_zs_recall_avg = 0\n",
    "n_zs_f1_avg = 0\n",
    "\n",
    "n_zs_precision_cumulative = 0\n",
    "n_zs_recall_cumulative = 0\n",
    "n_zs_f1_cumulative = 0\n",
    "\n",
    "for i in range(10):\n",
    "    result_df = pd.read_csv(f\"/home/yl3427/cylab/selfCorrectionAgent/result/t14_test_{i}.csv\").sort_values(by='patient_filename')\n",
    "    t_zs_df = pd.read_csv(\"/secure/shared_data/rag_tnm_results/t14_results/mixtral_all_brca_luad_zs_results.csv\")\n",
    "    t_zs_df = t_zs_df[t_zs_df[\"patient_filename\"].isin(result_df['patient_filename'])].sort_values(by='patient_filename')\n",
    "    t_zs_result = t14_calculate_metrics(result_df['t'], t_zs_df['ans_str'])['overall']\n",
    "    n_zs_precision_avg += t_zs_result['macro_precision']\n",
    "    n_zs_recall_avg += t_zs_result['macro_recall']\n",
    "    n_zs_f1_avg += t_zs_result['macro_f1']\n",
    "\n",
    "    result_df = pd.read_csv(f\"/home/yl3427/cylab/selfCorrectionAgent/result/n03_test_{i}.csv\").sort_values(by='patient_filename') \n",
    "    n_zs_df = pd.read_csv(\"/secure/shared_data/rag_tnm_results/n03_results/mixtral_all_brca_zs_results.csv\")\n",
    "    n_zs_df = n_zs_df[n_zs_df[\"patient_filename\"].isin(result_df['patient_filename'])].sort_values(by='patient_filename')\n",
    "    n_zs_result = n03_calculate_metrics(result_df['n'], n_zs_df['ans_str'])['overall']\n",
    "    n_zs_precision_cumulative += n_zs_result['macro_precision']\n",
    "    n_zs_recall_cumulative += n_zs_result['macro_recall']\n",
    "    n_zs_f1_cumulative += n_zs_result['macro_f1']\n",
    "\n",
    "# average\n",
    "n_zs_precision_avg = n_zs_precision_avg / 10\n",
    "n_zs_recall_avg = n_zs_recall_avg / 10\n",
    "n_zs_f1_avg = n_zs_f1_avg / 10\n",
    "\n",
    "n_zs_precision_avg = n_zs_precision_cumulative / 10\n",
    "n_zs_recall_avg = n_zs_recall_cumulative / 10\n",
    "n_zs_f1_avg = n_zs_f1_cumulative / 10\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "t_zscot_df = pd.read_csv(\"/secure/shared_data/rag_tnm_results/t14_results/mixtral_all_brca_zscot_results.csv\")\n",
    "t_zscot_df = t_zscot_df[t_zscot_df[\"patient_filename\"].isin(result_df['patient_filename'])].sort_values(by='patient_filename')\n",
    "n_zscot_df = pd.read_csv(\"/secure/shared_data/rag_tnm_results/n03_results/mixtral_all_brca_zscot_results.csv\")\n",
    "n_zscot_df = n_zscot_df[n_zscot_df[\"patient_filename\"].isin(result_df['patient_filename'])].sort_values(by='patient_filename')\n",
    "\n",
    "t_zscot_df.columns, n_zscot_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "result_df = pd.read_csv(\"/home/yl3427/cylab/selfCorrectionAgent/result/t14_test_1.csv\").sort_values(by='patient_filename')\n",
    "result_df = pd.read_csv(\"/home/yl3427/cylab/selfCorrectionAgent/result/n03_test_1.csv\").sort_values(by='patient_filename')\n",
    "# t_zs_df = pd.read_csv(\"/secure/shared_data/rag_tnm_results/t14_results/mixtral_all_brca_luad_zs_results.csv\")\n",
    "# t_zs_df = t_zs_df[t_zs_df[\"patient_filename\"].isin(t_memory_test_df['patient_filename'])].sort_values(by='patient_filename')\n",
    "# n_zs_df = pd.read_csv(\"/secure/shared_data/rag_tnm_results/n03_results/mixtral_all_brca_zs_results.csv\")\n",
    "# n_zs_df = n_zs_df[n_zs_df[\"patient_filename\"].isin(n_memory_test_df['patient_filename'])].sort_values(by='patient_filename')\n",
    "t_zscot_df = pd.read_csv(\"/secure/shared_data/rag_tnm_results/t14_results/mixtral_all_brca_zscot_results.csv\")\n",
    "t_zscot_df = t_zscot_df[t_zscot_df[\"patient_filename\"].isin(result_df['patient_filename'])].sort_values(by='patient_filename')\n",
    "n_zscot_df = pd.read_csv(\"/secure/shared_data/rag_tnm_results/n03_results/mixtral_all_brca_zscot_results.csv\")\n",
    "n_zscot_df = n_zscot_df[n_zscot_df[\"patient_filename\"].isin(result_df['patient_filename'])].sort_values(by='patient_filename')\n",
    " \n",
    "\n",
    "error_dict = {\"t_memory_test\": set(), \"n_memory_test\": set(), \"t_zscot\": set(), \"n_zscot\": set()}\n",
    "\n",
    "for _, row in result_df.iterrows():\n",
    "    if (row['cmem_t_is_parsed']) and (f\"T{row['t']+1}\" not in row['cmem_t_ans_str']):\n",
    "        error_dict[\"t_memory_test\"].add(row['patient_filename'])\n",
    "\n",
    "for _, row in t_zscot_df.iterrows():\n",
    "    if f\"T{row['t']+1}\" not in row['ans_str_0']:\n",
    "        error_dict[\"t_zscot\"].add(row['patient_filename'])\n",
    "\n",
    "for _, row in result_df.iterrows():\n",
    "    if (row['cmem_n_is_parsed']) and (f\"N{row['n']}\" not in row['cmem_n_ans_str']):\n",
    "        error_dict[\"n_memory_test\"].add(row['patient_filename'])\n",
    "\n",
    "for _, row in n_zscot_df.iterrows():\n",
    "    if f\"N{row['n']}\" not in row['ans_str_0']:\n",
    "        error_dict[\"n_zscot\"].add(row['patient_filename'])\n",
    "\n",
    "\n",
    "t_intersect = error_dict[\"t_memory_test\"] & error_dict[\"t_zscot\"] \n",
    "n_intersect = error_dict[\"n_memory_test\"] & error_dict[\"n_zscot\"] \n",
    "t_only_memory = error_dict[\"t_memory_test\"] - error_dict[\"t_zscot\"] \n",
    "n_only_memory = error_dict[\"n_memory_test\"] - error_dict[\"n_zscot\"]\n",
    "t_only_zscot = error_dict[\"t_zscot\"] - error_dict[\"t_memory_test\"] \n",
    "n_only_zscot = error_dict[\"n_zscot\"] - error_dict[\"n_memory_test\"] \n",
    "\n",
    "t_intersect_in_memory = result_df[result_df['patient_filename'].isin(t_intersect)][['patient_filename', 'text', 't', 'cmem_t_ans_str', 'cmem_t_reasoning']]\n",
    "t_intersect_in_zscot = t_zscot_df[t_zscot_df[\"patient_filename\"].isin(t_intersect)][['patient_filename', 'ans_str_0', 'step_by_step_0']]\n",
    "t_intersect_df = pd.merge(t_intersect_in_memory, t_intersect_in_zscot, on='patient_filename')\n",
    "\n",
    "n_intersect_in_memory = result_df[result_df['patient_filename'].isin(n_intersect)][['patient_filename', 'text', 'n', 'cmem_n_ans_str', 'cmem_n_reasoning']]\n",
    "n_intersect_in_zscot = n_zscot_df[n_zscot_df[\"patient_filename\"].isin(n_intersect)][['patient_filename', 'ans_str_0', 'step_by_step_0']]\n",
    "n_intersect_df = pd.merge(n_intersect_in_memory, n_intersect_in_zscot, on='patient_filename')\n",
    "\n",
    "t_only_memory_in_memory = result_df[result_df['patient_filename'].isin(t_only_memory)][['patient_filename', 'text', 't', 'cmem_t_ans_str', 'cmem_t_reasoning']]\n",
    "t_only_memory_in_zscot = t_zscot_df[t_zscot_df[\"patient_filename\"].isin(t_only_memory)][['patient_filename', 'ans_str_0', 'step_by_step_0']]\n",
    "t_only_memory_df = pd.merge(t_only_memory_in_memory, t_only_memory_in_zscot, on='patient_filename')\n",
    "\n",
    "n_only_memory_in_memory = result_df[result_df['patient_filename'].isin(n_only_memory)][['patient_filename', 'text', 'n', 'cmem_n_ans_str', 'cmem_n_reasoning']]\n",
    "n_only_memory_in_zscot = n_zscot_df[n_zscot_df[\"patient_filename\"].isin(n_only_memory)][['patient_filename', 'ans_str_0', 'step_by_step_0']]\n",
    "n_only_memory_df = pd.merge(n_only_memory_in_memory, n_only_memory_in_zscot, on='patient_filename')\n",
    "\n",
    "t_only_zscot_in_memory = result_df[result_df['patient_filename'].isin(t_only_zscot)][['patient_filename', 'text', 't', 'cmem_t_ans_str', 'cmem_t_reasoning']]\n",
    "t_only_zscot_in_zscot = t_zscot_df[t_zscot_df[\"patient_filename\"].isin(t_only_zscot)][['patient_filename', 'ans_str_0', 'step_by_step_0']]\n",
    "t_only_zscot_df = pd.merge(t_only_zscot_in_memory, t_only_zscot_in_zscot, on='patient_filename')\n",
    "\n",
    "n_only_zscot_in_memory = result_df[result_df['patient_filename'].isin(n_only_zscot)][['patient_filename', 'text', 'n', 'cmem_n_ans_str', 'cmem_n_reasoning']]\n",
    "n_only_zscot_in_zscot = n_zscot_df[n_zscot_df[\"patient_filename\"].isin(n_only_zscot)][['patient_filename', 'ans_str_0', 'step_by_step_0']]\n",
    "n_only_zscot_df = pd.merge(n_only_zscot_in_memory, n_only_zscot_in_zscot, on='patient_filename')\n",
    "\n",
    "# t_intersect_df.to_csv(\"t_intersect.csv\", index=False)\n",
    "# n_intersect_df.to_csv(\"n_intersect.csv\", index=False)\n",
    "# t_only_memory_df.to_csv(\"t_only_memory.csv\", index=False)\n",
    "# n_only_memory_df.to_csv(\"n_only_memory.csv\", index=False)\n",
    "# t_only_zscot_df.to_csv(\"t_only_zscot.csv\", index=False)\n",
    "# n_only_zscot_df.to_csv(\"n_only_zscot.csv\", index=False)\n",
    "\n",
    "t_intersect_df[['patient_filename', 'text', 't']].to_csv(\"t_intersect_for_newPrompt.csv\", index=False)\n",
    "n_intersect_df[['patient_filename', 'text', 'n']].to_csv(\"n_intersect_for_newPrompt.csv\", index=False)\n",
    "t_only_memory_df[['patient_filename', 'text', 't']].to_csv(\"t_only_memory_for_newPrompt.csv\", index=False)\n",
    "n_only_memory_df[['patient_filename', 'text', 'n']].to_csv(\"n_only_memory_for_newPrompt.csv\", index=False)\n",
    "t_only_zscot_df[['patient_filename', 'text', 't']].to_csv(\"t_only_zscot_for_newPrompt.csv\", index=False)\n",
    "n_only_zscot_df[['patient_filename', 'text', 'n']].to_csv(\"n_only_zscot_for_newPrompt.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_only_zscot_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_intersect_in_memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(t_intersect), len(n_intersect), len(t_only_memory), len(n_only_memory), len(t_only_zscot), len(n_only_zscot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stuck cases - rerun"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### T14"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from agent import ConditionalMemoryAgent\n",
    "from prompt import *\n",
    "from metrics import *\n",
    "from openai import OpenAI\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pydantic import BaseModel, Field\n",
    "from typing import List, Dict, Union\n",
    "from collections import defaultdict\n",
    "\n",
    "# class TrainingResponse(BaseModel):\n",
    "#     predictedStage: str = Field(description=\"predicted cancer stage\")\n",
    "#     reasoning: str = Field(description=\"reasoning to support predicted cancer stage\") \n",
    "#     rules: List[str] = Field(description=\"list of rules\") \n",
    "\n",
    "# class TestingResponse(BaseModel):\n",
    "#     predictedStage: str = Field(description=\"predicted cancer stage\")\n",
    "#     reasoning: str = Field(description=\"reasoning to support predicted cancer stage\") \n",
    "\n",
    "# class TestingResponseWithoutReasoning(BaseModel):\n",
    "#     predictedStage: str = Field(description=\"predicted cancer stage\")\n",
    " \n",
    "\n",
    "# training_schema = TrainingResponse.model_json_schema()\n",
    "# testing_schema = TestingResponseWithoutReasoning.model_json_schema()\n",
    "\n",
    "# client = OpenAI(\n",
    "#     api_key = \"empty\",\n",
    "#     base_url = \"http://localhost:8000/v1\"\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for split_idx in range(10):\n",
    "    t_df = pd.read_csv(f\"/home/yl3427/cylab/selfCorrectionAgent/t14_dynamic_test_{split_idx}_outof_10runs.csv\")\n",
    "    t_train_data = pd.read_csv(f\"/home/yl3427/cylab/selfCorrectionAgent/result/t14_train_{split_idx}.csv\")\n",
    "\n",
    "    memory_dict = []\n",
    "    for idx, row in t_train_data.iterrows():\n",
    "        memory_dict.append((idx+1,row['cmem_t_memory_str']))\n",
    "    memory_dict = memory_dict[9::10]\n",
    "\n",
    "    t_stuck_cases = defaultdict(list)\n",
    "\n",
    "    for _, row in t_df.iterrows():\n",
    "        for i, memory in memory_dict:\n",
    "            if (row[f'cmem_t_{i}reports_ans_str'] is None) or not isinstance(row[f'cmem_t_{i}reports_ans_str'], str):\n",
    "                print(\"있어>>>\")\n",
    "                t_stuck_cases[row['patient_filename']].append((i, memory))\n",
    "    if len(t_stuck_cases) == 0:\n",
    "        continue\n",
    "    print(f\"split_idx: {split_idx}\")\n",
    "    print(t_stuck_cases)\n",
    "    \n",
    "    # memory_agent_t14 = ConditionalMemoryAgent(client=client, model=\"mistralai/Mixtral-8x7B-Instruct-v0.1\",\n",
    "    #                     prompt_template_dict={\"initialized_prompt\":initial_predict_prompt_t14,\n",
    "    #                                             \"learning_prompt\":subsequent_predict_prompt_t14,\n",
    "    #                                             \"testing_prompt\":testing_predict_prompt_t14_without_reasoning},\n",
    "    #                     schema_dict={\"learning_schema\":training_schema,\n",
    "    #                                     \"testing_schema\":testing_schema},\n",
    "    #                                     label = \"t\")\n",
    "\n",
    "    # for patient, tupes in t_stuck_cases.items():\n",
    "    #     result = memory_agent_t14.dynamic_test(t_df[t_df['patient_filename'] == patient], tupes)\n",
    "    #     print(result)\n",
    "    #     t_df[t_df['patient_filename']== patient] = result\n",
    "    # t_df.to_csv(f\"/home/yl3427/cylab/selfCorrectionAgent/t14_dynamic_test_{split_idx}_outof_10runs.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_df[t_df['patient_filename'] == patient]['text'].values[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, memory in memory_dict:\n",
    "    metrics = t14_calculate_metrics(t_df['t'], t_df[f'cmem_t_{i}reports_ans_str'])\n",
    "    print(metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### N03"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for split_idx in range(10):\n",
    "    n_df = pd.read_csv(f\"/home/yl3427/cylab/selfCorrectionAgent/n03_dynamic_test_{split_idx}_outof_10runs.csv\")\n",
    "    n_train_data = pd.read_csv(f\"/home/yl3427/cylab/selfCorrectionAgent/result/n03_train_{split_idx}.csv\")\n",
    "\n",
    "    zscot_df = pd.read_csv(\"/secure/shared_data/rag_tnm_results/n03_results/mixtral_all_brca_zscot_results.csv\")\n",
    "    zscot_df = zscot_df[zscot_df[\"patient_filename\"].isin(n_df['patient_filename'])].sort_values(by='patient_filename').reset_index()\n",
    "\n",
    "    memory_dict = []\n",
    "    for idx, row in n_train_data.iterrows():\n",
    "        memory_dict.append((idx+1,row['cmem_n_memory_str']))\n",
    "    memory_dict = memory_dict[9::10]\n",
    "\n",
    "    n_stuck_cases = defaultdict(list)\n",
    "\n",
    "    for _, row in n_df.iterrows():\n",
    "        for i, memory in memory_dict:\n",
    "            if (row[f'cmem_n_{i}reports_ans_str'] is None) or not isinstance(row[f'cmem_n_{i}reports_ans_str'], str):\n",
    "                n_stuck_cases[row['patient_filename']].append((i, memory))\n",
    "    if len(n_stuck_cases) == 0:\n",
    "        continue    \n",
    "    \n",
    "    print(f\"split_idx: {split_idx}\")\n",
    "    print(n_stuck_cases)\n",
    "\n",
    "    # memory_agent_n03 = ConditionalMemoryAgent(client=client, model=\"mistralai/Mixtral-8x7B-Instruct-v0.1\",\n",
    "    #                     prompt_template_dict={\"initialized_prompt\":initial_predict_prompt_n03,\n",
    "    #                                             \"learning_prompt\":subsequent_predict_prompt_n03,\n",
    "    #                                             \"testing_prompt\":testing_predict_prompt_n03_without_reasoning},\n",
    "    #                     schema_dict={\"learning_schema\":training_schema,\n",
    "    #                                     \"testing_schema\":testing_schema},\n",
    "    #                                     label = \"n\")\n",
    "\n",
    "    # for patient, tupes in n_stuck_cases.items():\n",
    "    #     result = memory_agent_n03.dynamic_test(n_df[n_df['patient_filename'] == patient], tupes)\n",
    "    #     n_df[n_df['patient_filename']== patient] = result\n",
    "    \n",
    "    # n_df.to_csv(f\"/home/yl3427/cylab/selfCorrectionAgent/n03_dynamic_test_{split_idx}_outof_10runs.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, memory in memory_dict:\n",
    "    metrics = n03_calculate_metrics(n_df['n'], n_df[f'cmem_n_{i}reports_ans_str'])\n",
    "    print(metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [working] select zs data that corresponds to each split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### T14"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from metrics import *\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "split_idx = 10\n",
    "\n",
    "for num in range(split_idx):\n",
    "    result_df = pd.read_csv(f\"/home/yl3427/cylab/selfCorrectionAgent/t14_dynamic_test_{num}_outof_10runs.csv\")\n",
    "    train_df = pd.read_csv(f\"/home/yl3427/cylab/selfCorrectionAgent/result/t14_train_{num}.csv\")\n",
    "\n",
    "    zscot_df = pd.read_csv(\"/secure/shared_data/rag_tnm_results/t14_results/mixtral_all_brca_zscot_results.csv\")\n",
    "    zscot_df = t_zscot_df[t_zscot_df[\"patient_filename\"].isin(result_df['patient_filename'])].sort_values(by='patient_filename').reset_index()\n",
    "\n",
    "    memory_dict = []\n",
    "    for idx, row in train_df.iterrows():\n",
    "        memory_dict.append((idx+1,row['cmem_t_memory_str']))\n",
    "    memory_dict = memory_dict[9::10]\n",
    "\n",
    "    # Check for parsing error: \n",
    "    for i, _ in memory_dict:\n",
    "        if len(result_df[result_df[f\"cmem_t_{i}reports_is_parsed\"]==False]) > 0:\n",
    "            print(f\"parsing error at memory {i}\")\n",
    "            print(result_df[result_df[f\"cmem_t_{i}reports_is_parsed\"]==False][['patient_filename']])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.metrics import *\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "split_idx = 10\n",
    "\n",
    "for num in range(split_idx):\n",
    "    result_df = pd.read_csv(f\"/home/yl3427/cylab/selfCorrectionAgent/t14_dynamic_test_{num}_outof_10runs.csv\")\n",
    "    train_df = pd.read_csv(f\"/home/yl3427/cylab/selfCorrectionAgent/result/t14_train_{num}.csv\")\n",
    "\n",
    "    zscot_df = pd.read_csv(\"/secure/shared_data/rag_tnm_results/t14_results/mixtral_all_brca_zscot_results.csv\")\n",
    "    zscot_df = t_zscot_df[t_zscot_df[\"patient_filename\"].isin(result_df['patient_filename'])].sort_values(by='patient_filename').reset_index()\n",
    "\n",
    "    memory_dict = []\n",
    "    for idx, row in train_df.iterrows():\n",
    "        memory_dict.append((idx+1,row['cmem_t_memory_str']))\n",
    "    memory_dict = memory_dict[9::10]\n",
    "\n",
    "    # Check for parsing error: \n",
    "    for i, _ in memory_dict:\n",
    "        if len(result_df[result_df[f\"cmem_t_{i}reports_is_parsed\"]==False]) > 0:\n",
    "            print(f\"parsing error at memory {i}\")\n",
    "            print(result_df[result_df[f\"cmem_t_{i}reports_is_parsed\"]==False])\n",
    "            print()\n",
    "            # parsing error at memory 10\n",
    "        \n",
    "    # gather y-axis data\n",
    "    precision_lst = []\n",
    "    recall_lst = []\n",
    "    f1_lst = []\n",
    "\n",
    "    x_idx = []\n",
    "    for i, _ in memory_dict:\n",
    "        x_idx.append(i)\n",
    "        print(f\"memory at {i}\")\n",
    "        precision, recall, f1 = t14_performance_report(result_df, f'cmem_t_{i}reports_ans_str')\n",
    "        precision_lst.append(precision)\n",
    "        recall_lst.append(recall)\n",
    "        f1_lst.append(f1)\n",
    "\n",
    "\n",
    "    plt.figure(figsize=(15, 10))\n",
    "\n",
    "    plt.plot(x_idx, precision_lst, label='Memory Precision', color='blue', marker='o')\n",
    "    plt.plot(x_idx, recall_lst, label='Memory Recall', color='green', marker='o')\n",
    "    plt.plot(x_idx, f1_lst, label='Memory F1 Score', color='red', marker='o')\n",
    "\n",
    "    zs_precision, zs_recall, zs_f1 = t14_performance_report(zscot_df, 'ans_str_0')\n",
    "\n",
    "    plt.axhline(y=zs_precision, color='blue', linestyle='--', label='Zero-shot Precision')\n",
    "    plt.axhline(y=zs_recall, color='green', linestyle='--', label='Zero-shot Recall')\n",
    "    plt.axhline(y=zs_f1, color='red', linestyle='--', label='Zero-shot F1 Score')\n",
    "            \n",
    "    plt.ylim(0.7, 0.9)  # Set the y-axis scope here\n",
    "\n",
    "    plt.xlabel(f'# of Reports for Memory (t14_train_{num}.csv)')\n",
    "    plt.ylabel('Scores')\n",
    "    plt.title(f'Testing Results on 700 test Reports (t14_test_{num}.csv)')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### N03"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.metrics import *\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict\n",
    "\n",
    "def checker(row_num, col1, col2):\n",
    "    for i in range(row_num):\n",
    "        if col1[i] != col2[i]:\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "split_idx = 9\n",
    "\n",
    "for num in range(split_idx):\n",
    "    memory_df = pd.read_csv(f\"/home/yl3427/cylab/selfCorrectionAgent/result/n03_test_{num}_outof_10runs.csv\").sort_values(by='patient_filename').reset_index()\n",
    "    valid_memory_df = memory_df[memory_df['cmem_n_is_parsed'] == True]\n",
    "\n",
    "    n_zscot_df = pd.read_csv(\"/secure/shared_data/rag_tnm_results/n03_results/mixtral_all_brca_zscot_results.csv\")\n",
    "    n_zscot_df = n_zscot_df[n_zscot_df[\"patient_filename\"].isin(memory_df['patient_filename'])].sort_values(by='patient_filename').reset_index()\n",
    "    \n",
    "    assert len(valid_memory_df) == len(n_zscot_df) == 700\n",
    "    assert checker(700, valid_memory_df['patient_filename'], n_zscot_df['patient_filename'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.metrics import *\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "split_idx = 10\n",
    "\n",
    "for num in range(split_idx):\n",
    "    result_df = pd.read_csv(f\"/home/yl3427/cylab/selfCorrectionAgent/result/n03_test_{num}.csv\").sort_values(by='patient_filename').reset_index()\n",
    "    train_df = pd.read_csv(f\"/home/yl3427/cylab/selfCorrectionAgent/result/n03_train_{num}.csv\")\n",
    "\n",
    "    zscot_df = pd.read_csv(\"/secure/shared_data/rag_tnm_results/n03_results/mixtral_all_brca_zscot_results.csv\")\n",
    "    zscot_df = zscot_df[zscot_df[\"patient_filename\"].isin(result_df['patient_filename'])].sort_values(by='patient_filename').reset_index()\n",
    "\n",
    "    memory_dict = []\n",
    "    for idx, row in train_df.iterrows():\n",
    "        memory_dict.append((idx+1,row['cmem_n_memory_str']))\n",
    "    memory_dict = memory_dict[9::10]\n",
    "\n",
    "    # Check for parsing error: \n",
    "    for i, _ in memory_dict:\n",
    "        if len(result_df[result_df[f\"cmem_n_{i}reports_is_parsed\"]==False]) > 0:\n",
    "            print(f\"parsing error at memory {i}\")\n",
    "            print()\n",
    "            # parsing error at memory 10\n",
    "        \n",
    "    # gather y-axis data\n",
    "    precision_lst = []\n",
    "    recall_lst = []\n",
    "    f1_lst = []\n",
    "\n",
    "    x_idx = []\n",
    "    for i, _ in memory_dict:\n",
    "        x_idx.append(i)\n",
    "        print(f\"memory at {i}\")\n",
    "        precision, recall, f1 = n03_performance_report(result_df, f'cmem_n_{i}reports_ans_str')\n",
    "        precision_lst.append(precision)\n",
    "        recall_lst.append(recall)\n",
    "        f1_lst.append(f1)\n",
    "\n",
    "\n",
    "    plt.figure(figsize=(15, 10))\n",
    "\n",
    "    plt.plot(x_idx, precision_lst, label='Memory Precision', color='blue', marker='o')\n",
    "    plt.plot(x_idx, recall_lst, label='Memory Recall', color='green', marker='o')\n",
    "    plt.plot(x_idx, f1_lst, label='Memory F1 Score', color='red', marker='o')\n",
    "\n",
    "    zs_precision, zs_recall, zs_f1 = n03_performance_report(zscot_df, 'ans_str_0')\n",
    "\n",
    "    plt.axhline(y=zs_precision, color='blue', linestyle='--', label='Zero-shot Precision')\n",
    "    plt.axhline(y=zs_recall, color='green', linestyle='--', label='Zero-shot Recall')\n",
    "    plt.axhline(y=zs_f1, color='red', linestyle='--', label='Zero-shot F1 Score')\n",
    "            \n",
    "    plt.ylim(0.7, 0.9)  # Set the y-axis scope here\n",
    "\n",
    "    plt.xlabel(f'# of Reports for Memory (n03_train_{num}.csv)')\n",
    "    plt.ylabel('Scores')\n",
    "    plt.title(f'Testing Results on 700 test Reports (n03_test_{num}.csv)')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [working] Filter error cases to apply the reordered prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# 3: 97\n",
    "split_idx = 7 # 437\n",
    "\n",
    "t_df = pd.read_csv(f\"/home/yl3427/cylab/selfCorrectionAgent/t14_dynamic_test_{split_idx}_outof_10runs.csv\")\n",
    "t_train_data = pd.read_csv(f\"/home/yl3427/cylab/selfCorrectionAgent/result/t14_train_{split_idx}.csv\")\n",
    "\n",
    "error_dict = {\"t_memory_test\": set(), \"n_memory_test\": set()}\n",
    "skip_dict = {\"t_memory_test\": set(), \"n_memory_test\": set()}\n",
    "\n",
    "memory_dict = []\n",
    "for idx, row in t_train_data.iterrows():\n",
    "    memory_dict.append((idx+1,row['cmem_t_memory_str']))\n",
    "memory_dict = memory_dict[9::10]\n",
    "\n",
    "\n",
    "for _, row in t_df.iterrows():\n",
    "    for i, memory in memory_dict:\n",
    "        if (row[f'cmem_t_{i}reports_ans_str'] is None) or not isinstance(row[f'cmem_t_{i}reports_ans_str'], str):\n",
    "            print(f\"skip case at {row['patient_filename']}, memory {i}\")\n",
    "            skip_dict[\"t_memory_test\"].add(row['patient_filename'])\n",
    "            continue\n",
    "        if f\"T{row['t']+1}\" not in row[f'cmem_t_{i}reports_ans_str']:\n",
    "            print(f\"{row['t']+1} not in {row[f'cmem_t_{i}reports_ans_str']}\")\n",
    "            error_dict[\"t_memory_test\"].add(row['patient_filename'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(error_dict['t_memory_test'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # too many cases\n",
    "# import pandas as pd\n",
    "\n",
    "# error_dict = {\"t_memory_test\": set(), \"n_memory_test\": set()}\n",
    "# skip_dict = {\"t_memory_test\": set(), \"n_memory_test\": set()}\n",
    "\n",
    "# for split_idx in range(10):\n",
    "\n",
    "#     t_df = pd.read_csv(f\"/home/yl3427/cylab/selfCorrectionAgent/t14_dynamic_test_{split_idx}_outof_10runs.csv\")\n",
    "#     t_train_data = pd.read_csv(f\"/home/yl3427/cylab/selfCorrectionAgent/result/t14_train_{split_idx}.csv\")\n",
    "\n",
    "\n",
    "#     memory_tup = []\n",
    "#     for idx, row in t_train_data.iterrows():\n",
    "#         memory_tup.append((idx+1,row['cmem_t_memory_str']))\n",
    "#     memory_tup = memory_tup[9::10]\n",
    "\n",
    "\n",
    "#     for _, row in t_df.iterrows():\n",
    "#         for i, memory in memory_tup:\n",
    "#             if (row[f'cmem_t_{i}reports_ans_str'] is None) or not isinstance(row[f'cmem_t_{i}reports_ans_str'], str):\n",
    "#                 print(f\"skip case at {row['patient_filename']}, memory {i}\")\n",
    "#                 skip_dict[\"t_memory_test\"].add(row['patient_filename'])\n",
    "#                 continue\n",
    "#             if f\"T{row['t']+1}\" not in row[f'cmem_t_{i}reports_ans_str']:\n",
    "#                 error_dict[\"t_memory_test\"].add(row['patient_filename'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "split_idx = 0\n",
    "\n",
    "result_df = pd.read_csv(f\"/home/yl3427/cylab/selfCorrectionAgent/t14_dynamic_test_{split_idx}_outof_10runs.csv\").sort_values(by='patient_filename')\n",
    "result_df = pd.read_csv(f\"/home/yl3427/cylab/selfCorrectionAgent/n03_dynamic_test_{split_idx}_outof_10runs.csv\").sort_values(by='patient_filename')\n",
    "\n",
    "error_dict = {\"t_memory_test\": set(), \"n_memory_test\": set()}\n",
    "skip_dict = {\"t_memory_test\": set(), \"n_memory_test\": set()}\n",
    "\n",
    "\n",
    "for _, row in result_df.iterrows():\n",
    "\n",
    "    if (row['cmem_t_is_parsed']) and (f\"T{row['t']+1}\" not in row['cmem_t_ans_str']):\n",
    "        error_dict[\"t_memory_test\"].add(row['patient_filename'])\n",
    "\n",
    "for _, row in result_df.iterrows():\n",
    "    if (row[f'cmem_n_{i}reports_ans_str'] is None) or not isinstance(row[f'cmem_n_{i}reports_ans_str'], str):\n",
    "        skip_dict[\"n_memory_test\"].add(row['patient_filename'])\n",
    "        continue\n",
    "    if f\"N{row['n']}\" not in row['cmem_n_ans_str']:\n",
    "        error_dict[\"n_memory_test\"].add(row['patient_filename'])\n",
    "\n",
    "\n",
    "t_intersect = error_dict[\"t_memory_test\"] & error_dict[\"t_zscot\"] \n",
    "n_intersect = error_dict[\"n_memory_test\"] & error_dict[\"n_zscot\"] \n",
    "t_only_memory = error_dict[\"t_memory_test\"] - error_dict[\"t_zscot\"] \n",
    "n_only_memory = error_dict[\"n_memory_test\"] - error_dict[\"n_zscot\"]\n",
    "t_only_zscot = error_dict[\"t_zscot\"] - error_dict[\"t_memory_test\"] \n",
    "n_only_zscot = error_dict[\"n_zscot\"] - error_dict[\"n_memory_test\"] \n",
    "\n",
    "t_intersect_in_memory = result_df[result_df['patient_filename'].isin(t_intersect)][['patient_filename', 'text', 't', 'cmem_t_ans_str', 'cmem_t_reasoning']]\n",
    "t_intersect_in_zscot = t_zscot_df[t_zscot_df[\"patient_filename\"].isin(t_intersect)][['patient_filename', 'ans_str_0', 'step_by_step_0']]\n",
    "t_intersect_df = pd.merge(t_intersect_in_memory, t_intersect_in_zscot, on='patient_filename')\n",
    "\n",
    "n_intersect_in_memory = result_df[result_df['patient_filename'].isin(n_intersect)][['patient_filename', 'text', 'n', 'cmem_n_ans_str', 'cmem_n_reasoning']]\n",
    "n_intersect_in_zscot = n_zscot_df[n_zscot_df[\"patient_filename\"].isin(n_intersect)][['patient_filename', 'ans_str_0', 'step_by_step_0']]\n",
    "n_intersect_df = pd.merge(n_intersect_in_memory, n_intersect_in_zscot, on='patient_filename')\n",
    "\n",
    "t_only_memory_in_memory = result_df[result_df['patient_filename'].isin(t_only_memory)][['patient_filename', 'text', 't', 'cmem_t_ans_str', 'cmem_t_reasoning']]\n",
    "t_only_memory_in_zscot = t_zscot_df[t_zscot_df[\"patient_filename\"].isin(t_only_memory)][['patient_filename', 'ans_str_0', 'step_by_step_0']]\n",
    "t_only_memory_df = pd.merge(t_only_memory_in_memory, t_only_memory_in_zscot, on='patient_filename')\n",
    "\n",
    "n_only_memory_in_memory = result_df[result_df['patient_filename'].isin(n_only_memory)][['patient_filename', 'text', 'n', 'cmem_n_ans_str', 'cmem_n_reasoning']]\n",
    "n_only_memory_in_zscot = n_zscot_df[n_zscot_df[\"patient_filename\"].isin(n_only_memory)][['patient_filename', 'ans_str_0', 'step_by_step_0']]\n",
    "n_only_memory_df = pd.merge(n_only_memory_in_memory, n_only_memory_in_zscot, on='patient_filename')\n",
    "\n",
    "t_only_zscot_in_memory = result_df[result_df['patient_filename'].isin(t_only_zscot)][['patient_filename', 'text', 't', 'cmem_t_ans_str', 'cmem_t_reasoning']]\n",
    "t_only_zscot_in_zscot = t_zscot_df[t_zscot_df[\"patient_filename\"].isin(t_only_zscot)][['patient_filename', 'ans_str_0', 'step_by_step_0']]\n",
    "t_only_zscot_df = pd.merge(t_only_zscot_in_memory, t_only_zscot_in_zscot, on='patient_filename')\n",
    "\n",
    "n_only_zscot_in_memory = result_df[result_df['patient_filename'].isin(n_only_zscot)][['patient_filename', 'text', 'n', 'cmem_n_ans_str', 'cmem_n_reasoning']]\n",
    "n_only_zscot_in_zscot = n_zscot_df[n_zscot_df[\"patient_filename\"].isin(n_only_zscot)][['patient_filename', 'ans_str_0', 'step_by_step_0']]\n",
    "n_only_zscot_df = pd.merge(n_only_zscot_in_memory, n_only_zscot_in_zscot, on='patient_filename')\n",
    "\n",
    "t_intersect_df.to_csv(\"t_intersect.csv\", index=False)\n",
    "n_intersect_df.to_csv(\"n_intersect.csv\", index=False)\n",
    "t_only_memory_df.to_csv(\"t_only_memory.csv\", index=False)\n",
    "n_only_memory_df.to_csv(\"n_only_memory.csv\", index=False)\n",
    "t_only_zscot_df.to_csv(\"t_only_zscot.csv\", index=False)\n",
    "n_only_zscot_df.to_csv(\"n_only_zscot.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from metrics import *\n",
    "import pandas as pd\n",
    "old_df = pd.read_csv(\"/home/yl3427/cylab/selfCorrectionAgent/result/t14_test_1.csv\")\n",
    "new_df = pd.read_csv(\"/home/yl3427/cylab/selfCorrectionAgent/result/t14_test_1_newPrompt.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t14_calculate_metrics(old_df['t'], old_df['cmem_t_ans_str'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t14_calculate_metrics(new_df['t'], new_df['cmem_t_ans_str'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t14_performance_report(old_df, 'cmem_t_ans_str')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t14_performance_report(new_df, 'cmem_t_ans_str')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "run = 8\n",
    "\n",
    "# test data (common for both t14 and n03)\n",
    "test_data = pd.read_csv(f\"/home/yl3427/cylab/selfCorrectionAgent/result/t14_test_{run}.csv\")[['Unnamed: 0', 'patient_filename', 't', 'text', 'n']]\n",
    "\n",
    "# t14 training data to extract memory\n",
    "train_data = pd.read_csv(f\"/home/yl3427/cylab/selfCorrectionAgent/result/t14_train_{run}.csv\")\n",
    "\n",
    "memory_dict_t = {}\n",
    "for idx, row in train_data.iterrows():\n",
    "    # if row[\"cmem_t_is_updated\"] == True:\n",
    "    memory_dict_t[f\"{idx+1}\"] = row['cmem_t_memory_str']\n",
    "\n",
    "print(memory_dict_t['30'])\n",
    "\n",
    "# t_df = pd.read_csv(f\"/home/yl3427/cylab/selfCorrectionAgent/result/t14_dynamic_test_{run}_outof_10runs.csv\")\n",
    "\n",
    "\n",
    "\n",
    "# n03 training data to extract memory\n",
    "train_data = pd.read_csv(f\"/home/yl3427/cylab/selfCorrectionAgent/result/n03_train_{run}.csv\")\n",
    "\n",
    "memory_dict_n = {}\n",
    "for idx, row in train_data.iterrows():\n",
    "    # if row[\"cmem_t_is_updated\"] == True:\n",
    "    memory_dict_n[f\"{idx+1}\"] = row['cmem_n_memory_str']\n",
    "print(memory_dict_n['50'])\n",
    "\n",
    "# n_df = pd.read_csv(f\"/home/yl3427/cylab/selfCorrectionAgent/result/n03_dynamic_test_{run}_outof_10runs.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set(t_df.patient_filename) == set(n_df.patient_filename) == set(test_data.patient_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Qual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from metrics import *\n",
    "from pprint import pprint\n",
    "import json\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv(f\"/home/yl3427/cylab/selfCorrectionAgent/result/t14_train_8.csv\")\n",
    "memory_dict_t = {}\n",
    "for idx, row in train_data.iterrows():\n",
    "    # if row[\"cmem_t_is_updated\"] == True:\n",
    "    memory_dict_t[f\"{idx+1}\"] = row['cmem_t_memory_str']\n",
    "print(memory_dict_t['30'])\n",
    "\n",
    "t_memory_df = pd.read_csv(\"/home/yl3427/cylab/selfCorrectionAgent/t14_test_8_memory_at_30\")\n",
    "t_zs_df = pd.read_csv(\"/home/yl3427/cylab/selfCorrectionAgent/t14_zs_test_8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_memory_result = t14_calculate_metrics(t_memory_df['t'], t_memory_df['cmem_t_ans_str'])\n",
    "t_memory_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_zs_result = t14_calculate_metrics(t_zs_df['t'], t_zs_df['zs_t_ans_str'])\n",
    "t_zs_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_memory = pd.DataFrame(t_memory_result).T\n",
    "df_zs = pd.DataFrame(t_zs_result).T\n",
    "\n",
    "# Combine both DataFrames for comparison\n",
    "df_comparison = pd.concat([df_memory, df_zs], axis=1, keys=['t_memory_result', 't_zs_result'])\n",
    "\n",
    "df_comparison.to_csv(\"t14_test_8_comparison.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for memory_patient, zs_patient in zip(t_memory_df.patient_filename, t_zs_df.patient_filename):\n",
    "    assert memory_patient == zs_patient\n",
    "\n",
    "print(memory_dict_t['30'])\n",
    "print()\n",
    "\n",
    "output_dir = \"studio_data/both_error/t14\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "for idx, (filename, label, memory_ans, zs_ans, memory_rsn, zs_rsn) in enumerate(zip(t_memory_df.patient_filename, t_memory_df.t, t_memory_df.cmem_t_ans_str, t_zs_df.zs_t_ans_str, t_memory_df.cmem_t_reasoning, t_zs_df.zs_t_reasoning)):\n",
    "    # if (memory_ans == zs_ans) and (f\"T{label+1}\" in memory_ans.upper()): # cases where both are correct\n",
    "    # if (f\"T{label+1}\" not in zs_ans.upper()) and (f\"T{label+1}\" in memory_ans.upper()): # cases where only memory was correct\n",
    "    # if (f\"T{label+1}\" in zs_ans.upper()) and (f\"T{label+1}\" not in memory_ans.upper()): # cases where only zs was correct\n",
    "    if (f\"T{label+1}\" not in zs_ans.upper()) and (f\"T{label+1}\" not in memory_ans.upper()): # cases where both were wrong\n",
    "        data = {\n",
    "            \"data\": {\n",
    "                \"humanMachineDialogue\": [\n",
    "                    {\"author\": \"Patient filename\", \"text\": filename},\n",
    "                    {\"author\": \"Memory Reasoning\", \"text\": memory_rsn},\n",
    "                    {\"author\": \"ZS Reasoning\", \"text\": zs_rsn}, \n",
    "                    {\"author\": \"Answer\", \"text\": f\"T{label+1}\"},\n",
    "                    {\"author\": \"Memory Answer\", \"text\": memory_ans},\n",
    "                    {\"author\": \"ZS Answer\", \"text\": zs_ans}\n",
    "                ]\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        file_name = f\"t_data_{idx}.json\"\n",
    "        file_path = os.path.join(output_dir, file_name)\n",
    "        with open(file_path, 'w') as json_file:\n",
    "            json.dump(data, json_file, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rule 1: The N stage is determined by the number of positive lymph nodes.\n",
      "Rule 2: N0 indicates no positive lymph nodes.\n",
      "Modified Rule 6: N0 indicates no metastatic carcinoma in sentinel lymph nodes and regional lymph nodes.\n",
      "Rule 7: Nx indicates that the lymph nodes cannot be assessed.\n",
      "Rule 8: If no lymph nodes are positive for metastasis, the N stage is N0.\n",
      "Rule 9: Metastases in a specific number of lymph nodes corresponds to the N stage as per the AJCC staging system.\n",
      "New Rule 10: N0 can indicate no metastatic carcinoma in sentinel lymph nodes and regional lymph nodes.\n",
      "New Rule 11: N0 can indicate that all assessed lymph nodes are negative for metastatic carcinoma.\n",
      "New Rule 12: N1 indicates metastases in 1 to 3 axillary lymph nodes (No/Total)\n",
      "New Rule 13: N3 indicates metastases in 4 or more axillary lymph nodes, or in in-transit, satellite, or micrometastases (N3mi) in 4 or more lymph nodes, or in ipsilateral internal mammary lymph nodes in the absence of axillary lymph node metastases\n",
      "New Rule 14: N2 indicates metastases in 4 to 9 axillary lymph nodes, or in in-transit, satellite, or micrometastases (N2mi) in 1 to 3 lymph nodes\n",
      "New Rule 15: N2a indicates metastases in 4 to 9 axillary lymph nodes\n"
     ]
    }
   ],
   "source": [
    "train_data = pd.read_csv(f\"/home/yl3427/cylab/selfCorrectionAgent/result/n03_train_8.csv\")\n",
    "memory_dict_n = {}\n",
    "for idx, row in train_data.iterrows():\n",
    "    # if row[\"cmem_t_is_updated\"] == True:\n",
    "    memory_dict_n[f\"{idx+1}\"] = row['cmem_n_memory_str']\n",
    "print(memory_dict_n['50'])\n",
    "\n",
    "n_memory_df = pd.read_csv(\"/home/yl3427/cylab/selfCorrectionAgent/n03_test_8_memory_at_50\")\n",
    "n_zs_df = pd.read_csv(\"/home/yl3427/cylab/selfCorrectionAgent/n03_zs_test_8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_memory_result = n03_calculate_metrics(n_memory_df['n'], n_memory_df['cmem_n_ans_str'])\n",
    "n_memory_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_zs_result = n03_calculate_metrics(n_zs_df['n'], n_zs_df['zs_n_ans_str'])\n",
    "n_zs_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_memory = pd.DataFrame(n_memory_result).T\n",
    "df_zs = pd.DataFrame(n_zs_result).T\n",
    "\n",
    "# Combine both DataFrames for comparison\n",
    "df_comparison = pd.concat([df_memory, df_zs], axis=1, keys=['n_memory_result', 'n_zs_result'])\n",
    "\n",
    "df_comparison.to_csv(\"n03_test_8_comparison.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for memory_patient, zs_patient in zip(n_memory_df.patient_filename, n_zs_df.patient_filename):\n",
    "    assert memory_patient == zs_patient\n",
    "\n",
    "print(memory_dict_n['50'])\n",
    "print()\n",
    "\n",
    "output_dir = \"studio_data/both_error/n03\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "for idx, (filename, label, memory_ans, zs_ans, memory_rsn, zs_rsn) in enumerate(zip(n_memory_df.patient_filename, n_memory_df.n, n_memory_df.cmem_n_ans_str, n_zs_df.zs_n_ans_str, n_memory_df.cmem_n_reasoning, n_zs_df.zs_n_reasoning)):\n",
    "    # if (memory_ans == zs_ans) and (f\"N{label}\" in memory_ans.upper()): # cases where both are correct\n",
    "    # if (f\"N{label}\" not in zs_ans.upper()) and (f\"N{label}\" in memory_ans.upper()): # cases where only memory was correct\n",
    "    # if (f\"N{label}\" in zs_ans.upper()) and (f\"N{label}\" not in memory_ans.upper()): # cases where only zs was correct\n",
    "    if (f\"N{label}\" not in zs_ans.upper()) and (f\"N{label}\" not in memory_ans.upper()): # cases where both were wrong\n",
    "        data = {\n",
    "            \"data\": {\n",
    "                \"humanMachineDialogue\": [\n",
    "                    {\"author\": \"Patient filename\", \"text\": filename},\n",
    "                    {\"author\": \"Memory Reasoning\", \"text\": memory_rsn},\n",
    "                    {\"author\": \"ZS Reasoning\", \"text\": zs_rsn}, \n",
    "                    {\"author\": \"Answer\", \"text\": f\"N{label}\"},\n",
    "                    {\"author\": \"Memory Answer\", \"text\": memory_ans},\n",
    "                    {\"author\": \"ZS Answer\", \"text\": zs_ans}\n",
    "                ]\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        file_name = f\"n_data_{idx}.json\"\n",
    "        file_path = os.path.join(output_dir, file_name)\n",
    "        with open(file_path, 'w') as json_file:\n",
    "            json.dump(data, json_file, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Unnamed: 0', 'patient_filename', 't', 'text', 'n', 'zs_t_is_parsed',\n",
       "       'zs_t_reasoning', 'zs_t_ans_str', 'zs_n_is_parsed', 'zs_n_reasoning',\n",
       "       'zs_n_ans_str', 'cmem_t_is_parsed', 'cmem_t_reasoning',\n",
       "       'cmem_t_ans_str', 'cmem_n_is_parsed', 'cmem_n_reasoning',\n",
       "       'cmem_n_ans_str'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_memory_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "zs_predict_prompt_n03 = \"\"\"You are provided with a pathology report for a cancer patient.\n",
    "\n",
    "Please review this report and determine the pathologic N stage of the patient's cancer.\n",
    "\n",
    "Here is the report:\n",
    "{report}\n",
    "\n",
    "What is your reasoning to support your N stage prediction?\n",
    "\n",
    "What is the N stage from this report? Ignore any substaging information. Please select from the following four options: N0, N1, N2, N3.\n",
    "\"\"\"\n",
    "\n",
    "zs_predict_prompt_t14 = \"\"\"You are provided with a pathology report for a cancer patient.\n",
    "\n",
    "Please review this report and determine the pathologic T stage of the patient's cancer.\n",
    "\n",
    "Here is the report:\n",
    "{report}\n",
    "\n",
    "What is your reasoning to support your T stage prediction? \n",
    "\n",
    "What is the T stage from this report? Ignore any substaging information. Please select from the following four options: T1, T2, T3, T4.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are provided with a pathology report for a cancer patient.\n",
      "\n",
      "Please review this report and determine the pathologic N stage of the patient's cancer.\n",
      "\n",
      "Here is the report:\n",
      "FINAL PATHOLOGIC DIAGNOSIS. A. Breast, left; modified radical mastectomy: - Invasive ductal carcinoma, SBR grade 2, see parameters. - Ductal carcinoma in situ, intermediate grade, solid and cribriform types. with associated necrosis. - Surgical margins free of tumor. - Biopsy site changes. - Fourteen nodes, negative for carcinoma (0/14). - Nipple, areola, and skin without diagnostic abnormality. B. Breast, right; simple mastectomy: - Invasive ductal carcinoma, SBR grade 2, inferior-anterior of hematoma. cavity, see parameters. - Extensive ductal carcinoma in situ, intermediate grade, solid type, with. associated central necrosis. - Anterior-inferior margin positive for invasive carcinoma. - Attached skeletal muscle focally infiltrated by invasive carcinoma. (anterior-inferior). - Columnar cell change and adenosis. - Intraductal papilloma. - Nipple, areola, and skin without diagnostic abnormality. Breast Pathologic Parameters (Part A Left Breast). 1. Invasive carcinoma: A. Gross measurement: 1.9 X 1.5 X 1.5 cm. B. Composite histologic (modified SBR) grade: II. - Architecture: 2. - Nuclear grade: 3. - Mitotic count: 1. C. Associated intraductal carcinoma in situ (DCIS): - Within main mass (forming 10 % of tumor volume). - Extending away from main lesion. 2. Intraductal carcinoma: A. Gross or microscopic (specify) measurement: (5.2 cm, spanning 4 slices). B. Type: Cribriform and Solid. C. Nuclear grade: Intermediate. D. Associated features: Necrosis. 3. Excisional biopsy margins: Free of tumor. - DCIS 4 mm from posterior margin. - Invasive carcinoma 5 mm from posterior (closest) margin. 4. Blood vessel and lymphatic invasion: Not identified. 5. Nipple: unremarkable. 6. Skin: uninvolved. 7. Skeletal muscle: Focally present, negative for tumor. 8. Axillary lymph nodes: In combination with previous sentinel lymph node biopsy. (see. One of fifteen nodes positive for carcinoma (1/15). - Size of largest metastatic deposit: 18 mm. - Extranodal extension: present (2 mm; largest focus). 9. Special studies (see. - ER: Positive expression in >95% of invasive tumor nuclei. - PR: Positive expression in 80% of invasive tumor nuclei. - Her2/neu antigen (FISH): Non-amplified (1.02). 10. pTNM (AJCC, 7th edition, 2010): pT1cN1MX. Effective. this Checklist utilizes the 7th edition TNM staging. system for breast of the American Joint Committee on Cancer (AJCC) and the. International Union Against Cancer (UICC). Breast Pathologic Parameters (Part B Right Breast). 1. Invasive carcinoma: A. Microscopic measurement: 1.5 cm. B. Composite histologic (modified SBR) grade: II. - Architecture: 3. - Nuclear grade: 2. - Mitotic count: 1. C. Associated intraductal carcinoma in situ (DCIS): - Within main mass (forming 50 % of tumor volume). - Extending away from main mass. 2. Intraductal carcinoma: A. Microscopic measurement: 6.1 cm, spanning over 5 slices, involving lower. inner and upper inner quadrants and extending anteriorly towards nipple. B. Type: Solid. C. Nuclear grade: Intermediate. D. Associated features: Necrosis and cancerization of lobules. 3. Excisional biopsy margins: Positive. - DCIS >3 mm from posterior and inferior (closest) margins. - Invasive carcinoma at inferior-anterior margin with infiltration of. attached skeletal muscle. 4. Blood vessel and lymphatic invasion: Suspicious. 5. Nipple: unremarkable;DCIS noted 5 mm from areolar complex. 6. Skin: uninvolved. 7. Skeletal muscle: Focus attached inferior-anterior infiltrated by carcinoma. 8. Axillary lymph nodes: Negative (Two sentinel nodes, negative for carcinoma. (0/2), see. 9. Special studies (see. - ER: Strong expression in >90% of invasive tumor nuclei. - PR: Strong expression in >90% of invasive tumor nuclei. - Her2/neu antigen (FISH): Non-amplified (1.04). 10. pTNM (AJCC, 7th edition, 2010): pT1cNO(sn)MX (pending review of imaging). Effective. this Checklist utilizes the 7th edition TNM staging. system for breast of the American Joint Committee on Cancer (AJCC) and the. International Union Against Cancer (UICC). Clinical History: The patient is a. 'ear old female with malignant neoplasm of the breast. undergoing left modified radical mastectomy and right simple mastectomy. Comment. B. Stains for CKAE1/AE3 and myosin-heavy chain support extension of invasive. carcinoma into skeletal muscle at the inferior-anterior aspect of the breast. Findings were discussed with. Specimens Received: A: Left modified radicalmastectomy; mastectomy. B: Right simple mastectomy; partial mastectomy. Gross Description: Received are two containers, each labeled with the patient's name and medical. record number. A. Container A is further designated '1. Left modified radicalmastectomy;. mastectomy. The radiographic findings for this breast include an irreglarly. shaped mass measuring 1.4 X 1.2 X 1.9 cm and areas of heterogenous kinetics with. washout. The total extent of the abnormal enhancing is 2.8 X 2.3 X 1.9 cm. Received fresh and placed in formalin is a 468 gram mastectomy specimen. measuring 14.5 cm from medial to lateral, 17.5 cm from superior to inferior and. 3.7 cm from anterior to posterior. It has an axillary tail measuring 11 X 6.5. X. 1.5 cm. The skin measures 13.2 X 5 cm and an nipple areola complex measuring 3 X. 2.7 cm and a nipple measuring 1 X 1 cm. The specimen is inked as follows: posterior black, anterior-superior yellow, anterior-inferior green. It is. sliced into 11 slices to reveal a lesion that is present from slices 6 to 8 and. the lesion measures 1.9 X 1.5 cm and has a medial to lateral dimension of 1.5. cm. It focally abuts the deep margin and is 3 cm from the inferior margin and 9. cm from the superior margin. No other lesions or masses are identified. A. number of lymph node candidates are identified in the axillary tail, the largest. of which measures 2.5 X 2 X 1.8 cm. This large lymph node is sectioned to. reveal a central area of necrosis. Block Summary: A1-A3: lesion from slice 6. A2: superior to lesion. A3: inferior to lesion. A4: lesion in slice 7. A5: lesion in slice 8. A6: slice 5 next to lesion. A7: slice 9 next to lesion. A8: skin and nipple. A9: areola. A10: upper-inner quadrant from slice 2. A11: lower-inner quadrant from slice 4. A12: upper-outer quadrant from slice 7. A13: lower-outer quadrant from slice 7. A14: three lymph node candidates. A15: three lymph node candidates. A16: three lymph node candidates. A17: three lymph node candidates. A18: one lymph node candidate. A19: one lymph node candidate. A20-A21: largest lymph node candidate bisected. A22-A25: additional representative sections of axillary fat. B. Container B is further designated '2. Right simple mastectomy;. partial mastectomy. Received fresh and placed in formalin is a 350 gram. mastectomy specimen measuring 17 cm from medial to lateral, 13.5 from superior. to inferior, and 2.5 cm from anterior to posterior. There is a skin ellipse. measuring3 X 3.5 cm and an areola measuring 2.5 X 2.7 cm and a nipple measuring. 1 x 1 cm. It is sliced into 14 slices in which there is a hematoma cavity. measuring 1.5 X 1.5 x 3.1 cm found in slices 3, 4, 5 and 6. The cavity has a rim. of white-tan, firm tissue around it and the cavity is 1.1 cm from the deep. margin and 0.6 cm from the inferior margin. Also note that the deep margin is. inked red, the anterior-superior margin is inked blue and the anterior-inferior. margin is inked green. No other lesions or masses are identified. Block Summary: B1: lesion in slice 3. B2: superior to B1. B3: lesion in slice 4. B4: superior to B3. B5: lesion in slice 5. B6: superior to B5. B7: lesion in slice 6. B8: superior to B7. B9: slice 7 adjacent to lesion. B10: slice 2 adjacent to lesion. B11: upper-outer quadrant in slice 12. B12: lower-outer quadrant in slice 11. B13: upper-inner quadrant in slice 5. B14: lower-inner quadrant in slice 6. B15: skin and nipple. B16: areola. The specimen is inked as follows: posterior red, anterior-superior blue,. anterior inferior green. The tissue is fixed for at least six hours in NBF and no more than 72 hours.\n",
      "\n",
      "What is your reasoning to support your N stage prediction?\n",
      "\n",
      "What is the N stage from this report? Ignore any substaging information. Please select from the following four options: N0, N1, N2, N3.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "filename = \"TCGA-OL-A5RU.7507AC79-4C8E-4393-92B9-BFA6BEB492D0\"\n",
    "# retrieve report, given patient_filename\n",
    "import pandas as pd\n",
    "from metrics import *\n",
    "import json\n",
    "import os\n",
    "\n",
    "n_memory_df = pd.read_csv(\"/home/yl3427/cylab/selfCorrectionAgent/n03_test_8_memory_at_50\")\n",
    "for file_name, report in zip(n_memory_df['patient_filename'], n_memory_df['text']):\n",
    "    if file_name == filename:\n",
    "        print(zs_predict_prompt_n03.format(report=report))\n",
    "        # print(report)\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'T3': {'precision': 0.95, 'recall': 0.7, 'f1': 0.81, 'support': 108},\n",
       " 'T2': {'precision': 0.83, 'recall': 0.97, 'f1': 0.9, 'support': 468},\n",
       " 'T1': {'precision': 0.93, 'recall': 0.73, 'f1': 0.82, 'support': 188},\n",
       " 'T4': {'precision': 0.88, 'recall': 0.61, 'f1': 0.72, 'support': 36},\n",
       " 'overall': {'macro_precision': 0.9,\n",
       "  'macro_recall': 0.76,\n",
       "  'macro_f1': 0.81,\n",
       "  'support': 800}}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t_df = pd.read_csv(\"/home/yl3427/cylab/selfCorrectionAgent/result/0716/t14_zs_test_800.csv\")\n",
    "t14_calculate_metrics(t_df['t'], t_df['zs_t_ans_str'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'N1': {'precision': 0.86, 'recall': 0.89, 'f1': 0.87, 'support': 300},\n",
       " 'N2': {'precision': 0.67, 'recall': 0.67, 'f1': 0.67, 'support': 110},\n",
       " 'N0': {'precision': 0.95, 'recall': 0.96, 'f1': 0.95, 'support': 316},\n",
       " 'N3': {'precision': 0.98, 'recall': 0.76, 'f1': 0.85, 'support': 74},\n",
       " 'overall': {'macro_precision': 0.86,\n",
       "  'macro_recall': 0.82,\n",
       "  'macro_f1': 0.84,\n",
       "  'support': 800}}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_df = pd.read_csv(\"/home/yl3427/cylab/selfCorrectionAgent/result/0716/n03_zs_test_800.csv\")\n",
    "n03_calculate_metrics(n_df['n'], n_df['zs_n_ans_str'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "selfCorrection",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
